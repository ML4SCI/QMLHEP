{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Test"
      ],
      "metadata": {
        "id": "eHLXlaqWfuIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import pennylane as qml\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load and preprocess data\n",
        "jet_images_path = 'jet-images_Mass60-100_pT250-300_R1.25_Pix25.hdf5'\n",
        "with h5py.File(jet_images_path, 'r') as jet_mass_data:\n",
        "    X_jet = jet_mass_data['image']\n",
        "    print(\"Original shape:\", X_jet.shape)\n",
        "    X_jet = np.array(X_jet)\n",
        "    X_jet = torch.tensor(X_jet, dtype=torch.float32)\n",
        "\n",
        "# Normalize and resize images\n",
        "X_jet = (X_jet - X_jet.min()) / (X_jet.max() - X_jet.min())\n",
        "X_jet = X_jet.unsqueeze(1)\n",
        "X_jet_resized = nn.functional.interpolate(X_jet, size=(16, 16), mode='bilinear', align_corners=False)\n",
        "print(\"Resized shape:\", X_jet_resized.shape)\n",
        "\n",
        "# Sample data\n",
        "indices = random.sample(range(X_jet_resized.shape[0]), 10000)\n",
        "X_jet_sampled = X_jet_resized[indices]\n",
        "print(\"Sampled shape:\", X_jet_sampled.shape)\n",
        "\n",
        "# Flatten images and apply PCA\n",
        "X_flat = X_jet_sampled.view(-1, 256).numpy()\n",
        "n_components = 8\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_flat)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "class JetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = JetDataset(X_pca)\n",
        "batch_size = 64\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define quantum devices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vMgWzchgWE1",
        "outputId": "e15317c5-7a98-405a-c780-d9cb2356e8e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (872666, 25, 25)\n",
            "Resized shape: torch.Size([872666, 1, 16, 16])\n",
            "Sampled shape: torch.Size([10000, 1, 16, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@qml.qnode(dev_generator, interface='torch')\n",
        "def generator_qnode(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
        "\n",
        "@qml.qnode(dev_discriminator, interface='torch')\n",
        "def discriminator_qnode(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "# Define weight shapes for the models\n",
        "generator_weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "discriminator_weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "\n",
        "# Create the generator and discriminator models\n",
        "generator = qml.qnn.TorchLayer(generator_qnode, generator_weight_shapes).to(device)\n",
        "discriminator = qml.qnn.TorchLayer(discriminator_qnode, discriminator_weight_shapes).to(device)\n",
        "\n",
        "# Define optimizers and loss function\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "d_losses = []\n",
        "g_losses = []"
      ],
      "metadata": {
        "id": "3OFFiZeJNVZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, real_data in enumerate(dataloader):\n",
        "        batch_size = real_data.size(0)\n",
        "        real_data = real_data.to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "        # Real data\n",
        "        real_labels = torch.ones(batch_size).to(device)  # Changed shape to [batch_size]\n",
        "        real_output = discriminator(real_data)\n",
        "        real_output = (real_output + 1)/2  # Map to [0,1]\n",
        "        d_loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "        # Fake data\n",
        "        noise = torch.randn(batch_size, n_qubits).to(device)\n",
        "        fake_data = generator(noise)\n",
        "        fake_data = fake_data.detach()\n",
        "        fake_output = discriminator(fake_data)\n",
        "        fake_output = (fake_output + 1)/2  # Map to [0,1]\n",
        "        fake_labels = torch.zeros(batch_size).to(device)  # Changed shape to [batch_size]\n",
        "        d_loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        g_optimizer.zero_grad()\n",
        "        noise = torch.randn(batch_size, n_qubits).to(device)\n",
        "        fake_data = generator(noise)\n",
        "        fake_output = discriminator(fake_data)\n",
        "        fake_output = (fake_output + 1)/2  # Map to [0,1]\n",
        "        g_loss = criterion(fake_output, real_labels)  # Try to fool discriminator\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Save losses\n",
        "        d_losses.append(d_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}')"
      ],
      "metadata": {
        "id": "PqNmIH1df-Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptual-Quantum Loss\n",
        "\n",
        "Since we are working with quantum data, we want to balance two key objectives during training:\n",
        "\n",
        "1. **Image Quality**: Ensuring that the generated images are perceptually similar to real images.\n",
        "2. **Quantum Fidelity**: Ensuring that the quantum states of the generated data resemble those of the real data.\n",
        "\n",
        "To achieve this balance, we introduce a combined loss function known as **Perceptual Quantum Loss**.\n",
        "\n",
        "#### **1. Perceptual Loss**\n",
        "\n",
        "The **Perceptual Loss** is used to measure the similarity between real and generated images based on high-level features extracted from a pre-trained network (like **VGG16**). Instead of comparing images at the pixel level, the perceptual loss compares features from deeper layers of the network, capturing more abstract patterns like edges, textures, and shapes. This ensures that the generated images are perceptually similar to real ones.\n",
        "\n",
        "The pre-trained network (VGG16) acts as a \"feature extractor\" for this comparison. We resize the generated images and real images to match the input size of VGG16, extract their features, and compute the mean squared error (MSE) between these features. Lower perceptual loss indicates that the generated images look more like the real images.\n",
        "\n",
        "#### **2. Quantum Fidelity**\n",
        "\n",
        "Since we are working with quantum data, we also need to ensure that the quantum states of the generated data closely match those of the real data. **Quantum Fidelity** measures the similarity between two quantum states. Higher fidelity means that the quantum states are more similar, indicating that the generator is learning to produce data that is not only visually accurate but also quantum-accurate.\n",
        "\n",
        "In our implementation, we encode both real and generated data into quantum circuits and compute the fidelity between the quantum states.\n",
        "\n",
        "#### **3. Combined Loss Function**\n",
        "\n",
        "The **Perceptual Quantum Loss** combines these two components:\n",
        "\n",
        "$$\n",
        "L_{\\text{total}} = L_{\\text{adversarial}} + \\alpha \\times L_{\\text{perceptual}} - \\beta \\times L_{\\text{fidelity}}\n",
        "$$\n",
        "\n",
        "- **Adversarial Loss (`L_adversarial`)**: This is the standard GAN loss that drives the generator to produce images that fool the discriminator.\n",
        "- **Perceptual Loss (`L_perceptual`)**: Encourages the generator to produce images that are perceptually similar to real images.\n",
        "- **Quantum Fidelity (`L_fidelity`)**: Ensures that the quantum states of generated data match those of the real data.\n",
        "\n",
        "Here, `alpha` controls the importance of the perceptual loss, and `beta` controls the importance of quantum fidelity. By adjusting these hyperparameters, we can fine-tune the balance between visual similarity and quantum state similarity.\n",
        "\n",
        "#### **Why Use Perceptual Quantum Loss?**\n",
        "\n",
        "In Quantum GANs, we aim to generate data that is accurate in two domains:\n",
        "- **Visual domain**: Ensuring that the images look like real images.\n",
        "- **Quantum domain**: Ensuring that the quantum data respects the underlying quantum structure.\n",
        "\n",
        "Perceptual Quantum Loss allows us to address both goals in a single loss function, making it a powerful tool for training Quantum GANs to generate realistic data while maintaining quantum properties.\n"
      ],
      "metadata": {
        "id": "99fUeo7afcOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import pennylane as qml\n",
        "from torchvision import models, transforms\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "import pandas as pd\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# # Load and preprocess data\n",
        "# jet_images_path = 'jet-images_Mass60-100_pT250-300_R1.25_Pix25.hdf5'\n",
        "# with h5py.File(jet_images_path, 'r') as jet_mass_data:\n",
        "#     X_jet = jet_mass_data['image']\n",
        "#     print(\"Original shape:\", X_jet.shape)\n",
        "#     X_jet = np.array(X_jet)\n",
        "#     X_jet = torch.tensor(X_jet, dtype=torch.float32)\n",
        "\n",
        "# # Normalize and resize images\n",
        "# X_jet = (X_jet - X_jet.min()) / (X_jet.max() - X_jet.min())\n",
        "# X_jet = X_jet.unsqueeze(1)\n",
        "# X_jet_resized = nn.functional.interpolate(X_jet, size=(16, 16), mode='bilinear', align_corners=False)\n",
        "# print(\"Resized shape:\", X_jet_resized.shape)\n",
        "\n",
        "# # Sample data\n",
        "# indices = random.sample(range(X_jet_resized.shape[0]), 10000)\n",
        "# X_jet_sampled = X_jet_resized[indices]\n",
        "# print(\"Sampled shape:\", X_jet_sampled.shape)"
      ],
      "metadata": {
        "id": "WLk--oXH-3SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhMcMHM2rtek",
        "outputId": "b979d6d7-16ee-4245-82ae-ab67f904ba4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "jet_images_path = '/content/drive/MyDrive/jet-images_Mass60-100_pT250-300_R1.25_Pix25.hdf5'\n"
      ],
      "metadata": {
        "id": "j1CIaSNtr8uT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(jet_images_path, 'r') as jet_mass_data:\n",
        "    X_jet = jet_mass_data['image']\n",
        "    print(\"Original shape:\", X_jet.shape)\n",
        "    X_jet = np.array(X_jet)\n",
        "    X_jet = torch.tensor(X_jet, dtype=torch.float32)\n",
        "\n",
        "# Normalize and resize images\n",
        "X_jet = (X_jet - X_jet.min()) / (X_jet.max() - X_jet.min())\n",
        "X_jet = X_jet.unsqueeze(1)\n",
        "X_jet_resized = nn.functional.interpolate(X_jet, size=(16, 16), mode='bilinear', align_corners=False)\n",
        "print(\"Resized shape:\", X_jet_resized.shape)\n",
        "\n",
        "# Sample data\n",
        "indices = random.sample(range(X_jet_resized.shape[0]), 10000)\n",
        "X_jet_sampled = X_jet_resized[indices]\n",
        "print(\"Sampled shape:\", X_jet_sampled.shape)"
      ],
      "metadata": {
        "id": "eGu4mJdRr8l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten images and apply PCA\n",
        "X_flat = X_jet_sampled.view(-1, 256).numpy()\n",
        "n_components = 8\n",
        "pca = PCA(n_components=n_components)\n",
        "X_pca = pca.fit_transform(X_flat)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "class JetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = torch.tensor(data, dtype=torch.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = JetDataset(X_pca)\n",
        "batch_size = 64\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define quantum devices\n",
        "n_qubits = n_components\n",
        "n_layers = 3\n",
        "dev_generator = qml.device('default.qubit', wires=n_qubits)\n",
        "dev_discriminator = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "# Define the generator and discriminator quantum circuits\n",
        "@qml.qnode(dev_generator, interface='torch')\n",
        "def generator_qnode(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
        "\n",
        "@qml.qnode(dev_discriminator, interface='torch')\n",
        "def discriminator_qnode(inputs, weights):\n",
        "    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "    qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "    return qml.expval(qml.PauliZ(0))\n"
      ],
      "metadata": {
        "id": "7O0IYzG1t2zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weight shapes for the models\n",
        "generator_weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "discriminator_weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "\n",
        "# Create the generator and discriminator models\n",
        "generator = qml.qnn.TorchLayer(generator_qnode, generator_weight_shapes).to(device)\n",
        "discriminator = qml.qnn.TorchLayer(discriminator_qnode, discriminator_weight_shapes).to(device)\n",
        "\n",
        "# Define optimizers and loss function\n",
        "g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Load pre-trained VGG16 model for perceptual loss\n",
        "vgg = models.vgg16(pretrained=True).features[:8].eval().to(device)\n",
        "# Freeze VGG parameters\n",
        "for param in vgg.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Define image preprocessing for VGG\n",
        "vgg_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.Normalize(mean=[0.485], std=[0.229]),  # Normalization parameters for VGG\n",
        "])\n",
        "\n",
        "# Function to compute perceptual loss\n",
        "def perceptual_loss(real_images, fake_images):\n",
        "    # Both inputs are in PCA space, need to inverse transform and reshape\n",
        "    real_images = pca.inverse_transform(real_images.cpu().numpy())\n",
        "    fake_images = pca.inverse_transform(fake_images.cpu().detach().numpy())\n",
        "\n",
        "    real_images = torch.tensor(real_images, dtype=torch.float32).view(-1, 1, 16, 16).to(device)\n",
        "    fake_images = torch.tensor(fake_images, dtype=torch.float32).view(-1, 1, 16, 16).to(device)\n",
        "\n",
        "    # Upsample images to 224x224 and replicate channels to match VGG input\n",
        "    real_images = F.interpolate(real_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "    fake_images = F.interpolate(fake_images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "    real_images = real_images.repeat(1, 3, 1, 1)  # Convert to 3 channels\n",
        "    fake_images = fake_images.repeat(1, 3, 1, 1)\n",
        "\n",
        "    # Normalize images\n",
        "    real_images = vgg_transform(real_images)\n",
        "    fake_images = vgg_transform(fake_images)\n",
        "\n",
        "    # Extract features\n",
        "    real_features = vgg(real_images)\n",
        "    fake_features = vgg(fake_images)\n",
        "\n",
        "    # Compute perceptual loss\n",
        "    loss = F.mse_loss(fake_features, real_features)\n",
        "    return loss\n",
        "\n",
        "# Function to compute quantum fidelity\n",
        "# Function to compute quantum fidelity\n",
        "def quantum_fidelity(real_data, fake_data):\n",
        "    # real_data and fake_data are in PCA space\n",
        "    # Define a QNode that outputs the state vector\n",
        "    dev_fidelity = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "    @qml.qnode(dev_fidelity, interface='torch')\n",
        "    def state_preparation(inputs):\n",
        "        qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "        return qml.state()\n",
        "\n",
        "    real_states = [state_preparation(data).cpu().numpy() for data in real_data] # Move tensors to CPU before converting to NumPy\n",
        "    fake_states = [state_preparation(data).cpu().detach().numpy() for data in fake_data] # Move tensors to CPU before converting to NumPy and detach from the computation graph\n",
        "\n",
        "    fidelities = []\n",
        "    for real_state, fake_state in zip(real_states, fake_states):\n",
        "        fidelity = np.abs(np.vdot(real_state, fake_state)) ** 2\n",
        "        fidelities.append(fidelity)\n",
        "    return np.mean(fidelities)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 30\n",
        "d_losses = []\n",
        "g_losses = []\n",
        "perceptual_losses = []\n",
        "fidelities = []"
      ],
      "metadata": {
        "id": "1djFT5nGuAHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, real_data in enumerate(dataloader):\n",
        "        batch_size = real_data.size(0)\n",
        "        real_data = real_data.to(device)\n",
        "\n",
        "        # Train Discriminator\n",
        "        d_optimizer.zero_grad()\n",
        "        # Real data\n",
        "        real_labels = torch.ones(batch_size).to(device)\n",
        "        real_output = discriminator(real_data)\n",
        "        real_output = (real_output + 1)/2  # Map to [0,1]\n",
        "        d_loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "        # Fake data\n",
        "        noise = torch.randn(batch_size, n_qubits).to(device)\n",
        "        fake_data = generator(noise)\n",
        "        fake_data = fake_data.detach()\n",
        "        fake_output = discriminator(fake_data)\n",
        "        fake_output = (fake_output + 1)/2  # Map to [0,1]\n",
        "        fake_labels = torch.zeros(batch_size).to(device)\n",
        "        d_loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "        # Total discriminator loss\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        g_optimizer.zero_grad()\n",
        "        noise = torch.randn(batch_size, n_qubits).to(device)\n",
        "        fake_data = generator(noise)\n",
        "        fake_output = discriminator(fake_data)\n",
        "        fake_output = (fake_output + 1)/2  # Map to [0,1]\n",
        "        g_loss = criterion(fake_output, real_labels)  # Try to fool discriminator\n",
        "\n",
        "        # Compute Perceptual Loss\n",
        "        ploss = perceptual_loss(real_data, fake_data)\n",
        "\n",
        "        # Compute Quantum Fidelity\n",
        "        q_fidelity = quantum_fidelity(real_data, fake_data)\n",
        "\n",
        "        # Combine losses (you can adjust the weights)\n",
        "        alpha = 1.0  # Weight for perceptual loss\n",
        "        beta = 1.0   # Weight for fidelity\n",
        "        total_g_loss = g_loss + alpha * ploss - beta * torch.tensor(q_fidelity).to(device)\n",
        "\n",
        "        total_g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "        # Save losses and metrics\n",
        "        d_losses.append(d_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "        perceptual_losses.append(ploss.item())\n",
        "        fidelities.append(q_fidelity)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, '\n",
        "          f'Perceptual Loss: {ploss.item():.4f}, Fidelity: {q_fidelity:.4f}')"
      ],
      "metadata": {
        "id": "U42aBI4WuWjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper parameter tuning"
      ],
      "metadata": {
        "id": "oT4eVLsSO632"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(alpha, beta, num_epochs=10):\n",
        "    # Define quantum devices\n",
        "    dev_generator = qml.device('default.qubit', wires=n_qubits)\n",
        "    dev_discriminator = qml.device('default.qubit', wires=n_qubits)\n",
        "\n",
        "    # Define the generator and discriminator quantum circuits\n",
        "    @qml.qnode(dev_generator, interface='torch')\n",
        "    def generator_qnode(inputs, weights):\n",
        "        qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "        qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "        return [qml.expval(qml.PauliZ(w)) for w in range(n_qubits)]\n",
        "\n",
        "    @qml.qnode(dev_discriminator, interface='torch')\n",
        "    def discriminator_qnode(inputs, weights):\n",
        "        qml.AngleEmbedding(inputs, wires=range(n_qubits))\n",
        "        qml.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
        "        return qml.expval(qml.PauliZ(0))\n",
        "\n",
        "    # Define weight shapes for the models\n",
        "    generator_weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "    discriminator_weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
        "\n",
        "    # Create the generator and discriminator models\n",
        "    generator = qml.qnn.TorchLayer(generator_qnode, generator_weight_shapes).to(device)\n",
        "    discriminator = qml.qnn.TorchLayer(discriminator_qnode, discriminator_weight_shapes).to(device)\n",
        "\n",
        "    # Define optimizers and loss function\n",
        "    g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.01)\n",
        "    d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.01)\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Lists to store metrics\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    perceptual_losses = []\n",
        "    fidelities = []\n",
        "    total_g_losses = []\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, real_data in enumerate(dataloader):\n",
        "            batch_size = real_data.size(0)\n",
        "            real_data = real_data.to(device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            d_optimizer.zero_grad()\n",
        "            # Real data\n",
        "            real_labels = torch.ones(batch_size).to(device)\n",
        "            real_output = discriminator(real_data)\n",
        "            real_output = (real_output + 1)/2  # Map to [0,1]\n",
        "            d_loss_real = criterion(real_output, real_labels)\n",
        "\n",
        "            # Fake data\n",
        "            noise = torch.randn(batch_size, n_qubits).to(device)\n",
        "            fake_data = generator(noise)\n",
        "            fake_data = fake_data.detach()\n",
        "            fake_output = discriminator(fake_data)\n",
        "            fake_output = (fake_output + 1)/2  # Map to [0,1]\n",
        "            fake_labels = torch.zeros(batch_size).to(device)\n",
        "            d_loss_fake = criterion(fake_output, fake_labels)\n",
        "\n",
        "            # Total discriminator loss\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            g_optimizer.zero_grad()\n",
        "            noise = torch.randn(batch_size, n_qubits).to(device)\n",
        "            fake_data = generator(noise)\n",
        "            fake_output = discriminator(fake_data)\n",
        "            fake_output = (fake_output + 1)/2  # Map to [0,1]\n",
        "            g_loss = criterion(fake_output, real_labels)  # Try to fool discriminator\n",
        "\n",
        "            # Compute Perceptual Loss\n",
        "            ploss = perceptual_loss(real_data, fake_data)\n",
        "\n",
        "            # Compute Quantum Fidelity\n",
        "            q_fidelity = quantum_fidelity(real_data, fake_data)\n",
        "\n",
        "            # Combine losses\n",
        "            total_g_loss = g_loss + alpha * ploss - beta * torch.tensor(q_fidelity).to(device)\n",
        "\n",
        "            total_g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # Save losses and metrics\n",
        "            d_losses.append(d_loss.item())\n",
        "            g_losses.append(g_loss.item())\n",
        "            perceptual_losses.append(ploss.item())\n",
        "            fidelities.append(q_fidelity)\n",
        "            total_g_losses.append(total_g_loss.item())\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Time: {elapsed_time:.2f}s, '\n",
        "              f'd_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}, '\n",
        "              f'Total G Loss: {total_g_loss.item():.4f}, '\n",
        "              f'Perceptual Loss: {ploss.item():.4f}, Fidelity: {q_fidelity:.4f}')\n",
        "\n",
        "    # Return metrics for analysis\n",
        "    return {\n",
        "        'd_losses': d_losses,\n",
        "        'g_losses': g_losses,\n",
        "        'perceptual_losses': perceptual_losses,\n",
        "        'fidelities': fidelities,\n",
        "        'total_g_losses': total_g_losses,\n",
        "        'generator': generator  # Return the trained generator\n",
        "    }"
      ],
      "metadata": {
        "id": "fjhdfOonPv3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Grid Search over alpha and beta\n",
        "alpha_values = [0.1, 1.0, 10.0]\n",
        "beta_values = [0.1, 1.0, 10.0]\n",
        "num_epochs = 10  # Adjust as needed\n",
        "\n",
        "results = {}\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    for beta in beta_values:\n",
        "        print(f'\\nTraining with alpha={alpha}, beta={beta}')\n",
        "        metrics = train_model(alpha, beta, num_epochs=num_epochs)\n",
        "        results[(alpha, beta)] = metrics\n",
        "\n",
        "# Metrics Collection and Visualization\n",
        "summary = []\n",
        "\n",
        "for (alpha, beta), metrics in results.items():\n",
        "    final_perceptual_loss = metrics['perceptual_losses'][-1]\n",
        "    final_fidelity = metrics['fidelities'][-1]\n",
        "    summary.append({\n",
        "        'alpha': alpha,\n",
        "        'beta': beta,\n",
        "        'perceptual_loss': final_perceptual_loss,\n",
        "        'fidelity': final_fidelity\n",
        "    })"
      ],
      "metadata": {
        "id": "Eh090tyiP5ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_summary = pd.DataFrame(summary)\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(df_summary)\n",
        "\n",
        "# Plotting Perceptual Loss vs. Fidelity for different alpha and beta values\n",
        "plt.figure(figsize=(8,6))\n",
        "for alpha in alpha_values:\n",
        "    df_alpha = df_summary[df_summary['alpha'] == alpha]\n",
        "    plt.plot(df_alpha['fidelity'], df_alpha['perceptual_loss'], marker='o', label=f'alpha={alpha}')\n",
        "plt.xlabel('Quantum Fidelity')\n",
        "plt.ylabel('Perceptual Loss')\n",
        "plt.title('Perceptual Loss vs. Fidelity for Different Alpha Values')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Generator Outputs for Best Model\n",
        "# Find the model with lowest perceptual loss\n",
        "best_model_key = min(results, key=lambda x: results[x]['perceptual_losses'][-1])\n",
        "best_generator = results[best_model_key]['generator']\n",
        "\n",
        "print(f\"\\nBest model parameters: alpha={best_model_key[0]}, beta={best_model_key[1]}\")\n",
        "\n",
        "# Generate and visualize images using the best generator\n",
        "noise = torch.randn(16, n_qubits).to(device)\n",
        "fake_data = best_generator(noise)\n",
        "fake_data = fake_data.detach().cpu().numpy()\n",
        "\n",
        "# Inverse PCA to reconstruct images\n",
        "fake_data_pca = pca.inverse_transform(fake_data)\n",
        "fake_images = fake_data_pca.reshape(-1, 1, 16, 16)\n",
        "\n",
        "# Plot generated images\n",
        "plt.figure(figsize=(8,8))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i+1)\n",
        "    plt.imshow(fake_images[i, 0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.suptitle(f'Generated Images using Best Model (alpha={best_model_key[0]}, beta={best_model_key[1]})')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arUxIsZ1P8lw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSEFAOoLKPJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}