{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the machine learned algebras in notebook 1 and the extracted metric tensor $J$ in notebook 2, we are now ready for some equivariant quantum machine learning. In this final notebook, we walk you through a Lie-Equivariant Quantum Graph Neural Network (Lie-EQGNN). We aim specifically for a quantum model that not only is data efficient, but also has symmetry-preserving properties from any arbitrary Lie algebra that can be learned directly from the data. Given the limitations by noise of current quantum hardware, efficiently designing new parameterized circuit architectures is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /home/jogi/anaconda3/lib/python3.9/site-packages (2.5.2)\n",
      "Requirement already satisfied: tqdm in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.26.3)\n",
      "Requirement already satisfied: scipy in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.11.4)\n",
      "Requirement already satisfied: fsspec in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.9.3)\n",
      "Requirement already satisfied: requests in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-lcy59mga/torch-sparse_ff3c8ac0008f4b1d8e47a3a9bea01e6f/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25hCollecting torch_scatter\n",
      "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ch0ak2wl/torch-scatter_6bea335503cc468b8f0c565fd8280640/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# For Colab\n",
    "!pip install torch_geometric\n",
    "!pip install torch_sparse\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset\n",
    "\n",
    "In this work, we consider the task of determining whether a given jet originated from a quark or a gluon (this is known as jet-tagging). For illustration we use the high energy physics dataset $\\text{\\textit{Pythia8 Quark and Gluon Jets for Energy Flow}}$ (Patrick T. Komiske et al, 2019), which contains two million jets split equally into one million quark jets and one million gluon jets. These jets resulted from LHC collisions with total center of mass energy $\\sqrt{s} = 14$ TeV and were selected to have transverse momenta $p_T^{jet}$ between $500$ to $550$ GeV and rapidities $|y^{jet}| < 1.7$. For our analysis, we randomly picked $N = 12500$ jets and used the first $10000$ for training, the next $1250$ for validation, and the last $1250$ for testing. These sets happened to contain $4982$, $658$, and $583$ quark jets, respectively.\n",
    "\n",
    "<img src=\"../figures/particle_cloud_mpgan.png\" width=65% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "*(Figure: the coordinate system (left) used to represent components of the particle momentum $\\vec{p}$. Schematic representation of a gluon jet (middle) and a quark jet (right). The jet constituents (solid lines)  are collimated around the jet axis. [Figure adapted from Fig.~1 in (Raghav Kansal et al, 2021).])*\n",
    "\n",
    "\n",
    "For our purposes, we consider the jet dataset to be constituted of point-clouds, where each jet is represented as a graph $\\mathcal{G} = \\{\\mathcal{V,E}\\}$, i.e., a set of nodes and edges, respectively. (This is the natural data structure used by Graph Neural Networks.) In our case, each node has the transverse momentum $p_T$, pseudorapidity $\\eta$, azimuthal angle $\\phi$ (and other scalar-like quantities like particle ID, particle mass, etc.) of the respective constituent particle in the jet (see the Figure above). Generally, the number of features is always constant, but the number of nodes in each jet may vary. For our analysis we used jets with at least 10 particles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Quantum and Classical Graph Neural Networks\n",
    "\n",
    "Given the inherent graph structure of our dataset, which captures the complex interactions and relationships between particles, it is natural to employ graph neural networks (GNNs) for the task of tagging quarks versus gluons. GNNs have been successfully applied in particle physics for such classification tasks, as discussed in the paper \"[Graph Neural Networks in Particle Physics](https://arxiv.org/abs/2007.13681)\" by Shlomi et al. (2020). In our approach, we start with classical GNNs to establish a foundational understanding and then extend to their quantum counterparts to explore potential advantages offered by quantum computing.\n",
    "\n",
    "One method to implement a quantum GNN involves encoding the jets—representations of observed particle interactions—into a quantum Hamiltonian. This Hamiltonian encapsulates the graph's structure and properties, and its parameters are learned during training. The goal is to find the ground state (minimum energy state) of the Hamiltonian, which corresponds to the optimal representation of our graph for the classification task.\n",
    "\n",
    "The purpose of both classical and quantum GNNs is to learn deeper, topologically-preserving representations of input graph data. For quantum GNNs, a promising approach is to define a Hamiltonian that describes both the nodes (qubits) and their interactions (edges). A commonly used model for this purpose is the **transverse-field Ising model**, originally introduced to study phase transitions in magnetic systems. The Hamiltonian for this model is given by:\n",
    "\n",
    "$$\n",
    "\\hat{H}_{\\text{Ising}}(\\boldsymbol{\\theta}) = \\sum_{(i, j) \\in E} \\mathcal{W}_{ij} Z_i Z_j + \\sum_i \\mathcal{M}_i Z_i + \\sum_i X_i,\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $E$ is the set of edges (pairs of nodes) in the graph.\n",
    "- $Z_i$ and $X_i$ are the Pauli-Z and Pauli-X operators acting on qubit $i$.\n",
    "- $\\mathcal{W}_{ij}$ are learnable parameters associated with the edges, representing interaction strengths between qubits \\( i \\) and \\( j \\).\n",
    "- $\\mathcal{M}_i$ are learnable parameters associated with the nodes, representing the weights or biases of individual qubits.\n",
    "- $\\boldsymbol{\\theta} = \\{\\mathcal{W}, \\mathcal{M}\\}$ is the set of all learnable parameters.\n",
    "\n",
    "In this Hamiltonian:\n",
    "\n",
    "- The **first term** $\\sum_{(i, j) \\in E} \\mathcal{W}_{ij} Z_i Z_j$ models the interactions between connected nodes (edges) through coupled Pauli-Z operators. This term captures the pairwise relationships and correlations between qubits corresponding to connected nodes/particles in the graph.\n",
    "- The **second term** $\\sum_i \\mathcal{M}_i Z_i$ represents the individual contributions of the nodes (qubits) to the system's energy. Each $\\mathcal{M}_i$ is a learnable parameter that can encode node-specific features or biases, effectively measuring the energy of individual qubits.\n",
    "- The **third term** $\\sum_i X_i$ introduces quantum fluctuations via the Pauli-X operators. This transverse field term allows for quantum tunneling between states, enabling the exploration of the energy landscape and capturing quantum correlations.\n",
    "\n",
    "By finding the ground state of this Hamiltonian, one can obtain a state that encodes optimal representations of the graph for the task at hand. Implementing this Hamiltonian on a digital quantum computer involves constructing an associated quantum circuit, which is generally achieved by using the Trotter-Suzuki decomposition to approximate the time evolution operator.\n",
    "\n",
    "However, in our project, we propose an alternative approach that avoids encoding the entire graph into a Hamiltonian. Instead, we build upon the **LorentzNet** architecture, a message-passing (classical) neural network specifically designed for applications in particle physics that incorporates Lorentz symmetry into its structure. We enhance this model by integrating parameterized quantum circuits to capture quantum effects, aiming to leverage quantum computing's potential to model complex patterns and correlations in the data. As we'll see, this will allow our variational circuits to ingest invariant features, which then also leads to invariance and equivariance.\n",
    "\n",
    "We introduce three parameterized quantum circuits—$\\phi_m, \\phi_x,$ and $\\phi_h$—each designed to learn specific components of the network:\n",
    "\n",
    "1. **Message Passing Function Between Nodes $\\phi_m$**:\n",
    "   - This circuit learns the messages that are passed along the edges of the graph, effectively capturing the interactions between nodes.\n",
    "   - It updates the edge features based on the node features it connects, allowing for the aggregation of information from neighboring nodes.\n",
    "\n",
    "2. **Deeper Quantum Coordinate Embeddings $|\\psi_{x}^{l+1}\\rangle$**:\n",
    "   - The circuit $\\phi_x$ updates the coordinate embeddings of the nodes, producing quantum states that represent node features in higher-dimensional Hilbert spaces at layer $ l+1$.\n",
    "   - The intuition here is that we want the model to capture more complex geometric and spatial relationships inherent in particle interactions.\n",
    "   - Initially, that is, in layer $l=0$, the coordinate embeddings are the inputs represented by four-momentum vectors.\n",
    "\n",
    "3. **Deeper Scalar Embeddings $|\\psi_{h}^{l+1}\\rangle$**:\n",
    "   - The circuit $\\phi_h$ updates the scalar embeddings associated with the nodes, providing richer representations of node attributes in the quantum regime at layer $l+1$.\n",
    "   - Scalar features might include properties like energy or charge, which are crucial for particle classification tasks.\n",
    "   - At layer $l=0$, these scalar embeddings are just particle scalars, like color, charge and ID.\n",
    "\n",
    "By integrating these parameterized quantum circuits into the LorentzNet framework, our model performs quantum-enhanced message passing and feature transformation. This hybrid architecture combines the strengths of classical GNNs with the potential advantages of quantum computing, such as representing and processing information in exponentially larger state spaces. We aim to improve performance in distinguishing between quark and gluon jets by capturing subtler patterns and correlations that might be challenging for classical methods alone.\n",
    "\n",
    "In summary, our approach leverages quantum computing within a graph neural network framework tailored for particle physics data. By not encoding the entire graph into a Hamiltonian but instead focusing on quantum-enhanced components within the LorentzNet architecture, we strive to balance computational feasibility with the potential benefits of quantum processing. Our parameterized quantum circuits $\\phi_m, \\phi_x,$ and $\\phi_h$ serve as key elements in this endeavor, each contributing to the model's ability to learn and represent the complex structures within our dataset effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define our parameterized circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "\n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)\n",
    "\n",
    "\n",
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.q_depth = q_depth\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # Quantum Embedding (U(X))\n",
    "        q_in = torch.tanh(input_features) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, self.n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        # for batch in q_in:\n",
    "        for elem in q_in:\n",
    "            # print(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits))\n",
    "            q_out_elem = torch.hstack(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits)).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the batch measurement of the PQC\n",
    "        return q_out.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]])\n",
      "torch.Size([16]) torch.Size([16, 139, 4]) torch.Size([16, 139, 8]) torch.Size([16, 139]) torch.Size([16, 139, 139]) torch.Size([28736]) torch.Size([28736])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import energyflow\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "\n",
    "# we define a function to return an adjacencyy matrix\n",
    "# for our graph data representing the jets.\n",
    "def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "    rows, cols = [], []\n",
    "    # print(edge_mask[0])\n",
    "    # raise\n",
    "    for batch_idx in range(batch_size):\n",
    "        nn = batch_idx*n_nodes\n",
    "        x = coo_matrix(edge_mask[batch_idx])\n",
    "        rows.append(nn + x.row)\n",
    "        cols.append(nn + x.col)\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "\n",
    "    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n",
    "    return edges\n",
    "\n",
    "def collate_fn(data):\n",
    "    data = list(zip(*data)) # label p4s nodes atom_mask\n",
    "    data = [torch.stack(item) for item in data]\n",
    "    batch_size, n_nodes, _ = data[1].size()\n",
    "    atom_mask = data[-1]\n",
    "    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n",
    "    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "    edge_mask *= diag_mask\n",
    "    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n",
    "    return data + [edge_mask, edges]\n",
    "\n",
    "def retrieve_dataloaders(batch_size, num_data = -1, use_one_hot = True, cache_dir = './data', num_workers=4):\n",
    "    raw = energyflow.qg_jets.load(num_data=num_data, pad=True, ncol=4, generator='pythia',\n",
    "                            with_bc=False, cache_dir=cache_dir)\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = {type:{'raw':None,'label':None} for type in splits}\n",
    "    (data['train']['raw'],  data['val']['raw'],   data['test']['raw'],\n",
    "    data['train']['label'], data['val']['label'], data['test']['label']) = \\\n",
    "        energyflow.utils.data_split(*raw, train=0.8, val=0.1, test=0.1, shuffle = False)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore').fit([[11],[13],[22],[130],[211],[321],[2112],[2212]])\n",
    "    \n",
    "    for split, value in data.items():\n",
    "        pid = torch.from_numpy(np.abs(np.asarray(value['raw'][...,3], dtype=int))).unsqueeze(-1)\n",
    "        p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(value['raw'],error_on_unknown=True))\n",
    "        one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n",
    "        one_hot = torch.from_numpy(one_hot)\n",
    "        mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n",
    "        charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n",
    "        \n",
    "        if use_one_hot:\n",
    "            nodes = one_hot\n",
    "            \n",
    "        else:\n",
    "            nodes = torch.cat((mass,charge),dim=-1)\n",
    "\n",
    "            nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n",
    "\n",
    "                \n",
    "        atom_mask = (pid[...,0] != 0)\n",
    "        \n",
    "        value['p4s'] = p4s\n",
    "        value['nodes'] = nodes\n",
    "        value['label'] = torch.from_numpy(value['label'])\n",
    "        value['atom_mask'] = atom_mask.to(torch.bool)\n",
    "        \n",
    "        if split == 'train':\n",
    "            print(value['atom_mask'])\n",
    "\n",
    "    datasets = {split: TensorDataset(value['label'], value['p4s'],\n",
    "                                     value['nodes'], value['atom_mask'])\n",
    "                for split, value in data.items()}\n",
    "\n",
    "    # distributed training\n",
    "    # train_sampler = DistributedSampler(datasets['train'], shuffle=True)\n",
    "    # Construct PyTorch dataloaders from datasets\n",
    "    dataloaders = {split: DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n",
    "                                     pin_memory=False,\n",
    "                                     # persistent_workers=True,\n",
    "                                     drop_last=True if (split == 'train') else False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     collate_fn=collate_fn)\n",
    "                        for split, dataset in datasets.items()}\n",
    "\n",
    "    return dataloaders #train_sampler, dataloaders\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # train_sampler, dataloaders = retrieve_dataloaders(32, 100)\n",
    "    dataloaders = retrieve_dataloaders(16, 20)\n",
    "    for (label, p4s, nodes, atom_mask, edge_mask, edges) in dataloaders['train']:\n",
    "        print(label.shape, p4s.shape, nodes.shape, atom_mask.shape,\n",
    "              edge_mask.shape, edges[0].shape, edges[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 139, 4])\n",
      "torch.Size([16, 139, 8])\n",
      "torch.Size([16, 139])\n",
      "torch.Size([16, 139, 139])\n"
     ]
    }
   ],
   "source": [
    "print(p4s.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 139])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symmetric (undirected graph)\n",
    "\n",
    "atom_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 60])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 180])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "    rows, cols = [], []\n",
    "    for batch_idx in range(batch_size):\n",
    "        nn = batch_idx*n_nodes\n",
    "        x = coo_matrix(edge_mask[batch_idx])\n",
    "        rows.append(nn + x.row)\n",
    "        cols.append(nn + x.col)\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "\n",
    "    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n",
    "    return edges\n",
    "\n",
    "def collate_fn(data):\n",
    "    data = list(zip(*data)) # label p4s nodes atom_mask\n",
    "    data = [torch.stack(item) for item in data]\n",
    "    batch_size, n_nodes, _ = data[1].size()\n",
    "    atom_mask = data[-1]\n",
    "    # edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n",
    "    # diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "    # edge_mask *= diag_mask\n",
    "\n",
    "    edge_mask = data[-2]\n",
    "\n",
    "    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n",
    "    return data + [edges]\n",
    "\n",
    "\n",
    "p4s = torch.load('Roy/data/p4s.pt')\n",
    "nodes = torch.load('Roy/data/nodes.pt')\n",
    "labels = torch.load('Roy/data/labels.pt')\n",
    "atom_mask = torch.load('Roy/data/atom_mask.pt')\n",
    "edge_mask = torch.from_numpy(np.load('Roy/data/edge_mask.npy'))\n",
    "edges = torch.from_numpy(np.load('Roy/data/edges.npy'))\n",
    "\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate the lengths for each split\n",
    "total_size = len(dataset_all)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_all, [train_size, val_size, test_size])\n",
    "\n",
    "# Create a dictionary to hold the datasets\n",
    "datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n",
    "\n",
    "dataloaders = {split: DataLoader(dataset,\n",
    "                                 batch_size=16,\n",
    "                                 # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n",
    "                                 pin_memory=False,\n",
    "                                 # persistent_workers=True,\n",
    "                                 collate_fn = collate_fn,\n",
    "                                 drop_last=True if (split == 'train') else False,\n",
    "                                 num_workers=0)\n",
    "                    for split, dataset in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1]),\n",
       " tensor([[[ 0.0510,  0.4339,  0.5472,  0.5168],\n",
       "          [-0.0619,  0.2719,  0.3645,  0.3418],\n",
       "          [-0.0521,  0.2184,  0.2930,  0.2747]],\n",
       " \n",
       "         [[ 0.2931,  0.2831, -0.2194,  0.2539],\n",
       "          [ 0.1917,  0.1846, -0.1419,  0.1649],\n",
       "          [ 0.1467,  0.1419, -0.1088,  0.1265]],\n",
       " \n",
       "         [[ 0.2834, -0.1486, -0.0193,  0.1235],\n",
       "          [ 0.2280, -0.1222, -0.0152,  0.1000],\n",
       "          [ 0.1812, -0.1030, -0.0113,  0.0809]],\n",
       " \n",
       "         [[ 1.0000, -0.7343,  1.0000,  1.0000],\n",
       "          [ 0.1595, -0.1188,  0.1612,  0.1610],\n",
       "          [ 0.1382, -0.0996,  0.1375,  0.1374]],\n",
       " \n",
       "         [[-0.2010, -0.0383, -0.0326,  0.0793],\n",
       "          [-0.1457, -0.0260, -0.0241,  0.0575],\n",
       "          [-0.1455, -0.0258, -0.0233,  0.0571]],\n",
       " \n",
       "         [[ 0.4913, -0.0570,  0.4328,  0.4165],\n",
       "          [ 0.3204, -0.0389,  0.2890,  0.2770],\n",
       "          [ 0.2803, -0.0322,  0.2448,  0.2360]],\n",
       " \n",
       "         [[ 0.1178, -0.3572, -0.2832,  0.2993],\n",
       "          [ 0.0994, -0.2857, -0.2264,  0.2396],\n",
       "          [ 0.0829, -0.2617, -0.2044,  0.2169]],\n",
       " \n",
       "         [[ 0.6492, -0.0221,  0.0054,  0.2326],\n",
       "          [ 0.2552, -0.0119,  0.0016,  0.0915],\n",
       "          [ 0.2127, -0.0079,  0.0018,  0.0763]],\n",
       " \n",
       "         [[-0.5615,  1.0000,  0.0068,  0.5024],\n",
       "          [-0.2685,  0.4788,  0.0039,  0.2405],\n",
       "          [-0.0634,  0.1114,  0.0017,  0.0561]],\n",
       " \n",
       "         [[-1.0656,  0.4920,  0.4353,  0.5834],\n",
       "          [-0.5835,  0.2721,  0.2396,  0.3207],\n",
       "          [-0.1545,  0.0712,  0.0629,  0.0845]]], dtype=torch.float64),\n",
       " tensor([[[0.1485],\n",
       "          [0.0000],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.5254],\n",
       "          [0.1485],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.5296],\n",
       "          [0.0000],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.5296],\n",
       "          [0.1485],\n",
       "          [1.0000]],\n",
       " \n",
       "         [[0.9986],\n",
       "          [0.1485],\n",
       "          [0.1485]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.0000],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.1485],\n",
       "          [0.1485]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.1485],\n",
       "          [0.5254]],\n",
       " \n",
       "         [[0.9986],\n",
       "          [1.0000],\n",
       "          [0.1485]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.1485],\n",
       "          [0.0000]]], dtype=torch.float64),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]]]),\n",
       " [tensor([ 0,  0,  0,  3,  3,  3,  6,  6,  6,  9,  9,  9, 12, 12, 12, 15, 15, 15,\n",
       "          18, 18, 18, 21, 21, 21, 24, 24, 24, 27, 27, 27]),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])]]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can peek at a batch to see what it looks like.\n",
    "next(iter(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12500, 3, 4])\n",
      "torch.Size([12500, 3, 1])\n",
      "torch.Size([12500, 3])\n",
      "torch.Size([12500, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(p4s.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fc61cfc0dc0>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x7fc61cfc0760>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7fc61cfc0610>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, n_nodes...\n",
    "# 1         , 139...\n",
    "\n",
    "# labels: torch.Size([1])\n",
    "# p4s: torch.Size([1, 139, 4]) : 4-momentum\n",
    "# nodes: torch.Size([1, 139, 8]) : 8 is a one-hot vector of particle id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1#2500 #1\n",
    "n_nodes = 3 #139 \n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "atom_positions = p4s[:, :, :].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "\n",
    "atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n",
    "\n",
    "edges = [a.to(device) for a in edges]\n",
    "nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([139, 64])\n",
      "torch.Size([139, 128])\n",
      "torch.Size([139, 16])\n",
      "torch.Size([19321, 16])\n"
     ]
    }
   ],
   "source": [
    "print(atom_positions.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "print(atom_positions.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_mask[0]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0633, -0.0332,  0.0849,  0.0766], dtype=torch.float64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4s[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([139, 16])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 139, 4])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4s.shape # batch_size (number of jets or graphs), n_nodes (particles), n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom mask: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Atom positions (x features, 4-momenta): tensor([[ 2.8607e-01,  7.7928e-03, -2.6866e-01,  9.7974e-02,  1.6533e-01,\n",
      "         -2.5844e-02, -1.5798e-01, -4.1364e-02,  1.1594e+00, -2.3781e-01,\n",
      "         -1.1238e+00, -7.2296e-02,  4.2239e+00,  2.2232e-01, -4.1256e+00,\n",
      "          7.2634e-01,  1.7402e+00,  1.4311e-01, -1.6899e+00, -3.6378e-01,\n",
      "          2.1967e+00, -2.9915e-01, -2.1732e+00, -1.1518e-01,  1.6228e+00,\n",
      "         -1.0804e-01, -1.6155e+00, -1.0957e-01,  6.6001e+00,  3.5305e-01,\n",
      "         -6.5827e+00,  2.9199e-01,  3.8065e+00,  1.5963e-01, -3.7676e+00,\n",
      "          1.6072e-01,  1.3488e+01,  3.1063e-01, -1.3478e+01, -3.7756e-01,\n",
      "          4.1091e+00,  1.9105e-01, -4.1035e+00, -9.7634e-02,  2.1653e+01,\n",
      "          1.0296e+00, -2.1621e+01, -5.8444e-01,  6.7785e+00,  3.3111e-01,\n",
      "         -6.7674e+00, -2.0163e-01,  1.3265e+01,  4.9158e-01, -1.3246e+01,\n",
      "         -5.2290e-01,  2.9855e+00,  1.0084e-01, -2.9818e+00, -1.0923e-01,\n",
      "          3.7398e+01,  1.5851e+00, -3.7341e+01, -1.3249e+00],\n",
      "        [ 3.3558e+02,  1.2900e+01, -3.3515e+02, -1.0923e+01,  4.4238e+01,\n",
      "          1.8533e+00, -4.4175e+01, -1.4715e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "Nodes (scalars: mass & charge): tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n",
      "Edge mask: tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]])\n",
      "Edges: [tensor([   0,    0,    0,  ..., 2114, 2114, 2114]), tensor([   1,    2,    3,  ..., 2111, 2112, 2113])]\n"
     ]
    }
   ],
   "source": [
    "# Roy: x(atom_pos), edge_indx_tensor (edges = adj_matrix), edge_tensor (edge_mask = adj_matrix)\n",
    "print(\"Atom mask: {}\".format(atom_mask[:2]))\n",
    "print(\"Atom positions (x features, 4-momenta): {}\".format(atom_positions[:2]))\n",
    "print(\"Nodes (scalars: mass & charge): {}\".format(nodes[:2]))\n",
    "print(\"Edge mask: {}\".format(edge_mask[:2]))\n",
    "print(\"Edges: {}\".format(edges[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,    0,    0,  ..., 2114, 2114, 2114]),\n",
       " tensor([   1,    2,    3,  ..., 2111, 2112, 2113])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[:2]#[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                         edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LorentzNet\n",
    "Before delving into the realm of quantum graph neural networks (QGNNs), we shall examine the performance and structure of a very well-known equivariant GNN, **LorentzNet** ([arXiv:2201.08187](https://arxiv.org/abs/2201.08187)), which is classical, on our dataset. Understanding the structure underlying LorentzNet will allow us to understand where to fit in our quantum models, and this will be the heart of our approach.\n",
    "\n",
    "## 3.1. Dataset Representation as Graphs\n",
    "\n",
    "We already discussed this in the introduction, but again, let's remmber that in high-energy particle physics, **jets**—collimated streams of particles resulting from particle collisions—are complex objects that can be naturally represented as graphs. In our dataset:\n",
    "\n",
    "- Each **jet** is modeled as a graph \\( G = (V, E) \\), where:\n",
    "  - $V$ is the set of **nodes**, each corresponding to a constituent particle within the jet.\n",
    "  - $E$ is the set of **edges**, representing interactions or relationships between particles.\n",
    "- Each node (particle) is considered a point in Minkowski space $\\mathbb{R}^{1,3}$, respecting the spacetime symmetries of special relativity.\n",
    "- The number of particles (nodes) varies for each jet, reflecting the stochastic nature of particle collisions.\n",
    "\n",
    "**Reconstructing Four-Momentum Vectors**\n",
    "\n",
    "In practice, particle data may not be directly provided as four-momentum vectors. Instead, they are often given in terms of:\n",
    "\n",
    "- **Transverse Momentum $p_T$**: Momentum perpendicular to the beam axis.\n",
    "- **Pseudo-rapidity $\\eta$**: A spatial coordinate describing the angle of a particle relative to the beam (forward-backward) direction.\n",
    "- **Azimuthal Angle $\\phi$**: Angle around the beam axis in the transverse plane.\n",
    "- **Particle Identification (PID)**: Integer codes representing particle types.\n",
    "\n",
    "**Conversion to Four-Momentum**\n",
    "\n",
    "- Using the relationships:\n",
    "\n",
    "  - $p_x = p_T \\cos\\phi$\n",
    "  - $p_y = p_T \\sin\\phi$\n",
    "  - $p_z = p_T \\sinh\\eta$\n",
    "  - $E = \\sqrt{p_T^2 \\cosh^2\\eta + m^2}$, where $m$ is the particle mass.\n",
    "\n",
    "- The **[EnergyFlow](https://energyflow.network/)** package converts this for us.\n",
    "\n",
    "**Implementation in Code**:\n",
    "\n",
    "- The first step in the data preprocessing involves reconstructing the four-momentum vectors using the available kinematic variables, which is fundamental for us, since:\n",
    "    - First, we want to ensure that the input to LorentzNet is correctly formatted and physically meaningful.\n",
    "    - Also, given the limitations on current quantum hardware, and since we are performing simulations currently, then the number of particles in the jet has to be cut down.\n",
    "\n",
    "## 3.2. Architecture Overview\n",
    "\n",
    "The **LorentzNet** architecture is designed to process and analyze graphs while respecting the **Lorentz symmetry**, a fundamental symmetry in relativistic physics involving rotations and boosts in spacetime (changes in inertial frames).\n",
    "\n",
    "**Key Features of LorentzNet**:\n",
    "\n",
    "- Built upon the **universal approximation theorem** for **Lorentz-equivariant functions**. This theorem ensures that the network can approximate any Lorentz-equivariant function to arbitrary precision, given sufficient capacity.\n",
    "- Incorporates **message passing** mechanisms tailored to respect Lorentz symmetry.\n",
    "- Utilizes **continuous functions** modeled by neural networks to update node and edge features throughout the network layers.\n",
    "\n",
    "**Architecture Diagram**:\n",
    "\n",
    "<center>\n",
    "<img src=\"../figures/LorentzNet.png\" width=\"65%\" style=\"margin-left:auto; margin-right:auto\">\n",
    "</center>\n",
    "\n",
    "*(Figure: Schematic representation of the LorentzNet architecture.)*\n",
    "\n",
    "**Input Layer**\n",
    "\n",
    "The **input** to the LorentzNet consists of:\n",
    "\n",
    "- **Four-momentum vectors** (coordinate embeddings) of particles from collision events.\n",
    "  - Each particle $i$ has a four-momentum $v_i = (E_i, p_{x_i}, p_{y_i}, p_{z_i})$, where:\n",
    "    - $E_i$ is the energy.\n",
    "    - $p_{x_i}, p_{y_i}, p_{z_i}$ are momentum components in three-dimensional space.\n",
    "- **Scalar features** (scalar embeddings) $s_i$ associated with each particle, such as:\n",
    "  - Mass.\n",
    "  - Electric charge.\n",
    "  - Particle identification (PID) codes.\n",
    "\n",
    "The combined feature vector for each particle is:\n",
    "\n",
    "$$\n",
    "f_i = v_i \\oplus s_i,\n",
    "$$\n",
    "\n",
    "where $\\oplus$ denotes concatenation.\n",
    "\n",
    "**Lorentz Group Equivariant Block (LGEB)**\n",
    "\n",
    "At the core of LorentzNet is the **Lorentz Group Equivariant Block (LGEB)**, which updates the features of particles (nodes) and their interactions (edges) while preserving Lorentz equivariance.\n",
    "\n",
    "**The components of LGEB**:\n",
    "\n",
    "1. **Edge Message Function $\\phi_e$**:\n",
    "   - Computes messages passed between particles.\n",
    "   - Captures pairwise interactions and relativistic geometrical relationships.\n",
    "\n",
    "2. **Coordinate Update Function $\\phi_x$**:\n",
    "   - Updates the coordinate embeddings of particles.\n",
    "   - Incorporates attention mechanisms respecting Minkowski spacetime.\n",
    "\n",
    "3. **Scalar Feature Update Function $\\phi_h$**:\n",
    "   - Updates scalar features of particles.\n",
    "   - Aggregates information from neighboring particles.\n",
    "\n",
    "These functions are modeled using neural networks capable of approximating continuous functions.\n",
    "\n",
    "## 3.3. Detailed Formulation\n",
    "\n",
    "1. **Edge Message Computation $\\phi_e$**:\n",
    "\n",
    "   For particles $i$ and $j$ at layer $l$, the **edge message** $m_{ij}^{l}$ is computed as:\n",
    "\n",
    "   $$\n",
    "   m_{ij}^{l} = \\phi_e \\left( h_i^{l}, h_j^{l}, \\psi\\left( \\| x_i^{l} - x_j^{l} \\|^2 \\right), \\psi\\left( \\langle x_i^{l}, x_j^{l} \\rangle \\right) \\right),\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   - $h_i^{l}$ and $h_j^{l}$ are the scalar features of particles $i$ and $j$ at layer $l$.\n",
    "   - $x_i^{l}$ and $x_j^{l}$ are the coordinate embeddings (four-vectors) at layer $l$.\n",
    "   - $\\| x_i^{l} - x_j^{l} \\|^2$ is the squared Minkowski **distance** between particles $i$ and $j$.\n",
    "   - $\\langle x_i^{l}, x_j^{l} \\rangle$ is the Minkowski **inner product** (Lorentz dot product).\n",
    "   - $\\psi(\\cdot)$ is a normalization function defined as:\n",
    "\n",
    "     $$\n",
    "     \\psi(a) = \\operatorname{sgn}(a) \\cdot \\log\\left( |a| + 1 \\right),\n",
    "     $$\n",
    "\n",
    "     with $\\operatorname{sgn}(a)$ being the sign function.\n",
    "\n",
    "   **Purpose of $\\psi(\\cdot)$**:\n",
    "\n",
    "   - Helps normalize values that may have large magnitudes or come from different distributions.\n",
    "   - Ensures numerical stability during optimization by mapping inputs to a manageable range.\n",
    "\n",
    "\n",
    "2. **Coordinate Embedding Update $\\phi_x$**:\n",
    "\n",
    "   The **coordinate embeddings** of particles are updated via:\n",
    "\n",
    "   $$\n",
    "   x_i^{l+1} = x_i^{l} + c \\sum_{j \\in \\mathcal{N}(i)} \\phi_x ( m_{ij}^{l}) \\cdot x_j^{l},\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   - $\\mathcal{N}(i)$ denotes the **neighborhood** of particle $i$, i.e., particles connected to $i$ in the graph.\n",
    "   - $c$ is a scaling constant controlling the update magnitude.\n",
    "   - $\\phi_x ( m_{ij}^{l})$ computes an **attention weight** based on the edge message $m_{ij}^{l}$.\n",
    "\n",
    "   **Interpretation**:\n",
    "\n",
    "   - The update adds a weighted sum of neighboring coordinate embeddings $x_j^{l}$ to the current embedding $x_i^{l}$.\n",
    "   - This mechanism allows particles to incorporate spatial information from their neighbors, guided by the learned attention weights.\n",
    "\n",
    "\n",
    "3. **Scalar Feature Update $\\phi_h$**:\n",
    "\n",
    "   The **scalar features** are updated as:\n",
    "\n",
    "   $$\n",
    "   h_i^{l+1} = h_i^{l} + \\phi_h \\left( h_i^{l}, \\sum_{j \\in \\mathcal{N}(i)} w_{ij}^{l} m_{ij}^{l} \\right),\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   - $w_{ij}^{l}$ is an **edge significance weight** calculated by:\n",
    "\n",
    "     $$\n",
    "     w_{ij}^{l} = \\phi_m \\left( m_{ij}^{l} \\right) \\in [0, 1],\n",
    "     $$\n",
    "\n",
    "     with $\\phi_m$ being a neural network outputting values in the range [0, 1].\n",
    "\n",
    "   - $\\phi_h$ aggregates information from neighboring particles to update $h_i^{l}$.\n",
    "\n",
    "   And for the Purpose of $w_{ij}^{l}$ and $\\phi_h$:\n",
    "\n",
    "   - $w_{ij}^{l}$ signifies the importance of the edge between particles $i$ and $j$.\n",
    "   - $\\phi_h$ integrates these weighted messages to refine the scalar features, enabling the network to learn complex interactions.\n",
    "\n",
    "\n",
    "**Avoiding Redundancy**\n",
    "\n",
    "A noteworthy aspect of LorentzNet is its approach to handling outputs:\n",
    "\n",
    "- Although both **coordinate embeddings** $x_i^{l}$ and **scalar features** $h_i^{l}$ are updated through the layers, the final output only uses the **scalar features** $h_i^{L}$ from the last layer $L$.\n",
    "- This strategy reduces redundancy and computational overhead because:\n",
    "\n",
    "  - The edge messages $m_{ij}^{l}$ already incorporate information from both $x_i^{l}$ and $x_j^{l}$.\n",
    "  - Focusing on scalar features simplifies the network output without losing critical information.\n",
    "\n",
    "      \n",
    "**Implementation Details**\n",
    "\n",
    "To ensure fidelity with the original LorentzNet architecture and leverage existing optimizations, we utilize the official implementation provided by the authors:\n",
    "\n",
    "- **Repository**: [LorentzNet-release](https://github.com/sdogsq/LorentzNet-release/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Some auxiliary functions\"\"\"\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n",
    "    Adapted from https://github.com/vgsatorras/egnn.\n",
    "    '''\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n",
    "    Adapted from https://github.com/vgsatorras/egnn.\n",
    "    '''\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    count = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    count.index_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "def normsq4(p):\n",
    "    r''' Minkowski square norm\n",
    "         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n",
    "    ''' \n",
    "    psq = torch.pow(p, 2)\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "    \n",
    "def dotsq4(p,q):\n",
    "    r''' Minkowski inner product\n",
    "         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n",
    "    '''\n",
    "    psq = p*q\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "\n",
    "def normA_fn(A):\n",
    "    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n",
    "\n",
    "def dotA_fn(A):\n",
    "    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n",
    "    \n",
    "def psi(p):\n",
    "    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n",
    "    '''\n",
    "    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n",
    "\n",
    "\n",
    "\"\"\"Lorentz Group-Equivariant Block\"\"\"\n",
    "\n",
    "class LGEB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n",
    "        super(LGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.include_x = include_x\n",
    "        self.phi_e = nn.Sequential(\n",
    "            nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "        self.A = A\n",
    "        self.norm_fn = normA_fn(A) if A is not None else normsq4\n",
    "        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n",
    "        \n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        # print(\"m_model output: \", out.shape)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n",
    "        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m): # norms\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        # print(\"m: \", m.shape)\n",
    "        # print(\"trans: \", trans.shape)\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        # print(\"trans: \", trans.shape)\n",
    "        # print(\"x.size: \", x.size(0))\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = self.norm_fn(x_diff).unsqueeze(1)\n",
    "        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        if self.include_x:\n",
    "            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n",
    "        else:\n",
    "            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            # print(\"X: \", x)\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "            # print(\"phi_x(X) = \", x, '\\n---\\n')\n",
    "            \n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class LorentzNet(nn.Module):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of LGEB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n",
    "        super(LorentzNet, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "\n",
    "        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n",
    "        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "            \n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "\n",
    "        # print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n",
    "        return pred.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the official code for the classical, just for sanity checking, let's test for equivariance\n",
    "\n",
    "The cell below is just an auxiliary function to give us the boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Speed of light (m/s)\n",
    "c = 299792458\n",
    "\n",
    "\"\"\"Lorentz transformations describe the transition between two inertial reference\n",
    "frames F and F', each of which is moving in some direction with respect to the\n",
    "other. This code only calculates Lorentz transformations for movement in the x\n",
    "direction with no spatial rotation (i.e., a Lorentz boost in the x direction).\n",
    "The Lorentz transformations are calculated here as linear transformations of\n",
    "four-vectors [ct, x, y, z] described by Minkowski space. Note that t (time) is\n",
    "multiplied by c (the speed of light) in the first entry of each four-vector.\n",
    "\n",
    "Thus, if X = [ct; x; y; z] and X' = [ct'; x'; y'; z'] are the four-vectors for\n",
    "two inertial reference frames and X' moves in the x direction with velocity v\n",
    "with respect to X, then the Lorentz transformation from X to X' is X' = BX,\n",
    "where\n",
    "\n",
    "    | γ  -γβ  0  0|\n",
    "B = |-γβ  γ   0  0|\n",
    "    | 0   0   1  0|\n",
    "    | 0   0   0  1|\n",
    "\n",
    "is the matrix describing the Lorentz boost between X and X',\n",
    "γ = 1 / √(1 - v²/c²) is the Lorentz factor, and β = v/c is the velocity as\n",
    "a fraction of c.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def beta(velocity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates β = v/c, the given velocity as a fraction of c\n",
    "    >>> beta(c)\n",
    "    1.0\n",
    "    >>> beta(199792458)\n",
    "    0.666435904801848\n",
    "    \"\"\"\n",
    "    if velocity > c:\n",
    "        raise ValueError(\"Speed must not exceed light speed 299,792,458 [m/s]!\")\n",
    "    elif velocity < 1:\n",
    "        # Usually the speed should be much higher than 1 (c order of magnitude)\n",
    "        raise ValueError(\"Speed must be greater than or equal to 1!\")\n",
    "\n",
    "    return velocity / c\n",
    "\n",
    "\n",
    "def gamma(velocity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Lorentz factor γ = 1 / √(1 - v²/c²) for a given velocity\n",
    "    >>> gamma(4)\n",
    "    1.0000000000000002\n",
    "    >>> gamma(1e5)\n",
    "    1.0000000556325075\n",
    "    >>> gamma(3e7)\n",
    "    1.005044845777813\n",
    "    >>> gamma(2.8e8)\n",
    "    2.7985595722318277\n",
    "    \"\"\"\n",
    "    return 1 / sqrt(1 - beta(velocity) ** 2)\n",
    "\n",
    "\n",
    "def transformation_matrix(velocity: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Lorentz transformation matrix for movement in the x direction:\n",
    "\n",
    "    | γ  -γβ  0  0|\n",
    "    |-γβ  γ   0  0|\n",
    "    | 0   0   1  0|\n",
    "    | 0   0   0  1|\n",
    "\n",
    "    where γ is the Lorentz factor and β is the velocity as a fraction of c\n",
    "    >>> transformation_matrix(29979245)\n",
    "    array([[ 1.00503781, -0.10050378,  0.        ,  0.        ],\n",
    "           [-0.10050378,  1.00503781,  0.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  1.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [\n",
    "            [gamma(velocity), -gamma(velocity) * beta(velocity), 0, 0],\n",
    "            [-gamma(velocity) * beta(velocity), gamma(velocity), 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_scalar = 8 in original !\n",
    "model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.99137795  0.67032087  0.0894613  -0.5694246 ]\n",
      "h after (just the first particle): \n",
      " [-2.791172    1.3770922  -0.21466088  0.10523695]\n",
      "Final preds: \n",
      " [[0.07923214 0.10442689]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 0.6250086  -0.4157089   0.19434586  3.7227166 ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... taking any random nonsense transformation in the four-momentum vectors\n",
    "i.e.: multiplying by 0.1. Does the hidden rep stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 1.5326474  -0.09580445 -0.10811514  4.131195  ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.99137795  0.67032087  0.0894613  -0.5694246 ]\n",
      "h after (just the first particle): \n",
      " [-3.3101714  0.5831776 -2.4134033 -1.6305795]\n",
      "Final preds: \n",
      " [[-0.49384588 -0.16958103]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though the final logits in this case wasn't different, if we look the last output of h (which contains both scalar and 4-momenta information), it changed! Now, what about Lorentz transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 0.6251303  -0.41550016  0.1935861   3.721067  ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.99137795  0.67032087  0.0894613  -0.5694246 ]\n",
      "h after (just the first particle): \n",
      " [-2.791169    1.37709    -0.21465892  0.10523733]\n",
      "Final preds: \n",
      " [[-0.49874073 -0.1710372 ]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivariance works. Finally, let's train on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 179\n",
      " train samples: 10000\n",
      " val samples: 1250\n",
      " test samples: 1250\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:04, 125.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 1/60 \t Batch 624/625 \t Loss 0.6971 \t Running Acc 0.503 \t Total Acc 0.503 \t Avg Batch Time 0.0080\n",
      "Time: train: 4.98 \t Train loss 0.6971 \t Train acc: 0.5028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 268.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6919 \t Running Acc 3.911 \t Total Acc 0.494 \t Avg Batch Time 0.0005\n",
      "New best validation model, saving...\n",
      "Epoch 0/60 finished.\n",
      "Train time: 4.98 \t Val time 0.30\n",
      "Train loss 0.6971 \t Train acc: 0.5028\n",
      "Val loss: 0.6914 \t Val acc: 0.4944\n",
      "Best val acc: 0.4944 at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:04, 136.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 2/60 \t Batch 624/625 \t Loss 0.6845 \t Running Acc 0.567 \t Total Acc 0.567 \t Avg Batch Time 0.0073\n",
      "Time: train: 4.59 \t Train loss 0.6845 \t Train acc: 0.5672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 257.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6785 \t Running Acc 4.703 \t Total Acc 0.594 \t Avg Batch Time 0.0005\n",
      "New best validation model, saving...\n",
      "Epoch 1/60 finished.\n",
      "Train time: 4.59 \t Val time 0.31\n",
      "Train loss 0.6845 \t Train acc: 0.5672\n",
      "Val loss: 0.6770 \t Val acc: 0.5944\n",
      "Best val acc: 0.5944 at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:03, 167.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 3/60 \t Batch 624/625 \t Loss 0.6682 \t Running Acc 0.595 \t Total Acc 0.595 \t Avg Batch Time 0.0060\n",
      "Time: train: 3.72 \t Train loss 0.6682 \t Train acc: 0.5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 285.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6489 \t Running Acc 4.905 \t Total Acc 0.620 \t Avg Batch Time 0.0004\n",
      "New best validation model, saving...\n",
      "Epoch 2/60 finished.\n",
      "Train time: 3.72 \t Val time 0.28\n",
      "Train loss 0.6682 \t Train acc: 0.5952\n",
      "Val loss: 0.6467 \t Val acc: 0.6200\n",
      "Best val acc: 0.6200 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 143.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 217\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m### training and testing\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m test(model, res, model_path, log_path)\n",
      "Cell \u001b[0;32mIn[254], line 93\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, res, N_EPOCHS, model_path, log_path)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, res, N_EPOCHS, model_path, log_path):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m### training and validation\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> 93\u001b[0m         train_res \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: train: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train loss \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train acc: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# if epoch % args.val_interval == 0:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m             \n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# if (args.local_rank == 0):\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[254], line 46\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, epoch, loader, partition, N_EPOCHS)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     45\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 46\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m partition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# save labels and probilities for ROC / AUC\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# print(\"Preds \", pred)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(pred, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    178\u001b[0m         group,\n\u001b[1;32m    179\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         state_steps,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/adamw.py:419\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    416\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import json, time\n",
    "import utils_lorentz\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run(model, epoch, loader, partition, N_EPOCHS=None):\n",
    "    if partition == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    res = {'time':0, 'correct':0, 'loss': 0, 'counter': 0, 'acc': 0,\n",
    "           'loss_arr':[], 'correct_arr':[],'label':[],'score':[]}\n",
    "\n",
    "    tik = time.time()\n",
    "    loader_length = len(loader)\n",
    "\n",
    "    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in tqdm(enumerate(loader)):\n",
    "        if partition == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch_size, n_nodes, _ = p4s.size()\n",
    "        atom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "        atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device)\n",
    "        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n",
    "        nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)\n",
    "        edges = [a.to(device) for a in edges]\n",
    "        label = label.to(device, dtype).long()\n",
    "\n",
    "        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                         edge_mask=edge_mask, n_nodes=n_nodes)\n",
    "        \n",
    "        predict = pred.max(1).indices\n",
    "        correct = torch.sum(predict == label).item()\n",
    "        loss = loss_fn(pred, label)\n",
    "        \n",
    "        if partition == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif partition == 'test':\n",
    "            # save labels and probilities for ROC / AUC\n",
    "            # print(\"Preds \", pred)\n",
    "            score = torch.nn.functional.softmax(pred, dim = -1)\n",
    "            # print(\"Score test \", score)\n",
    "            # raise\n",
    "            res['label'].append(label)\n",
    "            res['score'].append(score)\n",
    "\n",
    "        res['time'] = time.time() - tik\n",
    "        res['correct'] += correct\n",
    "        res['loss'] += loss.item() * batch_size\n",
    "        res['counter'] += batch_size\n",
    "        res['loss_arr'].append(loss.item())\n",
    "        res['correct_arr'].append(correct)\n",
    "\n",
    "        # if i != 0 and i % args.log_interval == 0:\n",
    "        \n",
    "    running_loss = sum(res['loss_arr'])/len(res['loss_arr'])\n",
    "    running_acc = sum(res['correct_arr'])/(len(res['correct_arr'])*batch_size)\n",
    "    avg_time = res['time']/res['counter'] * batch_size\n",
    "    tmp_counter = res['counter']\n",
    "    tmp_loss = res['loss'] / tmp_counter\n",
    "    tmp_acc = res['correct'] / tmp_counter\n",
    "\n",
    "    if N_EPOCHS:\n",
    "        print(\">> %s \\t Epoch %d/%d \\t Batch %d/%d \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n",
    "             (partition, epoch + 1, N_EPOCHS, i, loader_length, running_loss, running_acc, tmp_acc, avg_time))\n",
    "    else:\n",
    "        print(\">> %s \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n",
    "             (partition, running_loss, running_acc, tmp_acc, avg_time))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    # ---------- reduce -----------\n",
    "    if partition == 'test':\n",
    "        res['label'] = torch.cat(res['label']).unsqueeze(-1)\n",
    "        res['score'] = torch.cat(res['score'])\n",
    "        res['score'] = torch.cat((res['label'],res['score']),dim=-1)\n",
    "    res['counter'] = res['counter']\n",
    "    res['loss'] = res['loss'] / res['counter']\n",
    "    res['acc'] = res['correct'] / res['counter']\n",
    "    return res\n",
    "\n",
    "def train(model, res, N_EPOCHS, model_path, log_path):\n",
    "    ### training and validation\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_res = run(model, epoch, dataloaders['train'], partition='train', N_EPOCHS = N_EPOCHS)\n",
    "        print(\"Time: train: %.2f \\t Train loss %.4f \\t Train acc: %.4f\" % (train_res['time'],train_res['loss'],train_res['acc']))\n",
    "        # if epoch % args.val_interval == 0:\n",
    "            \n",
    "        # if (args.local_rank == 0):\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, \"checkpoint-epoch-{}.pt\".format(epoch)) )\n",
    "        with torch.no_grad():\n",
    "            val_res = run(model, epoch, dataloaders['val'], partition='val')\n",
    "            \n",
    "        # if (args.local_rank == 0): # only master process save\n",
    "        res['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        res['train_time'].append(train_res['time'])\n",
    "        res['val_time'].append(val_res['time'])\n",
    "        res['train_loss'].append(train_res['loss'])\n",
    "        res['train_acc'].append(train_res['acc'])\n",
    "        res['val_loss'].append(val_res['loss'])\n",
    "        res['val_acc'].append(val_res['acc'])\n",
    "        res['epochs'].append(epoch)\n",
    "\n",
    "        ## save best model\n",
    "        if val_res['acc'] > res['best_val']:\n",
    "            print(\"New best validation model, saving...\")\n",
    "            torch.save(model.state_dict(), os.path.join(model_path,\"best-val-model.pt\"))\n",
    "            res['best_val'] = val_res['acc']\n",
    "            res['best_epoch'] = epoch\n",
    "\n",
    "        print(\"Epoch %d/%d finished.\" % (epoch, N_EPOCHS))\n",
    "        print(\"Train time: %.2f \\t Val time %.2f\" % (train_res['time'], val_res['time']))\n",
    "        print(\"Train loss %.4f \\t Train acc: %.4f\" % (train_res['loss'], train_res['acc']))\n",
    "        print(\"Val loss: %.4f \\t Val acc: %.4f\" % (val_res['loss'], val_res['acc']))\n",
    "        print(\"Best val acc: %.4f at epoch %d.\" % (res['best_val'],  res['best_epoch']))\n",
    "\n",
    "        json_object = json.dumps(res, indent=4)\n",
    "        with open(os.path.join(log_path, \"train-result-epoch{}.json\".format(epoch)), \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "        ## adjust learning rate\n",
    "        if (epoch < 31):\n",
    "            lr_scheduler.step(metrics=val_res['acc'])\n",
    "        else:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']*0.5\n",
    "\n",
    "\n",
    "def test(model, res, model_path, log_path):\n",
    "    ### test on best model\n",
    "    best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n",
    "    model.load_state_dict(best_model)\n",
    "    with torch.no_grad():\n",
    "        test_res = run(model, 0, dataloaders['test'], partition='test')\n",
    "\n",
    "    print(\"Final \", test_res['score'])\n",
    "    pred = test_res['score'].cpu()\n",
    "\n",
    "    np.save(os.path.join(log_path, \"score.npy\"), pred)\n",
    "    fpr, tpr, thres, eB, eS  = utils_lorentz.buildROC(pred[...,0], pred[...,2])\n",
    "    auc = utils_lorentz.roc_auc_score(pred[...,0], pred[...,2])\n",
    "\n",
    "    metric = {'test_loss': test_res['loss'], 'test_acc': test_res['acc'],\n",
    "              'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n",
    "    res.update(metric)\n",
    "    print(\"Test: Loss %.4f \\t Acc %.4f \\t AUC: %.4f \\t 1/eB 0.3: %.4f \\t 1/eB 0.5: %.4f\"\\\n",
    "           % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n",
    "    json_object = json.dumps(res, indent=4)\n",
    "    with open(os.path.join(log_path, \"test-result.json\"), \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    N_EPOCHS = 60\n",
    "\n",
    "    model_path = \"models/LorentzNet/\"\n",
    "    log_path = \"logs/LorentzNet/\"\n",
    "    # utils_lorentz.args_init(args)\n",
    "\n",
    "    ### set random seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ### initialize cuda\n",
    "    # dist.init_process_group(backend='nccl')\n",
    "    device = 'cpu' #torch.device(\"cuda\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    ### load data\n",
    "    # dataloaders = retrieve_dataloaders( batch_size,\n",
    "    #                                     num_data=100000, # use all data\n",
    "    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n",
    "    #                                     num_workers=0,\n",
    "    #                                     use_one_hot=True)\n",
    "\n",
    "    ### create parallel model\n",
    "    model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 1,\\\n",
    "                       c_weight = 1e-3)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    ### print model and dataset information\n",
    "    # if (args.local_rank == 0):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Model Size:\", pytorch_total_params)\n",
    "    for (split, dataloader) in dataloaders.items():\n",
    "        print(f\" {split} samples: {len(dataloader.dataset)}\")\n",
    "\n",
    "    ### optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "    ### lr scheduler\n",
    "    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n",
    "    lr_scheduler = utils_lorentz.GradualWarmupScheduler(optimizer, multiplier=1,\\\n",
    "                                                warmup_epoch=5,\\\n",
    "                                                after_scheduler=base_scheduler) ## warmup\n",
    "\n",
    "    ### loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### initialize logs\n",
    "    res = {'epochs': [], 'lr' : [],\\\n",
    "           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n",
    "           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n",
    "\n",
    "    ### training and testing\n",
    "    print(\"Training...\")\n",
    "    train(model, res, N_EPOCHS, model_path, log_path)\n",
    "    test(model, res, model_path, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Equivariant Quantum Neural Networks\n",
    "Now, let's move to quantum machine learning. Given some group $\\mathcal{G}$, one common way to achieve equivariance [6] is to have a quantum neural network of the form $h_{\\theta} = Tr[\\rho \\tilde{O}_{\\theta}]$ such that:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\tilde{O}_{\\theta} \\in Comm(G) = \\{A \\in \\mathbb{C}^{d\\times d} / [A, R(g)] = 0 \\text{ for all } g \\in G\\}\n",
    "\\end{align*}$$\n",
    "\n",
    "To see why, we need to observe that the trace is cyclical, so:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    h_{\\theta} (g\\cdot \\rho) = Tr[R(g)\\rho R^{\\dagger}(g)\\tilde{O}_{\\theta}] = Tr[\\rho R^{\\dagger}(g)\\tilde{O}_{\\theta}R(g)] &= Tr[\\rho R^{\\dagger}(g)R(g)\\tilde{O}_{\\theta}]\\\\ \n",
    "    &= Tr[\\rho \\tilde{O}_{\\theta}]\\\\\n",
    "    &= h_{\\theta}(\\rho).\n",
    "\\end{align*}$$\n",
    "\n",
    "Essentially, we are using the Heisenberg picture, where we apply the time evolution to the measurement operator instead of the initial quantum state. When the observable is included in the commutant of $\\mathcal{G}$, we can see how invariance is achieved.\n",
    "\n",
    "The challenge with this approach is that it only works for finite-dimensional and compact groups, like $p4m$, $SO(3)$, etc. The Lorentz group is known to be continuous and non-compact, so it has no finite-dimensional unitary representation. Hence, the approach above is of no use for us. Hopefully, there is another way: instead of baking equivariance directly into the ansatze, we'll do it in the feature space and in the message passing function. When the input is invariant, the message passing becomes equivariant.\n",
    "\n",
    "Similarly to LorentzNet, for standard jet tagging approach, our input is made of $4$-momentum vectors and any associated particle scalar one may wish to include, like color and charge. In fact, in this project, we start with the traditional LorentzNet architecture, but two modifications are made: first, the invariant metric can be extracted from the machine learned algebra; secondly, the $\\phi_e, \\phi_x, \\phi_h$ and $\\phi_m$ - classical parts modeled as classical multilayer perceptrons in Lorentznet, are now substituted by quantum parameterized circuits. Below we show how invariance-equivariance is preserved under this modification.\n",
    "\n",
    "## 4.1. Lie Equivariant Quantum Block (LEQB)\n",
    "\n",
    "LEQB is the main piece of our model. We aim to fundamentally learn deeper quantum representations of $|\\psi_{x}^{l+1}\\rangle$ and $|\\psi_h^{l+1} \\rangle$ from $|\\psi_{x}^{l} \\rangle$ and $|\\psi_{h}^{l}\\rangle$, where:\n",
    "\n",
    "$$\\begin{align}\n",
    "    |\\psi_{x}^{l+1}\\rangle &= \\mathcal{U}_{x^{l+1}}({x}^{l})|0\\rangle,\\\\\n",
    "    |\\psi_{h}^{l+1}\\rangle &= \\mathcal{U}_{h^{l+1}}({h}^{l})|0\\rangle,\n",
    "\\end{align}$$\n",
    "\n",
    "where $\\mathcal{U_{x^{l}}}, \\mathcal{U_{x^{l+1}}}, \\mathcal{U_{h^{l}}}, \\mathcal{U_{h^{l+1}}}$ are all parameterized standard gate unitaries, or variational circuits. Note that $x^{l}$ are the observables and $h^{l}$ are the particle scalars when $l=0$, but $x^{l} = \\langle \\psi_x \\| \\mathcal{M} \\| \\psi_x\\rangle$ and $h^{l} = \\langle \\psi_h \\| \\mathcal{M} \\| \\psi_h\\rangle$ for $l > 0$, where $\\mathcal{M}$ is some measurement operator.\n",
    "\n",
    "## 4.2. Theoretical analysis\n",
    "\n",
    "Let's start with the following proposition:\n",
    "\n",
    "> The coordinate embedding $x^{l} = \\{x_1^{l} , x_2^{l} , \\dots , x_n^{l}\\}$ is Lie group equivariant and the node embedding $h^{l} = \\{h_1^{l} , h_2^{l}, \\dots , h_n^{l}\\}$ - representing the particle scalars - is Lie group invariant.\n",
    "\n",
    "To prove it, let $Q$ be some Lie group transformation. If the message $m_{ij}^{l}$ is invariant under the action of $Q$ for all $i,j,l,$ then $x_{i}^{l}$ is naturally Lie group equivariant since:\n",
    "\n",
    "$$\\begin{align*}\n",
    "    Q\\cdot x_i^{l+1} &= Q(x_i^{l} + \\sum_{j\\in \\mathcal{N}(i)} x_j^{l}\\cdot \\phi_x (m_{ij}^{l}))\\\\\n",
    "    &= Q\\cdot x_i^{l} + \\sum_{j\\in \\mathcal{N}(i)} Q\\cdot x_j^{l}\\cdot \\phi_x (m_{ij}^{l}),\n",
    "\\end{align*}$$\n",
    "\n",
    "where $Q$ acts under matrix multiplication. The equation above means that acting with $Q$ from the outside is the same as acting with $Q$ from the inside - directly into the node embeddings from the layer before. Then, for the invariance of $m_{ij}^{l}$, since the norm induced by the extracted metric is invariant under the action of $Q$, it holds that $\\|\\|x_{i}^{0} - x_{j}^{0}\\|\\|^2 = \\|\\|Q\\cdot x_{i}^{0} - Q\\cdot x_{j}^{0}\\|\\|^2$, and $\\langle x_{i}^{0}, x_{j}^{0} \\rangle = \\langle Q\\cdot x_{i}^{0}, Q\\cdot x_{j}^{0} \\rangle$. Since $m_{ij}^{l+1} = \\phi_e(h_i^{l}, h_j^{l}, \\|\\|x_{i}^{l} - x_{j}^{l}\\|\\|^2, \\langle x_{i}^{l}, x_{j}^{l} \\rangle)$, and the norm and the inner product are already invariant, we just have to show that $h^{l}$ is also invariant, since:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "    h_i^{l+1} = h_i^{l} + \\phi_h (h_i^{l}, \\sum_{j\\in \\mathcal{N}(i)} w_{ij} m_{ij}^{l}).\n",
    "\\end{equation*}$$\n",
    "    \n",
    "For layer $l=0$, $h_{i}^{l}$ is already invariant (since it contains information only about the particle scalars). Then, $m_{ij}^{l+1}$ will be invariant, since all of its inputs are also invariant, and we follow the same logic for $x_{i}^{l+1}$. Given that these properties of $x,h,m$ hold for the first layer and the next, we reach the conclusion recursively.\n",
    "\n",
    "Having a quick glance at the discussion we had about groups, equivariance, particles and quantum machine learning, we are getting a hint that the marriage between Physics and symmetries is actually deep. Indeed it is! To quote Philip Anderson, who won the 1977 Nobel prize “for their fundamental theoretical investigations of the electronic structure of magnetic and disordered systems”:\n",
    "\n",
    "> It is only slightly overstating the case to say that physics is the study of symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "\n",
    "\"\"\"\n",
    "    Lie-Equivariant Quantum Block (LEQB).\n",
    "    \n",
    "        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n",
    "          that we develop further), once the metric tensor J is found via the equation:\n",
    "\n",
    "                          L.J + J.(L^T) = 0,\n",
    "                          \n",
    "          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group. \n",
    "          In the cells below, we can see how the model preserves symmetries (starting with the default Lorentz group),\n",
    "          and when we change J to some other metric (Euclidean, for example), Lorentz boosts **break** equivariance, while other\n",
    "          transformations preserve it (rotations, for the example shown in the cells below)\n",
    "\"\"\"\n",
    "class LEQB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n",
    "        super(LEQB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.include_x = include_x\n",
    "\n",
    "        \"\"\"\n",
    "            phi_e: input size: n_qubits -> output size: n_qubits\n",
    "            n_hidden has to be equal to n_input (n_input * 2 + n_edge_attr),\n",
    "            but this is just considering that this is a simple working example.\n",
    "        \"\"\"\n",
    "        self.phi_e = DressedQuantumNet(n_input * 2 + n_edge_attr)\n",
    "\n",
    "        n_hidden = n_input * 2 + n_edge_attr\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())        \n",
    "        # self.phi_e = nn.Sequential(\n",
    "        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "        #     nn.BatchNorm1d(n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_hidden, n_hidden),\n",
    "        #     nn.ReLU())\n",
    "\n",
    "        # self.phi_h = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "        #     nn.BatchNorm1d(n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        # layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        # torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        # self.phi_x = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden, n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     layer)\n",
    "\n",
    "        # self.phi_m = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden, 1),\n",
    "        #     nn.Sigmoid())\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "        self.A = A\n",
    "        self.norm_fn = normA_fn(A) if A is not None else normsq4\n",
    "        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        # print(\"Before embedding to |psi> : \", out)\n",
    "        out = self.phi_e(out).squeeze(0)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n",
    "        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n",
    "        out = self.phi_e(out).squeeze(0)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m):\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = self.norm_fn(x_diff).unsqueeze(1)\n",
    "        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        if self.include_x:\n",
    "            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n",
    "        else:\n",
    "            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class LieEQGNN(nn.Module):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of LEQB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n",
    "        super(LieEQGNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.LEQBs = nn.ModuleList([LEQB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "        \n",
    "        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.LEQBs[i](h, x, edges, node_attr=scalars)\n",
    "        \n",
    "        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        \n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "        return pred.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum model\n",
    "\n",
    "#### Let's start with a default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LieEQGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 0.9169079   1.3130671   0.57629734 -0.47118652]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.14373147 -0.01157023 -0.01360894 -0.06450406]\n",
      "h after (just the first particle): \n",
      " [-2.4968634  -1.5802901  -1.0678469  -0.01900774]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... taking any random nonsense transformation in the four-momentum vectors\n",
    "i.e.: multiplying by 0.1. Does the hidden rep stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 2.43436    1.324191  -3.4988296 -1.3810511]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.14373147 -0.01157023 -0.01360894 -0.06450406]\n",
      "h after (just the first particle): \n",
      " [-2.5144496  -0.36996183 -1.928361   -0.69840103]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not at all! What about Lorentz transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 0.91627324  1.3119451   0.5798764  -0.47567284]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(180000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.14373147 -0.01157023 -0.01360894 -0.06450406]\n",
      "h after (just the first particle): \n",
      " [-2.4969366  -1.5804261  -1.0679283  -0.01912272]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(180000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivariance holds!\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.  Symmetry-breaking\n",
    "\n",
    "Now, let's do the predictions again for some other metric tensor J. This will illustrate the situation where we found an infinitesimal generator for some experimental data\n",
    "(i.e.: following Robin Walter's approach in LieGAN; the oracle-preserving latents from Roy Forestano et. al, or some other approach that we develop further - would be interesting). Once we have the generators, suppose that we solved for the metric tensor through the following eq. (as proposed in Robin's paper):\n",
    "\n",
    "\\begin{equation}\n",
    "L\\cdot J + J\\cdot L^{T} = 0\n",
    "\\end{equation}\n",
    "\n",
    "Here, the Lorentz transformations should not anymore preserve equivariance. To illustrate this, let's consider $J = diag(1,1,1,1)$, that is, we recover the Euclidean norm and dot-product. So, if our model is working, then **boosts** should **break equivariance**, but **rotations** should **preserve** it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: \n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "J = torch.eye(4)\n",
    "print(\"J: \\n\", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Here we have a rotation matrix about the xy plane. Given that our Lie-EQGNN has a new metric,\n",
    "    the Lorentz boosts now should break equivariance, but rotations in this case, should preserve\n",
    "    it.\n",
    "\"\"\"\n",
    "rot = torch.tensor([[np.cos(np.pi), -np.sin(np.pi), 0, 0],\n",
    "                    [np.sin(np.pi), np.cos(np.pi),  0, 0],\n",
    "                    [     0       ,       0      ,  1, 0],\n",
    "                    [     0       ,       0      ,  0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LieEQGNN(n_scalar = 8, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3, A=J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, the default forward pass using the Euclidean metric J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.46950197 -3.965734   -3.0378942  -1.9866586 ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the Lorentz boosted jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.60887957 -3.9931903  -3.0501935  -2.342436  ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(240000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariance is broken. What about a rotation about the xy plane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.46950197 -3.965734   -3.0378942  -1.9866586 ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(rot @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariant again.\n",
    "I propose to work on this project, exploring how to improve the symmetry discovery. Also, besides incorporating arbitrary Lie invariances, Infrared Collinear (IRC) safety would be very interesting, and study how our model performs on tagging semi-visible jets for Beyond the Standard Model (BSM) discoveries, like was done in [6] for the Hidden Valley models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jogi/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 283\n",
      " train samples: 10000\n",
      " val samples: 1250\n",
      " test samples: 1250\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:41,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 1/60 \t Batch 624/625 \t Loss 0.6946 \t Running Acc 0.512 \t Total Acc 0.512 \t Avg Batch Time 1.6992\n",
      "Time: train: 1061.98 \t Train loss 0.6946 \t Train acc: 0.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:10,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6911 \t Running Acc 4.190 \t Total Acc 0.530 \t Avg Batch Time 0.1123\n",
      "New best validation model, saving...\n",
      "Epoch 0/60 finished.\n",
      "Train time: 1061.98 \t Val time 70.18\n",
      "Train loss 0.6946 \t Train acc: 0.5119\n",
      "Val loss: 0.6907 \t Val acc: 0.5296\n",
      "Best val acc: 0.5296 at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 2/60 \t Batch 624/625 \t Loss 0.6899 \t Running Acc 0.542 \t Total Acc 0.542 \t Avg Batch Time 1.6580\n",
      "Time: train: 1036.24 \t Train loss 0.6899 \t Train acc: 0.5417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6828 \t Running Acc 4.715 \t Total Acc 0.596 \t Avg Batch Time 0.1112\n",
      "New best validation model, saving...\n",
      "Epoch 1/60 finished.\n",
      "Train time: 1036.24 \t Val time 69.49\n",
      "Train loss 0.6899 \t Train acc: 0.5417\n",
      "Val loss: 0.6819 \t Val acc: 0.5960\n",
      "Best val acc: 0.5960 at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 3/60 \t Batch 624/625 \t Loss 0.6763 \t Running Acc 0.574 \t Total Acc 0.574 \t Avg Batch Time 1.6583\n",
      "Time: train: 1036.42 \t Train loss 0.6763 \t Train acc: 0.5743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:16,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6666 \t Running Acc 4.810 \t Total Acc 0.608 \t Avg Batch Time 0.1220\n",
      "New best validation model, saving...\n",
      "Epoch 2/60 finished.\n",
      "Train time: 1036.42 \t Val time 76.23\n",
      "Train loss 0.6763 \t Train acc: 0.5743\n",
      "Val loss: 0.6647 \t Val acc: 0.6080\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:14,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 4/60 \t Batch 624/625 \t Loss 0.6675 \t Running Acc 0.587 \t Total Acc 0.587 \t Avg Batch Time 1.6557\n",
      "Time: train: 1034.79 \t Train loss 0.6675 \t Train acc: 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6675 \t Running Acc 4.804 \t Total Acc 0.607 \t Avg Batch Time 0.1117\n",
      "Epoch 3/60 finished.\n",
      "Train time: 1034.79 \t Val time 69.84\n",
      "Train loss 0.6675 \t Train acc: 0.5867\n",
      "Val loss: 0.6648 \t Val acc: 0.6072\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 5/60 \t Batch 624/625 \t Loss 0.6634 \t Running Acc 0.598 \t Total Acc 0.598 \t Avg Batch Time 1.6598\n",
      "Time: train: 1037.40 \t Train loss 0.6634 \t Train acc: 0.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6671 \t Running Acc 4.778 \t Total Acc 0.604 \t Avg Batch Time 0.1117\n",
      "Epoch 4/60 finished.\n",
      "Train time: 1037.40 \t Val time 69.80\n",
      "Train loss 0.6634 \t Train acc: 0.5978\n",
      "Val loss: 0.6648 \t Val acc: 0.6040\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:30,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 6/60 \t Batch 624/625 \t Loss 0.6620 \t Running Acc 0.601 \t Total Acc 0.601 \t Avg Batch Time 1.6801\n",
      "Time: train: 1050.08 \t Train loss 0.6620 \t Train acc: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6659 \t Running Acc 4.791 \t Total Acc 0.606 \t Avg Batch Time 0.1117\n",
      "Epoch 5/60 finished.\n",
      "Train time: 1050.08 \t Val time 69.81\n",
      "Train loss 0.6620 \t Train acc: 0.6011\n",
      "Val loss: 0.6630 \t Val acc: 0.6056\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:28,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 7/60 \t Batch 624/625 \t Loss 0.6620 \t Running Acc 0.600 \t Total Acc 0.600 \t Avg Batch Time 1.6780\n",
      "Time: train: 1048.75 \t Train loss 0.6620 \t Train acc: 0.6002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6627 \t Running Acc 4.810 \t Total Acc 0.608 \t Avg Batch Time 0.1110\n",
      "Epoch 6/60 finished.\n",
      "Train time: 1048.75 \t Val time 69.38\n",
      "Train loss 0.6620 \t Train acc: 0.6002\n",
      "Val loss: 0.6600 \t Val acc: 0.6080\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:14,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 8/60 \t Batch 624/625 \t Loss 0.6615 \t Running Acc 0.595 \t Total Acc 0.595 \t Avg Batch Time 1.6547\n",
      "Time: train: 1034.21 \t Train loss 0.6615 \t Train acc: 0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6628 \t Running Acc 4.766 \t Total Acc 0.602 \t Avg Batch Time 0.1116\n",
      "Epoch 7/60 finished.\n",
      "Train time: 1034.21 \t Val time 69.72\n",
      "Train loss 0.6615 \t Train acc: 0.5949\n",
      "Val loss: 0.6599 \t Val acc: 0.6024\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:37,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 9/60 \t Batch 624/625 \t Loss 0.6613 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.6924\n",
      "Time: train: 1057.77 \t Train loss 0.6613 \t Train acc: 0.6016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6637 \t Running Acc 4.886 \t Total Acc 0.618 \t Avg Batch Time 0.1117\n",
      "New best validation model, saving...\n",
      "Epoch 8/60 finished.\n",
      "Train time: 1057.77 \t Val time 69.82\n",
      "Train loss 0.6613 \t Train acc: 0.6016\n",
      "Val loss: 0.6609 \t Val acc: 0.6176\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:14,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 10/60 \t Batch 624/625 \t Loss 0.6626 \t Running Acc 0.599 \t Total Acc 0.599 \t Avg Batch Time 1.6559\n",
      "Time: train: 1034.91 \t Train loss 0.6626 \t Train acc: 0.5985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:45,  1.11it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 11/60 \t Batch 624/625 \t Loss 0.6601 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.6588\n",
      "Time: train: 1036.78 \t Train loss 0.6601 \t Train acc: 0.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6622 \t Running Acc 4.778 \t Total Acc 0.604 \t Avg Batch Time 0.1114\n",
      "Epoch 10/60 finished.\n",
      "Train time: 1036.78 \t Val time 69.63\n",
      "Train loss 0.6601 \t Train acc: 0.6035\n",
      "Val loss: 0.6594 \t Val acc: 0.6040\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:15,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 12/60 \t Batch 624/625 \t Loss 0.6614 \t Running Acc 0.599 \t Total Acc 0.599 \t Avg Batch Time 1.6568\n",
      "Time: train: 1035.50 \t Train loss 0.6614 \t Train acc: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6601 \t Running Acc 4.880 \t Total Acc 0.617 \t Avg Batch Time 0.1120\n",
      "Epoch 11/60 finished.\n",
      "Train time: 1035.50 \t Val time 69.99\n",
      "Train loss 0.6614 \t Train acc: 0.5994\n",
      "Val loss: 0.6573 \t Val acc: 0.6168\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 13/60 \t Batch 624/625 \t Loss 0.6600 \t Running Acc 0.597 \t Total Acc 0.597 \t Avg Batch Time 1.6597\n",
      "Time: train: 1037.31 \t Train loss 0.6600 \t Train acc: 0.5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:12,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6595 \t Running Acc 4.759 \t Total Acc 0.602 \t Avg Batch Time 0.1157\n",
      "Epoch 12/60 finished.\n",
      "Train time: 1037.31 \t Val time 72.33\n",
      "Train loss 0.6600 \t Train acc: 0.5973\n",
      "Val loss: 0.6565 \t Val acc: 0.6016\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 14/60 \t Batch 624/625 \t Loss 0.6593 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.6608\n",
      "Time: train: 1037.99 \t Train loss 0.6593 \t Train acc: 0.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:14,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6572 \t Running Acc 4.816 \t Total Acc 0.609 \t Avg Batch Time 0.1195\n",
      "Epoch 13/60 finished.\n",
      "Train time: 1037.99 \t Val time 74.68\n",
      "Train loss 0.6593 \t Train acc: 0.6035\n",
      "Val loss: 0.6544 \t Val acc: 0.6088\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:28,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 15/60 \t Batch 624/625 \t Loss 0.6612 \t Running Acc 0.600 \t Total Acc 0.600 \t Avg Batch Time 1.6775\n",
      "Time: train: 1048.44 \t Train loss 0.6612 \t Train acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:40,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 16/60 \t Batch 624/625 \t Loss 0.6602 \t Running Acc 0.599 \t Total Acc 0.599 \t Avg Batch Time 1.6962\n",
      "Time: train: 1060.14 \t Train loss 0.6602 \t Train acc: 0.5993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6577 \t Running Acc 4.949 \t Total Acc 0.626 \t Avg Batch Time 0.1118\n",
      "New best validation model, saving...\n",
      "Epoch 15/60 finished.\n",
      "Train time: 1060.14 \t Val time 69.85\n",
      "Train loss 0.6602 \t Train acc: 0.5993\n",
      "Val loss: 0.6548 \t Val acc: 0.6256\n",
      "Best val acc: 0.6256 at epoch 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:46,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 17/60 \t Batch 624/625 \t Loss 0.6592 \t Running Acc 0.600 \t Total Acc 0.600 \t Avg Batch Time 1.7071\n",
      "Time: train: 1066.96 \t Train loss 0.6592 \t Train acc: 0.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6620 \t Running Acc 4.823 \t Total Acc 0.610 \t Avg Batch Time 0.1113\n",
      "Epoch 16/60 finished.\n",
      "Train time: 1066.96 \t Val time 69.58\n",
      "Train loss 0.6592 \t Train acc: 0.6001\n",
      "Val loss: 0.6589 \t Val acc: 0.6096\n",
      "Best val acc: 0.6256 at epoch 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:45,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 18/60 \t Batch 624/625 \t Loss 0.6605 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.7053\n",
      "Time: train: 1065.84 \t Train loss 0.6605 \t Train acc: 0.6015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6574 \t Running Acc 4.918 \t Total Acc 0.622 \t Avg Batch Time 0.1111\n",
      "Epoch 17/60 finished.\n",
      "Train time: 1065.84 \t Val time 69.45\n",
      "Train loss 0.6605 \t Train acc: 0.6015\n",
      "Val loss: 0.6545 \t Val acc: 0.6216\n",
      "Best val acc: 0.6256 at epoch 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:30,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 19/60 \t Batch 624/625 \t Loss 0.6599 \t Running Acc 0.601 \t Total Acc 0.601 \t Avg Batch Time 1.6806\n",
      "Time: train: 1050.35 \t Train loss 0.6599 \t Train acc: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6560 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1113\n",
      "New best validation model, saving...\n",
      "Epoch 18/60 finished.\n",
      "Train time: 1050.35 \t Val time 69.59\n",
      "Train loss 0.6599 \t Train acc: 0.6011\n",
      "Val loss: 0.6535 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [05:30,  1.65s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "625it [18:03,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 25/60 \t Batch 624/625 \t Loss 0.6581 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.7336\n",
      "Time: train: 1083.50 \t Train loss 0.6581 \t Train acc: 0.6042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:10,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6520 \t Running Acc 4.962 \t Total Acc 0.627 \t Avg Batch Time 0.1130\n",
      "Epoch 24/60 finished.\n",
      "Train time: 1083.50 \t Val time 70.63\n",
      "Train loss 0.6581 \t Train acc: 0.6042\n",
      "Val loss: 0.6489 \t Val acc: 0.6272\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 26/60 \t Batch 624/625 \t Loss 0.6587 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.6579\n",
      "Time: train: 1036.16 \t Train loss 0.6587 \t Train acc: 0.6041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6507 \t Running Acc 4.873 \t Total Acc 0.616 \t Avg Batch Time 0.1119\n",
      "Epoch 25/60 finished.\n",
      "Train time: 1036.16 \t Val time 69.94\n",
      "Train loss 0.6587 \t Train acc: 0.6041\n",
      "Val loss: 0.6477 \t Val acc: 0.6160\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:58,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 27/60 \t Batch 624/625 \t Loss 0.6557 \t Running Acc 0.608 \t Total Acc 0.608 \t Avg Batch Time 1.7249\n",
      "Time: train: 1078.07 \t Train loss 0.6557 \t Train acc: 0.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:11,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6506 \t Running Acc 4.861 \t Total Acc 0.614 \t Avg Batch Time 0.1139\n",
      "Epoch 26/60 finished.\n",
      "Train time: 1078.07 \t Val time 71.17\n",
      "Train loss 0.6557 \t Train acc: 0.6078\n",
      "Val loss: 0.6474 \t Val acc: 0.6144\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:35,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 28/60 \t Batch 624/625 \t Loss 0.6554 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.6883\n",
      "Time: train: 1055.16 \t Train loss 0.6554 \t Train acc: 0.6020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6497 \t Running Acc 4.892 \t Total Acc 0.618 \t Avg Batch Time 0.1118\n",
      "Epoch 27/60 finished.\n",
      "Train time: 1055.16 \t Val time 69.85\n",
      "Train loss 0.6554 \t Train acc: 0.6020\n",
      "Val loss: 0.6464 \t Val acc: 0.6184\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 29/60 \t Batch 624/625 \t Loss 0.6553 \t Running Acc 0.606 \t Total Acc 0.606 \t Avg Batch Time 1.6606\n",
      "Time: train: 1037.90 \t Train loss 0.6553 \t Train acc: 0.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6495 \t Running Acc 4.873 \t Total Acc 0.616 \t Avg Batch Time 0.1111\n",
      "Epoch 28/60 finished.\n",
      "Train time: 1037.90 \t Val time 69.46\n",
      "Train loss 0.6553 \t Train acc: 0.6060\n",
      "Val loss: 0.6462 \t Val acc: 0.6160\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 30/60 \t Batch 624/625 \t Loss 0.6550 \t Running Acc 0.608 \t Total Acc 0.608 \t Avg Batch Time 1.6584\n",
      "Time: train: 1036.50 \t Train loss 0.6550 \t Train acc: 0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6494 \t Running Acc 4.867 \t Total Acc 0.615 \t Avg Batch Time 0.1118\n",
      "Epoch 29/60 finished.\n",
      "Train time: 1036.50 \t Val time 69.90\n",
      "Train loss 0.6550 \t Train acc: 0.6076\n",
      "Val loss: 0.6463 \t Val acc: 0.6152\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 31/60 \t Batch 624/625 \t Loss 0.6568 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.6586\n",
      "Time: train: 1036.65 \t Train loss 0.6568 \t Train acc: 0.6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6491 \t Running Acc 4.937 \t Total Acc 0.624 \t Avg Batch Time 0.1112\n",
      "Epoch 30/60 finished.\n",
      "Train time: 1036.65 \t Val time 69.53\n",
      "Train loss 0.6568 \t Train acc: 0.6023\n",
      "Val loss: 0.6461 \t Val acc: 0.6240\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [18:54,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 32/60 \t Batch 624/625 \t Loss 0.6560 \t Running Acc 0.607 \t Total Acc 0.607 \t Avg Batch Time 1.8156\n",
      "Time: train: 1134.77 \t Train loss 0.6560 \t Train acc: 0.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [18:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 33/60 \t Batch 624/625 \t Loss 0.6555 \t Running Acc 0.606 \t Total Acc 0.606 \t Avg Batch Time 1.7293\n",
      "Time: train: 1080.84 \t Train loss 0.6555 \t Train acc: 0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6490 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1118\n",
      "Epoch 32/60 finished.\n",
      "Train time: 1080.84 \t Val time 69.88\n",
      "Train loss 0.6555 \t Train acc: 0.6061\n",
      "Val loss: 0.6460 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 34/60 \t Batch 624/625 \t Loss 0.6564 \t Running Acc 0.603 \t Total Acc 0.603 \t Avg Batch Time 1.6595\n",
      "Time: train: 1037.21 \t Train loss 0.6564 \t Train acc: 0.6032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6490 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1113\n",
      "Epoch 33/60 finished.\n",
      "Train time: 1037.21 \t Val time 69.59\n",
      "Train loss 0.6564 \t Train acc: 0.6032\n",
      "Val loss: 0.6460 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 35/60 \t Batch 624/625 \t Loss 0.6552 \t Running Acc 0.607 \t Total Acc 0.607 \t Avg Batch Time 1.6584\n",
      "Time: train: 1036.49 \t Train loss 0.6552 \t Train acc: 0.6069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:13,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6490 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1174\n",
      "Epoch 34/60 finished.\n",
      "Train time: 1036.49 \t Val time 73.38\n",
      "Train loss 0.6552 \t Train acc: 0.6069\n",
      "Val loss: 0.6460 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [02:44,  1.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m### training and testing\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m test(model, res, model_path, log_path)\n",
      "Cell \u001b[0;32mIn[250], line 93\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, res, N_EPOCHS, model_path, log_path)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, res, N_EPOCHS, model_path, log_path):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m### training and validation\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> 93\u001b[0m         train_res \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: train: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train loss \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train acc: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# if epoch % args.val_interval == 0:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m             \n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# if (args.local_rank == 0):\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[250], line 37\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, epoch, loader, partition, N_EPOCHS)\u001b[0m\n\u001b[1;32m     34\u001b[0m edges \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m edges]\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device, dtype)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 37\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscalars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matom_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matom_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                 \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m predict \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m     41\u001b[0m correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(predict \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 171\u001b[0m, in \u001b[0;36mQLieEGNN.forward\u001b[0;34m(self, scalars, x, edges, node_mask, edge_mask, n_nodes)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 171\u001b[0m     h, x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQLieGEBs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m    175\u001b[0m h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m node_mask\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 135\u001b[0m, in \u001b[0;36mQLieGEB.forward\u001b[0;34m(self, h, x, edges, node_attr)\u001b[0m\n\u001b[1;32m    133\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdots\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B*N, hidden]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer:\n\u001b[1;32m    137\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_model(x, edges, x_diff, m)\n",
      "Cell \u001b[0;32mIn[251], line 89\u001b[0m, in \u001b[0;36mQLieGEB.m_model\u001b[0;34m(self, hi, hj, norms, dots)\u001b[0m\n\u001b[1;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([hi, hj, norms, dots], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# print(\"Before embedding to |psi> : \", out)\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     90\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi_m(out)\n\u001b[1;32m     91\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m w\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[172], line 92\u001b[0m, in \u001b[0;36mDressedQuantumNet.forward\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# for batch in q_in:\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m q_in:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# print(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits))\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     q_out_elem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack(\u001b[43mquantum_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_qubits\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m     q_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((q_out, q_out_elem))\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# return the batch measurement of the PQC\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/qnode.py:1098\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn(shots\u001b[38;5;241m=\u001b[39moverride_shots, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_interface \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/qnode.py:1052\u001b[0m, in \u001b[0;36mQNode._execution_component\u001b[0;34m(self, args, kwargs, override_shots)\u001b[0m\n\u001b[1;32m   1049\u001b[0m full_transform_program\u001b[38;5;241m.\u001b[39mprune_dynamic_transform()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/execution.py:616\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[0;32m--> 616\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[1;32m    619\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/execution.py:297\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[0;34m(tapes, **_)\u001b[0m\n\u001b[1;32m    294\u001b[0m transformed_tapes, transform_post_processing \u001b[38;5;241m=\u001b[39m transform_program(tapes)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_tapes:\n\u001b[0;32m--> 297\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mdevice_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_tapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     results \u001b[38;5;241m=\u001b[39m ()\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/modifiers/simulator_tracking.py:30\u001b[0m, in \u001b[0;36m_track_execute.<locals>.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config\u001b[38;5;241m=\u001b[39mDefaultExecutionConfig):\n\u001b[0;32m---> 30\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43muntracked_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[1;32m     32\u001b[0m         batch \u001b[38;5;241m=\u001b[39m (circuits,)\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/modifiers/single_tape_support.py:32\u001b[0m, in \u001b[0;36m_make_execute.<locals>.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     30\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[0;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:593\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m    590\u001b[0m prng_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prng_keys()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(circuits))]\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebugger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprng_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[1;32m    608\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:594\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    590\u001b[0m prng_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prng_keys()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(circuits))]\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 594\u001b[0m         \u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebugger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprng_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c, _key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(circuits, prng_keys)\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    607\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[1;32m    608\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:841\u001b[0m, in \u001b[0;36m_simulate_wrapper\u001b[0;34m(circuit, kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_simulate_wrapper\u001b[39m(circuit, kwargs):\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/qubit/simulate.py:287\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(circuit, debugger, state_cache, **execution_kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simulate_one_shot_native_mcm(\n\u001b[1;32m    283\u001b[0m         circuit, debugger\u001b[38;5;241m=\u001b[39mdebugger, rng\u001b[38;5;241m=\u001b[39mrng, prng_key\u001b[38;5;241m=\u001b[39mprng_key, interface\u001b[38;5;241m=\u001b[39minterface\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    286\u001b[0m ops_key, meas_key \u001b[38;5;241m=\u001b[39m jax_random_split(prng_key)\n\u001b[0;32m--> 287\u001b[0m state, is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mops_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     state_cache[circuit\u001b[38;5;241m.\u001b[39mhash] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/qubit/simulate.py:150\u001b[0m, in \u001b[0;36mget_final_state\u001b[0;34m(circuit, debugger, **execution_kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, MidMeasureMP):\n\u001b[1;32m    149\u001b[0m     prng_key, key \u001b[38;5;241m=\u001b[39m jax_random_split(prng_key)\n\u001b[0;32m--> 150\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mapply_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmid_measurements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmid_measurements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/qubit/apply_operation.py:356\u001b[0m, in \u001b[0;36mapply_cnot\u001b[0;34m(op, state, is_state_batched, debugger, **_)\u001b[0m\n\u001b[1;32m    353\u001b[0m sl_0 \u001b[38;5;241m=\u001b[39m _get_slice(\u001b[38;5;241m0\u001b[39m, control_axes, n_dim)\n\u001b[1;32m    354\u001b[0m sl_1 \u001b[38;5;241m=\u001b[39m _get_slice(\u001b[38;5;241m1\u001b[39m, control_axes, n_dim)\n\u001b[0;32m--> 356\u001b[0m state_x \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl_1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mstack([state[sl_0], state_x], axis\u001b[38;5;241m=\u001b[39mcontrol_axes)\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/autoray/autoray.py:30\u001b[0m, in \u001b[0;36mdo\u001b[0;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict, defaultdict\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    dispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    the ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     backend \u001b[38;5;241m=\u001b[39m _choose_backend(fn, args, kwargs, like\u001b[38;5;241m=\u001b[39mlike)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    N_EPOCHS = 60\n",
    "\n",
    "    model_path = \"models/LieEQGNN/\"\n",
    "    log_path = \"logs/LieEQGNN/\"\n",
    "    # utils_lorentz.args_init(args)\n",
    "\n",
    "    ### set random seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ### initialize cuda\n",
    "    # dist.init_process_group(backend='nccl')\n",
    "    device = 'cpu' #torch.device(\"cuda\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    ### load data\n",
    "    # dataloaders = retrieve_dataloaders( batch_size,\n",
    "    #                                     num_data=100000, # use all data\n",
    "    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n",
    "    #                                     num_workers=0,\n",
    "    #                                     use_one_hot=True)\n",
    "\n",
    "    ### create parallel model\n",
    "    # model = LorentzNet(n_scalar = 8, n_hidden = 72, n_class = 2,\\\n",
    "    #                    dropout = 0.2, n_layers = 1,\\\n",
    "    #                    c_weight = 1e-3)\n",
    "\n",
    "    model = LieEQGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 1,\\\n",
    "                       c_weight = 1e-3)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    ### print model and dataset information\n",
    "    # if (args.local_rank == 0):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Model Size:\", pytorch_total_params)\n",
    "    for (split, dataloader) in dataloaders.items():\n",
    "        print(f\" {split} samples: {len(dataloader.dataset)}\")\n",
    "\n",
    "    ### optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "    ### lr scheduler\n",
    "    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n",
    "    lr_scheduler = utils_lorentz.GradualWarmupScheduler(optimizer, multiplier=1,\\\n",
    "                                                warmup_epoch=5,\\\n",
    "                                                after_scheduler=base_scheduler) ## warmup\n",
    "\n",
    "    ### loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### initialize logs\n",
    "    res = {'epochs': [], 'lr' : [],\\\n",
    "           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n",
    "           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n",
    "\n",
    "    ### training and testing\n",
    "    print(\"Training...\")\n",
    "    train(model, res, N_EPOCHS, model_path, log_path)\n",
    "    test(model, res, model_path, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train-result-epoch9.json'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "max(os.listdir(\"logs/LorentzNet/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# log_path = \"logs/LorentzNet/train-result-epoch59.json\"\n",
    "# model_name = \"LorentzNet\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/phi_m/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_phi_m\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/phi_h/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_phi_h\"\n",
    "\n",
    "log_path = \"logs/LieEQGNN/phi_x/train-result-epoch51.json\"\n",
    "model_name = \"LieEQGNN_phi_x\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/phi_e/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_phi_e\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/full_quantum/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_full_quantum\"\n",
    "\n",
    "with open(log_path) as f:\n",
    "\n",
    "    data = json.load(f)\n",
    "    epochs = data[\"epochs\"]\n",
    "    train_loss = data[\"train_loss\"]\n",
    "    val_loss = data[\"val_loss\"]\n",
    "    train_acc = data[\"train_acc\"]\n",
    "    val_acc = data[\"val_acc\"]\n",
    "    \n",
    "    # Plotting Loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='--')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', color='blue', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"figures/{}_loss.png\".format(model_name), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting Accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy', color='green', linestyle='--')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy', color='green', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"figures/{}_accuracy.png\".format(model_name), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# List of log paths and model names\n",
    "\n",
    "log_paths_and_model_names = [\n",
    "\n",
    "    (\"logs/LorentzNet/train-result-epoch59.json\", \"LorentzNet\"),\n",
    "    (\"logs/LieEQGNN/phi_m/train-result-epoch59.json\", \"LieEQGNN_phi_m\"),\n",
    "    (\"logs/LieEQGNN/phi_h/train-result-epoch48.json\", \"LieEQGNN_phi_h\"),\n",
    "    (\"logs/LieEQGNN/phi_x/train-result-epoch59.json\", \"LieEQGNN_phi_x\"),\n",
    "    (\"logs/LieEQGNN/phi_e/train-result-epoch50.json\", \"LieEQGNN_phi_e\"),\n",
    "    (\"logs/LieEQGNN/full_quantum/train-result-epoch18.json\", \"LieEQGNN_full_quantum\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for log_path, model_name in log_paths_and_model_names:\n",
    "\n",
    "    with open(log_path) as f:\n",
    "\n",
    "        data = json.load(f)\n",
    "        \n",
    "        epochs = data[\"epochs\"]\n",
    "        train_loss = data[\"train_loss\"]\n",
    "        val_loss = data[\"val_loss\"]\n",
    "        train_acc = data[\"train_acc\"]\n",
    "        val_acc = data[\"val_acc\"]\n",
    "\n",
    "        # Plotting Loss\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='--')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss', color='orange', linestyle='-')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(model_name)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"figures/{}_loss.png\".format(model_name), dpi=300)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "\n",
    "        # Plotting Accuracy\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epochs, train_acc, label='Train Accuracy', color='green', linestyle='--')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy', color='orange', linestyle='-')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(model_name)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"figures/{}_accuracy.png\".format(model_name), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Shlomi, Jonathan, Peter Battaglia, and Jean-Roch Vlimant. \"[Graph neural networks in particle physics.](https://arxiv.org/abs/2007.13681)\" Machine Learning: Science and Technology 2.2 (2020): 021001.\n",
    "\n",
    "[2] Forestano, Roy T., et al. \"[Deep learning symmetries and their Lie groups, algebras, and subalgebras from first principles.](https://arxiv.org/abs/2301.05638)\" Machine Learning: Science and Technology 4.2 (2023): 025027.\n",
    "\n",
    "[3] Yang, Jianke, et al. \"[Generative adversarial symmetry discovery.](https://arxiv.org/abs/2302.00236)\" International Conference on Machine Learning. PMLR, 2023.\n",
    "\n",
    "[4] Verdon, Guillaume, et al. \"[Quantum graph neural networks.](https://arxiv.org/abs/1909.12264)\" arXiv preprint arXiv:1909.12264 (2019).\n",
    "\n",
    "[5] Naomichi Hatano and Masuo Suzuki. [Finding Exponential Product Formulas of Higher Orders](https://arxiv.org/abs/math-ph/0506007), page 37–68. Springer Berlin Heidelberg, November 2005.\n",
    "\n",
    "[6] Nguyen, Quynh T., et al. \"[Theory for Equivariant Quantum Neural Networks](https://arxiv.org/abs/2210.08566)\" PRX Quantum 5.2 (2024): 020328.\n",
    "\n",
    "[7] Gong, S., et al. \"An Efficient Lorentz Equivariant Graph Neural Network for Jet Tagging\" [arXiv:2201.08187](https://arxiv.org/abs/2201.08187).\n",
    "\n",
    "[8] Komiske, P. T., Metodiev, E. M., & Thaler, J. \"EnergyFlow: A Python Package for High Energy Physics.\" [EnergyFlow](https://energyflow.network/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
