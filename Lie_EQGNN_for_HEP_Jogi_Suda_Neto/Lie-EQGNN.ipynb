{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Lie-Equivariant GNN (Q-LieEGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Todo\n",
    "[X] - Get code for LorentzNet\n",
    "    [X] - Sanity checking, test for equivariance against Lorentz boosts.\n",
    "[X] - Implement Lie-EQGNN.\n",
    "    [X] - Sanity checking, test for equivariance against Lorentz boosts.\n",
    "[X] - Test training function for classical LorentzNet.\n",
    "[X] - Test training function for Lie EQGNN.\n",
    "[X] - Convert Roy's data into LorentzNet format.\n",
    "[X] - Get a full script for training LieEQGNN.\n",
    "[ ] - What is performance of LorentzNet vs Lie-EQGNN vs QLorentzNet on the same jet data?\n",
    "[ ] - What is performance of LorentzNet vs Lie-EQGNN vs QLorentzNet against boosts?\n",
    "[ ] - What is the performance of LorentzNet vs Lie-EQGNN vs QLorentzNet on noisy data (ie: gaussian)?\n",
    "[ ] - What is the performance on semi-visible jets (like in hidden valley models) ?\n",
    "[ ] - Implement Infrared (IR) and possibly Collinear (C) safety in the message passing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### in this task, we were asked to implement and draw the architecture of a possible Quantum Graph Neural Network\n",
    "\n",
    "I will start from the LorentzNet paper and official implementation, load the quark-gluon tagging data, and perform equivariance tests just for sanity checks, to confirm that the code works.\n",
    "\n",
    "Once this is done, here I put a simple modification to incorporate parameterized circuits using Pennylane. Once this is done, we test again for equivariance. Once equivariance test is passed, I show equivariance to an arbitrary metric tensor $J$, where the Lorentz boosts are now symmetry-breaking, but rotations about a fixed plane are preserving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_geometric in /home/jogi/anaconda3/lib/python3.9/site-packages (2.5.2)\n",
      "Requirement already satisfied: tqdm in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (4.65.0)\n",
      "Requirement already satisfied: numpy in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.26.3)\n",
      "Requirement already satisfied: scipy in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.11.4)\n",
      "Requirement already satisfied: fsspec in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2024.3.1)\n",
      "Requirement already satisfied: jinja2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.1.2)\n",
      "Requirement already satisfied: aiohttp in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.9.3)\n",
      "Requirement already satisfied: requests in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (2.31.0)\n",
      "Requirement already satisfied: pyparsing in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (3.1.1)\n",
      "Requirement already satisfied: scikit-learn in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from torch_geometric) (5.9.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from jinja2->torch_geometric) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jogi/anaconda3/lib/python3.9/site-packages (from requests->torch_geometric) (2023.11.17)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jogi/anaconda3/lib/python3.9/site-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting torch_sparse\n",
      "  Using cached torch_sparse-0.6.18.tar.gz (209 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-lcy59mga/torch-sparse_ff3c8ac0008f4b1d8e47a3a9bea01e6f/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25hCollecting torch_scatter\n",
      "  Using cached torch_scatter-2.1.2.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[6 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-ch0ak2wl/torch-scatter_6bea335503cc468b8f0c565fd8280640/setup.py\", line 8, in <module>\n",
      "  \u001b[31m   \u001b[0m     import torch\n",
      "  \u001b[31m   \u001b[0m ModuleNotFoundError: No module named 'torch'\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# For Colab\n",
    "!pip install torch_geometric\n",
    "!pip install torch_sparse\n",
    "!pip install torch_scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two ways of equivariance for QGNNs:\n",
    "\n",
    "### - Invariant Theory\n",
    "### - Marco Cerezo, Roy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pennylane as qml\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "\n",
    "def H_layer(nqubits):\n",
    "    \"\"\"Layer of single-qubit Hadamard gates.\n",
    "    \"\"\"\n",
    "    for idx in range(nqubits):\n",
    "        qml.Hadamard(wires=idx)\n",
    "\n",
    "\n",
    "def RY_layer(w):\n",
    "    \"\"\"Layer of parametrized qubit rotations around the y axis.\n",
    "    \"\"\"\n",
    "    for idx, element in enumerate(w):\n",
    "        qml.RY(element, wires=idx)\n",
    "\n",
    "\n",
    "def entangling_layer(nqubits):\n",
    "    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n",
    "    \"\"\"\n",
    "    # In other words it should apply something like :\n",
    "    # CNOT  CNOT  CNOT  CNOT...  CNOT\n",
    "    #   CNOT  CNOT  CNOT...  CNOT\n",
    "    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "\n",
    "\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def quantum_net(q_input_features, q_weights_flat, q_depth, n_qubits):\n",
    "    \"\"\"\n",
    "    The variational quantum circuit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Reshape weights\n",
    "    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n",
    "\n",
    "    # Start from state |+> , unbiased w.r.t. |0> and |1>\n",
    "    H_layer(n_qubits)\n",
    "\n",
    "    # Embed features in the quantum node\n",
    "    RY_layer(q_input_features)\n",
    "\n",
    "    # Sequence of trainable variational layers\n",
    "    for k in range(q_depth):\n",
    "        entangling_layer(n_qubits)\n",
    "        RY_layer(q_weights[k])\n",
    "\n",
    "    # Expectation values in the Z basis\n",
    "    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n",
    "    return tuple(exp_vals)\n",
    "\n",
    "\n",
    "class DressedQuantumNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Torch module implementing the *dressed* quantum net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_qubits, q_depth = 1, q_delta=0.001):\n",
    "        \"\"\"\n",
    "        Definition of the *dressed* layout.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.q_depth = q_depth\n",
    "        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        \"\"\"\n",
    "        Defining how tensors are supposed to move through the *dressed* quantum\n",
    "        net.\n",
    "        \"\"\"\n",
    "\n",
    "        # Quantum Embedding (U(X))\n",
    "        q_in = torch.tanh(input_features) * np.pi / 2.0\n",
    "\n",
    "        # Apply the quantum circuit to each element of the batch and append to q_out\n",
    "        q_out = torch.Tensor(0, self.n_qubits)\n",
    "        q_out = q_out.to(device)\n",
    "        # for batch in q_in:\n",
    "        for elem in q_in:\n",
    "            # print(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits))\n",
    "            q_out_elem = torch.hstack(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits)).float().unsqueeze(0)\n",
    "            q_out = torch.cat((q_out, q_out_elem))\n",
    "\n",
    "        # return the batch measurement of the PQC\n",
    "        return q_out.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        ...,\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False],\n",
      "        [ True,  True,  True,  ..., False, False, False]])\n",
      "torch.Size([16]) torch.Size([16, 139, 4]) torch.Size([16, 139, 8]) torch.Size([16, 139]) torch.Size([16, 139, 139]) torch.Size([28736]) torch.Size([28736])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import energyflow\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "    rows, cols = [], []\n",
    "    # print(edge_mask[0])\n",
    "    # raise\n",
    "    for batch_idx in range(batch_size):\n",
    "        nn = batch_idx*n_nodes\n",
    "        x = coo_matrix(edge_mask[batch_idx])\n",
    "        rows.append(nn + x.row)\n",
    "        cols.append(nn + x.col)\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "\n",
    "    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n",
    "    return edges\n",
    "\n",
    "def collate_fn(data):\n",
    "    data = list(zip(*data)) # label p4s nodes atom_mask\n",
    "    data = [torch.stack(item) for item in data]\n",
    "    batch_size, n_nodes, _ = data[1].size()\n",
    "    atom_mask = data[-1]\n",
    "    edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n",
    "    diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "    edge_mask *= diag_mask\n",
    "    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n",
    "    return data + [edge_mask, edges]\n",
    "\n",
    "def retrieve_dataloaders(batch_size, num_data = -1, use_one_hot = True, cache_dir = './data', num_workers=4):\n",
    "    raw = energyflow.qg_jets.load(num_data=num_data, pad=True, ncol=4, generator='pythia',\n",
    "                            with_bc=False, cache_dir=cache_dir)\n",
    "    splits = ['train', 'val', 'test']\n",
    "    data = {type:{'raw':None,'label':None} for type in splits}\n",
    "    (data['train']['raw'],  data['val']['raw'],   data['test']['raw'],\n",
    "    data['train']['label'], data['val']['label'], data['test']['label']) = \\\n",
    "        energyflow.utils.data_split(*raw, train=0.8, val=0.1, test=0.1, shuffle = False)\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown='ignore').fit([[11],[13],[22],[130],[211],[321],[2112],[2212]])\n",
    "    \n",
    "    for split, value in data.items():\n",
    "        pid = torch.from_numpy(np.abs(np.asarray(value['raw'][...,3], dtype=int))).unsqueeze(-1)\n",
    "        p4s = torch.from_numpy(energyflow.p4s_from_ptyphipids(value['raw'],error_on_unknown=True))\n",
    "        one_hot = enc.transform(pid.reshape(-1,1)).toarray().reshape(pid.shape[:2]+(-1,))\n",
    "        one_hot = torch.from_numpy(one_hot)\n",
    "        mass = torch.from_numpy(energyflow.ms_from_p4s(p4s)).unsqueeze(-1)\n",
    "        charge = torch.from_numpy(energyflow.pids2chrgs(pid))\n",
    "        \n",
    "        # if split == 'train':\n",
    "        #     print(\"Mass: \", mass.shape, mass)\n",
    "        #     print(\"Charge: \", charge.shape, charge)\n",
    "        if use_one_hot:\n",
    "            nodes = one_hot\n",
    "            # if split == 'train':\n",
    "            #     print(\"Nodes one-hot: \", nodes.shape, nodes)\n",
    "        else:\n",
    "            nodes = torch.cat((mass,charge),dim=-1)\n",
    "\n",
    "            # if split == 'train':\n",
    "            #     print(\"Nodes before: \", nodes.shape, nodes)\n",
    "            nodes = torch.sign(nodes) * torch.log(torch.abs(nodes) + 1)\n",
    "            # if split == 'train':\n",
    "            #     print(\"Nodes after: \", nodes.shape, nodes)\n",
    "                \n",
    "        atom_mask = (pid[...,0] != 0)\n",
    "        \n",
    "        # if split == 'train':\n",
    "        #     print(atom_mask)\n",
    "        value['p4s'] = p4s\n",
    "        value['nodes'] = nodes\n",
    "        value['label'] = torch.from_numpy(value['label'])\n",
    "        value['atom_mask'] = atom_mask.to(torch.bool)\n",
    "        \n",
    "        if split == 'train':\n",
    "            print(value['atom_mask'])\n",
    "\n",
    "    datasets = {split: TensorDataset(value['label'], value['p4s'],\n",
    "                                     value['nodes'], value['atom_mask'])\n",
    "                for split, value in data.items()}\n",
    "\n",
    "    # distributed training\n",
    "    # train_sampler = DistributedSampler(datasets['train'], shuffle=True)\n",
    "    # Construct PyTorch dataloaders from datasets\n",
    "    dataloaders = {split: DataLoader(dataset,\n",
    "                                     batch_size=batch_size,\n",
    "                                     # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n",
    "                                     pin_memory=False,\n",
    "                                     # persistent_workers=True,\n",
    "                                     drop_last=True if (split == 'train') else False,\n",
    "                                     num_workers=num_workers,\n",
    "                                     collate_fn=collate_fn)\n",
    "                        for split, dataset in datasets.items()}\n",
    "\n",
    "    return dataloaders #train_sampler, dataloaders\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # train_sampler, dataloaders = retrieve_dataloaders(32, 100)\n",
    "    dataloaders = retrieve_dataloaders(16, 20)\n",
    "    for (label, p4s, nodes, atom_mask, edge_mask, edges) in dataloaders['train']:\n",
    "        print(label.shape, p4s.shape, nodes.shape, atom_mask.shape,\n",
    "              edge_mask.shape, edges[0].shape, edges[1].shape)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 139, 4])\n",
      "torch.Size([16, 139, 8])\n",
      "torch.Size([16, 139])\n",
      "torch.Size([16, 139, 139])\n"
     ]
    }
   ],
   "source": [
    "print(p4s.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 139])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# symmetric (undirected graph)\n",
    "\n",
    "atom_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 60])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 180])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def get_adj_matrix(n_nodes, batch_size, edge_mask):\n",
    "    rows, cols = [], []\n",
    "    # print(edge_mask[0])\n",
    "    # raise\n",
    "    for batch_idx in range(batch_size):\n",
    "        nn = batch_idx*n_nodes\n",
    "        x = coo_matrix(edge_mask[batch_idx])\n",
    "        rows.append(nn + x.row)\n",
    "        cols.append(nn + x.col)\n",
    "    rows = np.concatenate(rows)\n",
    "    cols = np.concatenate(cols)\n",
    "\n",
    "    edges = [torch.LongTensor(rows), torch.LongTensor(cols)]\n",
    "    return edges\n",
    "\n",
    "def collate_fn(data):\n",
    "    data = list(zip(*data)) # label p4s nodes atom_mask\n",
    "    data = [torch.stack(item) for item in data]\n",
    "    batch_size, n_nodes, _ = data[1].size()\n",
    "    atom_mask = data[-1]\n",
    "    # edge_mask = atom_mask.unsqueeze(1) * atom_mask.unsqueeze(2)\n",
    "    # diag_mask = ~torch.eye(edge_mask.size(1), dtype=torch.bool).unsqueeze(0)\n",
    "    # edge_mask *= diag_mask\n",
    "\n",
    "    edge_mask = data[-2]\n",
    "\n",
    "    edges = get_adj_matrix(n_nodes, batch_size, edge_mask)\n",
    "    return data + [edges]\n",
    "\n",
    "\n",
    "p4s = torch.load('Roy/data/p4s.pt')\n",
    "nodes = torch.load('Roy/data/nodes.pt')\n",
    "labels = torch.load('Roy/data/labels.pt')\n",
    "atom_mask = torch.load('Roy/data/atom_mask.pt')\n",
    "edge_mask = torch.from_numpy(np.load('Roy/data/edge_mask.npy'))\n",
    "edges = torch.from_numpy(np.load('Roy/data/edges.npy'))\n",
    "\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset_all = TensorDataset(labels, p4s, nodes, atom_mask, edge_mask)\n",
    "\n",
    "# Define the split ratios\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "# Calculate the lengths for each split\n",
    "total_size = len(dataset_all)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = int(total_size * val_ratio)\n",
    "test_size = total_size - train_size - val_size  # Ensure all data is used\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset_all, [train_size, val_size, test_size])\n",
    "\n",
    "# Create a dictionary to hold the datasets\n",
    "datasets = {\n",
    "    \"train\": train_dataset,\n",
    "    \"val\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "}\n",
    "\n",
    "# datasets = {split: TensorDataset(labels, p4s,\n",
    "                                 # nodes, atom_mask, edge_mask) for split in [\"train\", \"val\", \"test\"]}\n",
    "\n",
    "\n",
    "dataloaders = {split: DataLoader(dataset,\n",
    "                                 batch_size=16,\n",
    "                                 # sampler=train_sampler if (split == 'train') else DistributedSampler(dataset, shuffle=False),\n",
    "                                 pin_memory=False,\n",
    "                                 # persistent_workers=True,\n",
    "                                 collate_fn = collate_fn,\n",
    "                                 drop_last=True if (split == 'train') else False,\n",
    "                                 num_workers=0)\n",
    "                    for split, dataset in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 1, 0, 0, 1, 1, 1, 1]),\n",
       " tensor([[[ 0.0510,  0.4339,  0.5472,  0.5168],\n",
       "          [-0.0619,  0.2719,  0.3645,  0.3418],\n",
       "          [-0.0521,  0.2184,  0.2930,  0.2747]],\n",
       " \n",
       "         [[ 0.2931,  0.2831, -0.2194,  0.2539],\n",
       "          [ 0.1917,  0.1846, -0.1419,  0.1649],\n",
       "          [ 0.1467,  0.1419, -0.1088,  0.1265]],\n",
       " \n",
       "         [[ 0.2834, -0.1486, -0.0193,  0.1235],\n",
       "          [ 0.2280, -0.1222, -0.0152,  0.1000],\n",
       "          [ 0.1812, -0.1030, -0.0113,  0.0809]],\n",
       " \n",
       "         [[ 1.0000, -0.7343,  1.0000,  1.0000],\n",
       "          [ 0.1595, -0.1188,  0.1612,  0.1610],\n",
       "          [ 0.1382, -0.0996,  0.1375,  0.1374]],\n",
       " \n",
       "         [[-0.2010, -0.0383, -0.0326,  0.0793],\n",
       "          [-0.1457, -0.0260, -0.0241,  0.0575],\n",
       "          [-0.1455, -0.0258, -0.0233,  0.0571]],\n",
       " \n",
       "         [[ 0.4913, -0.0570,  0.4328,  0.4165],\n",
       "          [ 0.3204, -0.0389,  0.2890,  0.2770],\n",
       "          [ 0.2803, -0.0322,  0.2448,  0.2360]],\n",
       " \n",
       "         [[ 0.1178, -0.3572, -0.2832,  0.2993],\n",
       "          [ 0.0994, -0.2857, -0.2264,  0.2396],\n",
       "          [ 0.0829, -0.2617, -0.2044,  0.2169]],\n",
       " \n",
       "         [[ 0.6492, -0.0221,  0.0054,  0.2326],\n",
       "          [ 0.2552, -0.0119,  0.0016,  0.0915],\n",
       "          [ 0.2127, -0.0079,  0.0018,  0.0763]],\n",
       " \n",
       "         [[-0.5615,  1.0000,  0.0068,  0.5024],\n",
       "          [-0.2685,  0.4788,  0.0039,  0.2405],\n",
       "          [-0.0634,  0.1114,  0.0017,  0.0561]],\n",
       " \n",
       "         [[-1.0656,  0.4920,  0.4353,  0.5834],\n",
       "          [-0.5835,  0.2721,  0.2396,  0.3207],\n",
       "          [-0.1545,  0.0712,  0.0629,  0.0845]]], dtype=torch.float64),\n",
       " tensor([[[0.1485],\n",
       "          [0.0000],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.5254],\n",
       "          [0.1485],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.5296],\n",
       "          [0.0000],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.5296],\n",
       "          [0.1485],\n",
       "          [1.0000]],\n",
       " \n",
       "         [[0.9986],\n",
       "          [0.1485],\n",
       "          [0.1485]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.0000],\n",
       "          [0.0000]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.1485],\n",
       "          [0.1485]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.1485],\n",
       "          [0.5254]],\n",
       " \n",
       "         [[0.9986],\n",
       "          [1.0000],\n",
       "          [0.1485]],\n",
       " \n",
       "         [[0.1485],\n",
       "          [0.1485],\n",
       "          [0.0000]]], dtype=torch.float64),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]],\n",
       " \n",
       "         [[False,  True,  True],\n",
       "          [ True, False,  True],\n",
       "          [ True,  True, False]]]),\n",
       " [tensor([ 0,  0,  0,  3,  3,  3,  6,  6,  6,  9,  9,  9, 12, 12, 12, 15, 15, 15,\n",
       "          18, 18, 18, 21, 21, 21, 24, 24, 24, 27, 27, 27]),\n",
       "  tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "          18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29])]]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloaders['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12500, 3, 4])\n",
      "torch.Size([12500, 3, 1])\n",
      "torch.Size([12500, 3])\n",
      "torch.Size([12500, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(p4s.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7fc61cfc0dc0>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x7fc61cfc0760>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7fc61cfc0610>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size, n_nodes...\n",
    "# 1         , 139...\n",
    "\n",
    "# labels: torch.Size([1])\n",
    "# p4s: torch.Size([1, 139, 4]) : 4-momentum\n",
    "# nodes: torch.Size([1, 139, 8]) : 8 is a one-hot vector of particle id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1#2500 #1\n",
    "n_nodes = 3 #139 \n",
    "device = 'cpu'\n",
    "dtype = torch.float32\n",
    "\n",
    "atom_positions = p4s[:, :, :].view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "\n",
    "atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n",
    "\n",
    "edges = [a.to(device) for a in edges]\n",
    "nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([139, 64])\n",
      "torch.Size([139, 128])\n",
      "torch.Size([139, 16])\n",
      "torch.Size([19321, 16])\n"
     ]
    }
   ],
   "source": [
    "print(atom_positions.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "print(atom_positions.shape) # p4s\n",
    "print(nodes.shape) # mass\n",
    "print(atom_mask.shape) # torch.ones\n",
    "print(edge_mask.shape) # adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_mask[0]#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0633, -0.0332,  0.0849,  0.0766], dtype=torch.float64)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4s[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([139, 16])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 139, 4])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p4s.shape # batch_size (number of jets or graphs), n_nodes (particles), n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom mask: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Atom positions (x features, 4-momenta): tensor([[ 2.8607e-01,  7.7928e-03, -2.6866e-01,  9.7974e-02,  1.6533e-01,\n",
      "         -2.5844e-02, -1.5798e-01, -4.1364e-02,  1.1594e+00, -2.3781e-01,\n",
      "         -1.1238e+00, -7.2296e-02,  4.2239e+00,  2.2232e-01, -4.1256e+00,\n",
      "          7.2634e-01,  1.7402e+00,  1.4311e-01, -1.6899e+00, -3.6378e-01,\n",
      "          2.1967e+00, -2.9915e-01, -2.1732e+00, -1.1518e-01,  1.6228e+00,\n",
      "         -1.0804e-01, -1.6155e+00, -1.0957e-01,  6.6001e+00,  3.5305e-01,\n",
      "         -6.5827e+00,  2.9199e-01,  3.8065e+00,  1.5963e-01, -3.7676e+00,\n",
      "          1.6072e-01,  1.3488e+01,  3.1063e-01, -1.3478e+01, -3.7756e-01,\n",
      "          4.1091e+00,  1.9105e-01, -4.1035e+00, -9.7634e-02,  2.1653e+01,\n",
      "          1.0296e+00, -2.1621e+01, -5.8444e-01,  6.7785e+00,  3.3111e-01,\n",
      "         -6.7674e+00, -2.0163e-01,  1.3265e+01,  4.9158e-01, -1.3246e+01,\n",
      "         -5.2290e-01,  2.9855e+00,  1.0084e-01, -2.9818e+00, -1.0923e-01,\n",
      "          3.7398e+01,  1.5851e+00, -3.7341e+01, -1.3249e+00],\n",
      "        [ 3.3558e+02,  1.2900e+01, -3.3515e+02, -1.0923e+01,  4.4238e+01,\n",
      "          1.8533e+00, -4.4175e+01, -1.4715e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
      "Nodes (scalars: mass & charge): tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n",
      "Edge mask: tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True, False, False, False, False, False, False, False, False,\n",
      "         False, False, False, False, False, False]])\n",
      "Edges: [tensor([   0,    0,    0,  ..., 2114, 2114, 2114]), tensor([   1,    2,    3,  ..., 2111, 2112, 2113])]\n"
     ]
    }
   ],
   "source": [
    "# Roy: x(atom_pos), edge_indx_tensor (edges = adj_matrix), edge_tensor (edge_mask = adj_matrix)\n",
    "print(\"Atom mask: {}\".format(atom_mask[:2]))\n",
    "print(\"Atom positions (x features, 4-momenta): {}\".format(atom_positions[:2]))\n",
    "print(\"Nodes (scalars: mass & charge): {}\".format(nodes[:2]))\n",
    "print(\"Edge mask: {}\".format(edge_mask[:2]))\n",
    "print(\"Edges: {}\".format(edges[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([   0,    0,    0,  ..., 2114, 2114, 2114]),\n",
       " tensor([   1,    2,    3,  ..., 2111, 2112, 2113])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[:2]#[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                         edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LorentzNet\n",
    "\n",
    "Now, let's look at the performance of LorentzNet on our dataset. Before, I projected only the first three features to the point-cloud, as a simpler test, but given the pt, rapidity, azimuthal_angle and the particle id, we can retrieve a more complete information, like its four-momentum, and also associated scalars, like its mass and charge, or particle id.\n",
    "\n",
    "Here, each jet is denoted as a graph consisting of an unordered set of nodes (particles), where each node is considered a point-cloud living in $\\mathbb{R}^{4}$. Each jet can have different number of constituent particles, and the LorentzNet is built upon the universal approximation theorem for Lorentz-equivariant functions. This is the sketched out architecture:\n",
    "\n",
    "<img src=\"../figures/LorentzNet.png\" width=65% style=\"margin-left:auto; margin-right:auto\">\n",
    "\n",
    "The **input layer** consists of four-momentum vectors of particles from a collision event, and the input is a set of vectors $f_i = v_i \\mathop{\\bigoplus} s_i$, where $f_i = \\{E_i, p_{x_i}, p_{y_i}, p_{z_i}\\}$ is the four-momentum vector for particle $i$, and $s_i$ is a set of scalars for particle $i$, like mass, charge, particle id. \n",
    "\n",
    "Now, the **Lorentz Group Equivariant Block** (LGEB) constitutes of $\\phi_{e}(x), \\phi_{h}(x), \\phi_{x}(x)$, which are continuous functions modeled by neural networks, where $\\phi_{e}(x)$ is responsible for computing the edge message between particles $i$ and $j$ for the $l$-th layer via:\n",
    "\n",
    "\\begin{equation}\n",
    "m_{ij}^{l} = \\phi_{e}(h_i, h_j, \\psi(||x_{i}^{l} - x_{j}^{l}||^2), \\psi(\\langle x_{i}, x_{j}\\rangle)),\n",
    "\\end{equation}\n",
    "\n",
    "and $\\psi(\\cdot) = sgn(\\cdot)log(|\\cdot| + 1)$ is just a function to normalize large numbers from different distributions, making optimization easier. Now, $\\phi_{x}(x)$ is the responsible for the Minkowski dot product attention, which is proposed in the paper as:\n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}^{l+1} = x_{i}^{l} + c\\sum_{j\\in\\mathcal{N}(i)}\\phi_{x}(m_{ij}^{l})\\cdot x_{j}^{l},\n",
    "\\end{equation}\n",
    "\n",
    "so the feature for each particle in the next layer is the feature for the current layer and a weighted feature sum of neighboring particles $\\mathcal{N}(i)$, where the weights depend on each edge message calculated before and $c$, which is just a scalar to control the scale again. Finally, $\\phi_{h}(x)$ updates the scalar features for particle $i$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "h_{i}^{l+1} = h_{i}^{l} + \\phi_{h}(h_{i}^{l}, \\sum_{j\\in\\mathcal{N}(i)}w_{ij}m_{ij}^{l}),\n",
    "\\end{equation}\n",
    "\n",
    "where $w_{ij}$ learns the edge significance between particle $i$ and $j$ through again another neural network $\\phi_{m}(m_{ij}^{l}) \\in [0, 1]$ . It's important to note that the initial scalar features $h_{i}^{0}$ are embedded into deeper representations through the layers, and since the **edge message $m_{ij}$** contains information about **both** **$x_{i}$** and **$x_{j}$**, in the very end, if you look into the diagram, you'll see that only **$h^{l}$** is returned, a clever trick to avoid redundancy.\n",
    "\n",
    "\n",
    "Lastly, given the transversal momentum **$p_{T}$**, pseudo-rapidity **$\\eta$**, and azimuthal angle **$\\phi$** and the particle id, it is possible to retrieve the 4-momenta for each particle. The energyflow package facilitates this, so, in the code, this is the first thing done to retrieve our dataset.\n",
    "\n",
    "\n",
    "I am going to use the official implementation from the Author's GitHub here: https://github.com/sdogsq/LorentzNet-release/tree/maine/tree/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Some auxiliary functions\"\"\"\n",
    "\n",
    "def unsorted_segment_sum(data, segment_ids, num_segments):\n",
    "    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_sum`.\n",
    "    Adapted from https://github.com/vgsatorras/egnn.\n",
    "    '''\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    return result\n",
    "\n",
    "def unsorted_segment_mean(data, segment_ids, num_segments):\n",
    "    r'''Custom PyTorch op to replicate TensorFlow's `unsorted_segment_mean`.\n",
    "    Adapted from https://github.com/vgsatorras/egnn.\n",
    "    '''\n",
    "    result = data.new_zeros((num_segments, data.size(1)))\n",
    "    count = data.new_zeros((num_segments, data.size(1)))\n",
    "    result.index_add_(0, segment_ids, data)\n",
    "    count.index_add_(0, segment_ids, torch.ones_like(data))\n",
    "    return result / count.clamp(min=1)\n",
    "\n",
    "def normsq4(p):\n",
    "    r''' Minkowski square norm\n",
    "         `\\|p\\|^2 = p[0]^2-p[1]^2-p[2]^2-p[3]^2`\n",
    "    ''' \n",
    "    psq = torch.pow(p, 2)\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "    \n",
    "def dotsq4(p,q):\n",
    "    r''' Minkowski inner product\n",
    "         `<p,q> = p[0]q[0]-p[1]q[1]-p[2]q[2]-p[3]q[3]`\n",
    "    '''\n",
    "    psq = p*q\n",
    "    return 2 * psq[..., 0] - psq.sum(dim=-1)\n",
    "\n",
    "def normA_fn(A):\n",
    "    return lambda p: torch.einsum('...i, ij, ...j->...', p, A, p)\n",
    "\n",
    "def dotA_fn(A):\n",
    "    return lambda p, q: torch.einsum('...i, ij, ...j->...', p, A, q)\n",
    "    \n",
    "def psi(p):\n",
    "    ''' `\\psi(p) = Sgn(p) \\cdot \\log(|p| + 1)`\n",
    "    '''\n",
    "    return torch.sign(p) * torch.log(torch.abs(p) + 1)\n",
    "\n",
    "\n",
    "\"\"\"Lorentz Group-Equivariant Block\"\"\"\n",
    "\n",
    "class LGEB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n",
    "        super(LGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.include_x = include_x\n",
    "        self.phi_e = nn.Sequential(\n",
    "            nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "        self.A = A\n",
    "        self.norm_fn = normA_fn(A) if A is not None else normsq4\n",
    "        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n",
    "        \n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        # print(\"m_model output: \", out.shape)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n",
    "        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n",
    "        out = self.phi_e(out)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m): # norms\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        # print(\"m: \", m.shape)\n",
    "        # print(\"trans: \", trans.shape)\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        # print(\"trans: \", trans.shape)\n",
    "        # print(\"x.size: \", x.size(0))\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight # * norms[i, j], smth like that, or norms\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = self.norm_fn(x_diff).unsqueeze(1)\n",
    "        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        if self.include_x:\n",
    "            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n",
    "        else:\n",
    "            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            # print(\"X: \", x)\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "            # print(\"phi_x(X) = \", x, '\\n---\\n')\n",
    "            \n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class LorentzNet(nn.Module):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of LGEB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n",
    "        super(LorentzNet, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.LGEBs = nn.ModuleList([LGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "\n",
    "        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.LGEBs[i](h, x, edges, node_attr=scalars)\n",
    "        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "            \n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "\n",
    "        # print(\"Final preds: \\n\", pred.cpu().detach().numpy())\n",
    "        return pred.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the official code for the classical, just for sanity checking, let's test for equivariance\n",
    "\n",
    "The cell below is just an auxiliary function to give us the boosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "\n",
    "# Speed of light (m/s)\n",
    "c = 299792458\n",
    "\n",
    "\"\"\"Lorentz transformations describe the transition between two inertial reference\n",
    "frames F and F', each of which is moving in some direction with respect to the\n",
    "other. This code only calculates Lorentz transformations for movement in the x\n",
    "direction with no spatial rotation (i.e., a Lorentz boost in the x direction).\n",
    "The Lorentz transformations are calculated here as linear transformations of\n",
    "four-vectors [ct, x, y, z] described by Minkowski space. Note that t (time) is\n",
    "multiplied by c (the speed of light) in the first entry of each four-vector.\n",
    "\n",
    "Thus, if X = [ct; x; y; z] and X' = [ct'; x'; y'; z'] are the four-vectors for\n",
    "two inertial reference frames and X' moves in the x direction with velocity v\n",
    "with respect to X, then the Lorentz transformation from X to X' is X' = BX,\n",
    "where\n",
    "\n",
    "    | γ  -γβ  0  0|\n",
    "B = |-γβ  γ   0  0|\n",
    "    | 0   0   1  0|\n",
    "    | 0   0   0  1|\n",
    "\n",
    "is the matrix describing the Lorentz boost between X and X',\n",
    "γ = 1 / √(1 - v²/c²) is the Lorentz factor, and β = v/c is the velocity as\n",
    "a fraction of c.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def beta(velocity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculates β = v/c, the given velocity as a fraction of c\n",
    "    >>> beta(c)\n",
    "    1.0\n",
    "    >>> beta(199792458)\n",
    "    0.666435904801848\n",
    "    \"\"\"\n",
    "    if velocity > c:\n",
    "        raise ValueError(\"Speed must not exceed light speed 299,792,458 [m/s]!\")\n",
    "    elif velocity < 1:\n",
    "        # Usually the speed should be much higher than 1 (c order of magnitude)\n",
    "        raise ValueError(\"Speed must be greater than or equal to 1!\")\n",
    "\n",
    "    return velocity / c\n",
    "\n",
    "\n",
    "def gamma(velocity: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Lorentz factor γ = 1 / √(1 - v²/c²) for a given velocity\n",
    "    >>> gamma(4)\n",
    "    1.0000000000000002\n",
    "    >>> gamma(1e5)\n",
    "    1.0000000556325075\n",
    "    >>> gamma(3e7)\n",
    "    1.005044845777813\n",
    "    >>> gamma(2.8e8)\n",
    "    2.7985595722318277\n",
    "    \"\"\"\n",
    "    return 1 / sqrt(1 - beta(velocity) ** 2)\n",
    "\n",
    "\n",
    "def transformation_matrix(velocity: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Lorentz transformation matrix for movement in the x direction:\n",
    "\n",
    "    | γ  -γβ  0  0|\n",
    "    |-γβ  γ   0  0|\n",
    "    | 0   0   1  0|\n",
    "    | 0   0   0  1|\n",
    "\n",
    "    where γ is the Lorentz factor and β is the velocity as a fraction of c\n",
    "    >>> transformation_matrix(29979245)\n",
    "    array([[ 1.00503781, -0.10050378,  0.        ,  0.        ],\n",
    "           [-0.10050378,  1.00503781,  0.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  1.        ,  0.        ],\n",
    "           [ 0.        ,  0.        ,  0.        ,  1.        ]])\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [\n",
    "            [gamma(velocity), -gamma(velocity) * beta(velocity), 0, 0],\n",
    "            [-gamma(velocity) * beta(velocity), gamma(velocity), 0, 0],\n",
    "            [0, 0, 1, 0],\n",
    "            [0, 0, 0, 1],\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_scalar = 8 in original !\n",
    "model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.99137795  0.67032087  0.0894613  -0.5694246 ]\n",
      "h after (just the first particle): \n",
      " [-2.791172    1.3770922  -0.21466088  0.10523695]\n",
      "Final preds: \n",
      " [[0.07923214 0.10442689]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 0.6250086  -0.4157089   0.19434586  3.7227166 ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... taking any random nonsense transformation in the four-momentum vectors\n",
    "i.e.: multiplying by 0.1. Does the hidden rep stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 1.5326474  -0.09580445 -0.10811514  4.131195  ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.99137795  0.67032087  0.0894613  -0.5694246 ]\n",
      "h after (just the first particle): \n",
      " [-3.3101714  0.5831776 -2.4134033 -1.6305795]\n",
      "Final preds: \n",
      " [[-0.49384588 -0.16958103]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= 0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even though the final logits in this case wasn't different, if we look the last output of h (which contains both scalar and 4-momenta information), it changed! Now, what about Lorentz transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.22275785 -0.00887579 -0.45730796  0.4752541 ]\n",
      "h after (just the first particle): \n",
      " [ 0.6251303  -0.41550016  0.1935861   3.721067  ]\n",
      "Final preds: \n",
      " [[0.25635535 0.08354717]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.99137795  0.67032087  0.0894613  -0.5694246 ]\n",
      "h after (just the first particle): \n",
      " [-2.791169    1.37709    -0.21465892  0.10523733]\n",
      "Final preds: \n",
      " [[-0.49874073 -0.1710372 ]]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x= (torch.tensor(transformation_matrix(220000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equivariance works. Finally, let's train on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 179\n",
      " train samples: 10000\n",
      " val samples: 1250\n",
      " test samples: 1250\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:04, 125.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 1/60 \t Batch 624/625 \t Loss 0.6971 \t Running Acc 0.503 \t Total Acc 0.503 \t Avg Batch Time 0.0080\n",
      "Time: train: 4.98 \t Train loss 0.6971 \t Train acc: 0.5028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 268.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6919 \t Running Acc 3.911 \t Total Acc 0.494 \t Avg Batch Time 0.0005\n",
      "New best validation model, saving...\n",
      "Epoch 0/60 finished.\n",
      "Train time: 4.98 \t Val time 0.30\n",
      "Train loss 0.6971 \t Train acc: 0.5028\n",
      "Val loss: 0.6914 \t Val acc: 0.4944\n",
      "Best val acc: 0.4944 at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:04, 136.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 2/60 \t Batch 624/625 \t Loss 0.6845 \t Running Acc 0.567 \t Total Acc 0.567 \t Avg Batch Time 0.0073\n",
      "Time: train: 4.59 \t Train loss 0.6845 \t Train acc: 0.5672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 257.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6785 \t Running Acc 4.703 \t Total Acc 0.594 \t Avg Batch Time 0.0005\n",
      "New best validation model, saving...\n",
      "Epoch 1/60 finished.\n",
      "Train time: 4.59 \t Val time 0.31\n",
      "Train loss 0.6845 \t Train acc: 0.5672\n",
      "Val loss: 0.6770 \t Val acc: 0.5944\n",
      "Best val acc: 0.5944 at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [00:03, 167.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 3/60 \t Batch 624/625 \t Loss 0.6682 \t Running Acc 0.595 \t Total Acc 0.595 \t Avg Batch Time 0.0060\n",
      "Time: train: 3.72 \t Train loss 0.6682 \t Train acc: 0.5952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [00:00, 285.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6489 \t Running Acc 4.905 \t Total Acc 0.620 \t Avg Batch Time 0.0004\n",
      "New best validation model, saving...\n",
      "Epoch 2/60 finished.\n",
      "Train time: 3.72 \t Val time 0.28\n",
      "Train loss 0.6682 \t Train acc: 0.5952\n",
      "Val loss: 0.6467 \t Val acc: 0.6200\n",
      "Best val acc: 0.6200 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:00, 143.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[254], line 217\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m### training and testing\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m test(model, res, model_path, log_path)\n",
      "Cell \u001b[0;32mIn[254], line 93\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, res, N_EPOCHS, model_path, log_path)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, res, N_EPOCHS, model_path, log_path):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m### training and validation\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> 93\u001b[0m         train_res \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: train: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train loss \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train acc: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# if epoch % args.val_interval == 0:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m             \n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# if (args.local_rank == 0):\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[254], line 46\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, epoch, loader, partition, N_EPOCHS)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m partition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     45\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 46\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m partition \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# save labels and probilities for ROC / AUC\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# print(\"Preds \", pred)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     score \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(pred, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    178\u001b[0m         group,\n\u001b[1;32m    179\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m         state_steps,\n\u001b[1;32m    186\u001b[0m     )\n\u001b[0;32m--> 188\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/adamw.py:419\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    416\u001b[0m param\u001b[38;5;241m.\u001b[39mmul_(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m lr \u001b[38;5;241m*\u001b[39m weight_decay)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import json, time\n",
    "import utils_lorentz\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run(model, epoch, loader, partition, N_EPOCHS=None):\n",
    "    if partition == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    res = {'time':0, 'correct':0, 'loss': 0, 'counter': 0, 'acc': 0,\n",
    "           'loss_arr':[], 'correct_arr':[],'label':[],'score':[]}\n",
    "\n",
    "    tik = time.time()\n",
    "    loader_length = len(loader)\n",
    "\n",
    "    for i, (label, p4s, nodes, atom_mask, edge_mask, edges) in tqdm(enumerate(loader)):\n",
    "        if partition == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        batch_size, n_nodes, _ = p4s.size()\n",
    "        atom_positions = p4s.view(batch_size * n_nodes, -1).to(device, dtype)\n",
    "        atom_mask = atom_mask.view(batch_size * n_nodes, -1).to(device)\n",
    "        edge_mask = edge_mask.reshape(batch_size * n_nodes * n_nodes, -1).to(device)\n",
    "        nodes = nodes.view(batch_size * n_nodes, -1).to(device,dtype)\n",
    "        edges = [a.to(device) for a in edges]\n",
    "        label = label.to(device, dtype).long()\n",
    "\n",
    "        pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                         edge_mask=edge_mask, n_nodes=n_nodes)\n",
    "        \n",
    "        predict = pred.max(1).indices\n",
    "        correct = torch.sum(predict == label).item()\n",
    "        loss = loss_fn(pred, label)\n",
    "        \n",
    "        if partition == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        elif partition == 'test':\n",
    "            # save labels and probilities for ROC / AUC\n",
    "            # print(\"Preds \", pred)\n",
    "            score = torch.nn.functional.softmax(pred, dim = -1)\n",
    "            # print(\"Score test \", score)\n",
    "            # raise\n",
    "            res['label'].append(label)\n",
    "            res['score'].append(score)\n",
    "\n",
    "        res['time'] = time.time() - tik\n",
    "        res['correct'] += correct\n",
    "        res['loss'] += loss.item() * batch_size\n",
    "        res['counter'] += batch_size\n",
    "        res['loss_arr'].append(loss.item())\n",
    "        res['correct_arr'].append(correct)\n",
    "\n",
    "        # if i != 0 and i % args.log_interval == 0:\n",
    "        \n",
    "    running_loss = sum(res['loss_arr'])/len(res['loss_arr'])\n",
    "    running_acc = sum(res['correct_arr'])/(len(res['correct_arr'])*batch_size)\n",
    "    avg_time = res['time']/res['counter'] * batch_size\n",
    "    tmp_counter = res['counter']\n",
    "    tmp_loss = res['loss'] / tmp_counter\n",
    "    tmp_acc = res['correct'] / tmp_counter\n",
    "\n",
    "    if N_EPOCHS:\n",
    "        print(\">> %s \\t Epoch %d/%d \\t Batch %d/%d \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n",
    "             (partition, epoch + 1, N_EPOCHS, i, loader_length, running_loss, running_acc, tmp_acc, avg_time))\n",
    "    else:\n",
    "        print(\">> %s \\t Loss %.4f \\t Running Acc %.3f \\t Total Acc %.3f \\t Avg Batch Time %.4f\" %\n",
    "             (partition, running_loss, running_acc, tmp_acc, avg_time))\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "    # ---------- reduce -----------\n",
    "    if partition == 'test':\n",
    "        res['label'] = torch.cat(res['label']).unsqueeze(-1)\n",
    "        res['score'] = torch.cat(res['score'])\n",
    "        res['score'] = torch.cat((res['label'],res['score']),dim=-1)\n",
    "    res['counter'] = res['counter']\n",
    "    res['loss'] = res['loss'] / res['counter']\n",
    "    res['acc'] = res['correct'] / res['counter']\n",
    "    return res\n",
    "\n",
    "def train(model, res, N_EPOCHS, model_path, log_path):\n",
    "    ### training and validation\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_res = run(model, epoch, dataloaders['train'], partition='train', N_EPOCHS = N_EPOCHS)\n",
    "        print(\"Time: train: %.2f \\t Train loss %.4f \\t Train acc: %.4f\" % (train_res['time'],train_res['loss'],train_res['acc']))\n",
    "        # if epoch % args.val_interval == 0:\n",
    "            \n",
    "        # if (args.local_rank == 0):\n",
    "        torch.save(model.state_dict(), os.path.join(model_path, \"checkpoint-epoch-{}.pt\".format(epoch)) )\n",
    "        with torch.no_grad():\n",
    "            val_res = run(model, epoch, dataloaders['val'], partition='val')\n",
    "            \n",
    "        # if (args.local_rank == 0): # only master process save\n",
    "        res['lr'].append(optimizer.param_groups[0]['lr'])\n",
    "        res['train_time'].append(train_res['time'])\n",
    "        res['val_time'].append(val_res['time'])\n",
    "        res['train_loss'].append(train_res['loss'])\n",
    "        res['train_acc'].append(train_res['acc'])\n",
    "        res['val_loss'].append(val_res['loss'])\n",
    "        res['val_acc'].append(val_res['acc'])\n",
    "        res['epochs'].append(epoch)\n",
    "\n",
    "        ## save best model\n",
    "        if val_res['acc'] > res['best_val']:\n",
    "            print(\"New best validation model, saving...\")\n",
    "            torch.save(model.state_dict(), os.path.join(model_path,\"best-val-model.pt\"))\n",
    "            res['best_val'] = val_res['acc']\n",
    "            res['best_epoch'] = epoch\n",
    "\n",
    "        print(\"Epoch %d/%d finished.\" % (epoch, N_EPOCHS))\n",
    "        print(\"Train time: %.2f \\t Val time %.2f\" % (train_res['time'], val_res['time']))\n",
    "        print(\"Train loss %.4f \\t Train acc: %.4f\" % (train_res['loss'], train_res['acc']))\n",
    "        print(\"Val loss: %.4f \\t Val acc: %.4f\" % (val_res['loss'], val_res['acc']))\n",
    "        print(\"Best val acc: %.4f at epoch %d.\" % (res['best_val'],  res['best_epoch']))\n",
    "\n",
    "        json_object = json.dumps(res, indent=4)\n",
    "        with open(os.path.join(log_path, \"train-result-epoch{}.json\".format(epoch)), \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "\n",
    "        ## adjust learning rate\n",
    "        if (epoch < 31):\n",
    "            lr_scheduler.step(metrics=val_res['acc'])\n",
    "        else:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']*0.5\n",
    "\n",
    "\n",
    "def test(model, res, model_path, log_path):\n",
    "    ### test on best model\n",
    "    best_model = torch.load(os.path.join(model_path, \"best-val-model.pt\"), map_location=device)\n",
    "    model.load_state_dict(best_model)\n",
    "    with torch.no_grad():\n",
    "        test_res = run(model, 0, dataloaders['test'], partition='test')\n",
    "\n",
    "    print(\"Final \", test_res['score'])\n",
    "    pred = test_res['score'].cpu()\n",
    "\n",
    "    np.save(os.path.join(log_path, \"score.npy\"), pred)\n",
    "    fpr, tpr, thres, eB, eS  = utils_lorentz.buildROC(pred[...,0], pred[...,2])\n",
    "    auc = utils_lorentz.roc_auc_score(pred[...,0], pred[...,2])\n",
    "\n",
    "    metric = {'test_loss': test_res['loss'], 'test_acc': test_res['acc'],\n",
    "              'test_auc': auc, 'test_1/eB_0.3':1./eB[0],'test_1/eB_0.5':1./eB[1]}\n",
    "    res.update(metric)\n",
    "    print(\"Test: Loss %.4f \\t Acc %.4f \\t AUC: %.4f \\t 1/eB 0.3: %.4f \\t 1/eB 0.5: %.4f\"\\\n",
    "           % (test_res['loss'], test_res['acc'], auc, 1./eB[0], 1./eB[1]))\n",
    "    json_object = json.dumps(res, indent=4)\n",
    "    with open(os.path.join(log_path, \"test-result.json\"), \"w\") as outfile:\n",
    "        outfile.write(json_object)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    N_EPOCHS = 60\n",
    "\n",
    "    model_path = \"models/LorentzNet/\"\n",
    "    log_path = \"logs/LorentzNet/\"\n",
    "    # utils_lorentz.args_init(args)\n",
    "\n",
    "    ### set random seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ### initialize cuda\n",
    "    # dist.init_process_group(backend='nccl')\n",
    "    device = 'cpu' #torch.device(\"cuda\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    ### load data\n",
    "    # dataloaders = retrieve_dataloaders( batch_size,\n",
    "    #                                     num_data=100000, # use all data\n",
    "    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n",
    "    #                                     num_workers=0,\n",
    "    #                                     use_one_hot=True)\n",
    "\n",
    "    ### create parallel model\n",
    "    model = LorentzNet(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 1,\\\n",
    "                       c_weight = 1e-3)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    ### print model and dataset information\n",
    "    # if (args.local_rank == 0):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Model Size:\", pytorch_total_params)\n",
    "    for (split, dataloader) in dataloaders.items():\n",
    "        print(f\" {split} samples: {len(dataloader.dataset)}\")\n",
    "\n",
    "    ### optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "    ### lr scheduler\n",
    "    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n",
    "    lr_scheduler = utils_lorentz.GradualWarmupScheduler(optimizer, multiplier=1,\\\n",
    "                                                warmup_epoch=5,\\\n",
    "                                                after_scheduler=base_scheduler) ## warmup\n",
    "\n",
    "    ### loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### initialize logs\n",
    "    res = {'epochs': [], 'lr' : [],\\\n",
    "           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n",
    "           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n",
    "\n",
    "    ### training and testing\n",
    "    print(\"Training...\")\n",
    "    train(model, res, N_EPOCHS, model_path, log_path)\n",
    "    test(model, res, model_path, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's move to our Lie EQGNN\n",
    "\n",
    "In LorentzNet, we explained earlier that $\\phi_{e}(x), \\phi_{h}(x), \\phi_{x}(x)$ are neural nets. Here, each $\\phi$ is modeled by a parameterized circuit, and in the code below, you can choose to use a hybrid setting too.\n",
    "\n",
    "## Infrared safe observables\n",
    "Another interesting bias to incorporate is the infrared and collinear (IRC) safety. An infrared and collinear safe observable is the same in the presence or absence of soft or collinear particles. In [6], an IRC-safe equivariant (classical) GNN was proposed for tagging simulated semi-visible jets from Hidden Valley models, showing superior performance on this data for Beyond the Standard Model (BSM) search.\n",
    "\n",
    "We saw before that in LorentzNet, the message is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "m_{ij}^{l} = \\phi_{e}(h_i, h_j, \\psi(||x_{i}^{l} - x_{j}^{l}||^2), \\psi(\\langle x_{i}, x_{j}\\rangle)),\n",
    "\\end{equation}\n",
    "\n",
    "Now, intuitively, an IRC-safe model should give us a graph that stays invariant under any particle corresponding to an infinitesimal emission, or a collinear one. This means that such particles have no influence on other particles in our point cloud. But, how can we do this? Message passing!\n",
    "\n",
    "\\begin{align}\n",
    "\\text{IR safety}:& m^{l}(i,j) \\rightarrow 0 \\text{ as } z \\rightarrow 0,\\\\\n",
    "\\text{C safety}:& m^{l}(i,j + r) = m^{l}(i,j) + m^{l}(i,r) \\text{ as } \\Delta_{jr} \\rightarrow 0,\n",
    "\\end{align}\n",
    "\n",
    "To ensure IR safety, we can not use $z_j$ directly, as it breaks equivariance. We propose, thus, the following substitution:\n",
    "\n",
    "\\begin{equation}\n",
    "m_{ij}^{l} = \\frac{\\langle x_i , x_j\\rangle}{\\sum_{k \\in \\mathcal{N(j)} } \\langle x_i , x_k\\rangle } \\cdot \\phi_{e}(h_i, h_j, \\psi(||x_{i}^{l} - x_{j}^{l}||^2), \\psi(\\langle x_{i}, x_{j}\\rangle)),\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Where $\\langle \\cdot,\\cdot\\rangle$ is the Minkowski inner product, and $\\mathcal{N(j)}$ represents all neighboring particles of $j$. If $j$ is a soft particle, then the Minkowski inner product should be small, thus , which makes the edge connection irrelevant, thus ensuring IR safety. Also, any Lorentz transformation preserves the inner product, so the message should remain symmetry-preserving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "\n",
    "\"\"\"\n",
    "    Quantum Lie-Equivariant Block (QLieGEB).\n",
    "    \n",
    "        - Given the Lie generators found (i.e.: through LieGAN, oracle-preserving latent flow, or some other approach\n",
    "          that we develop further), once the metric tensor J is found via the equation:\n",
    "\n",
    "                          L.J + J.(L^T) = 0,\n",
    "                          \n",
    "          we just have to specify the metric to make the model symmetry-preserving to the corresponding Lie group. \n",
    "          In the cells below, I will show first how the model preserves symmetries (starting with the default Lorentz group),\n",
    "          and when we change J to some other metric (Euclidean, for example), Lorentz boosts break equivariance, while other\n",
    "          transformations preserve it (rotations, for the example shown in the cells below)\n",
    "\"\"\"\n",
    "class QLieGEB(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_node_attr=0,\n",
    "                 dropout = 0., c_weight=1.0, last_layer=False, A=None, include_x=False):\n",
    "        super(QLieGEB, self).__init__()\n",
    "        self.c_weight = c_weight\n",
    "        n_edge_attr = 2 if not include_x else 10 # dims for Minkowski norm & inner product\n",
    "\n",
    "        self.include_x = include_x\n",
    "\n",
    "        \"\"\"\n",
    "            phi_e: input size: n_qubits -> output size: n_qubits\n",
    "            n_hidden has to be equal to n_input (n_input * 2 + n_edge_attr),\n",
    "            but this is just considering that this is a simple working example.\n",
    "        \"\"\"\n",
    "        self.phi_e = DressedQuantumNet(n_input * 2 + n_edge_attr)\n",
    "\n",
    "        n_hidden = n_input * 2 + n_edge_attr\n",
    "        self.phi_h = nn.Sequential(\n",
    "            nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "            nn.BatchNorm1d(n_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        self.phi_x = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_hidden),\n",
    "            nn.ReLU(),\n",
    "            layer)\n",
    "\n",
    "        self.phi_m = nn.Sequential(\n",
    "            nn.Linear(n_hidden, 1),\n",
    "            nn.Sigmoid())        \n",
    "        # self.phi_e = nn.Sequential(\n",
    "        #     nn.Linear(n_input * 2 + n_edge_attr, n_hidden, bias=False),\n",
    "        #     nn.BatchNorm1d(n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_hidden, n_hidden),\n",
    "        #     nn.ReLU())\n",
    "\n",
    "        # self.phi_h = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden + n_input + n_node_attr, n_hidden),\n",
    "        #     nn.BatchNorm1d(n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(n_hidden, n_output))\n",
    "\n",
    "        # layer = nn.Linear(n_hidden, 1, bias=False)\n",
    "        # torch.nn.init.xavier_uniform_(layer.weight, gain=0.001)\n",
    "\n",
    "        # self.phi_x = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden, n_hidden),\n",
    "        #     nn.ReLU(),\n",
    "        #     layer)\n",
    "\n",
    "        # self.phi_m = nn.Sequential(\n",
    "        #     nn.Linear(n_hidden, 1),\n",
    "        #     nn.Sigmoid())\n",
    "        \n",
    "        self.last_layer = last_layer\n",
    "        if last_layer:\n",
    "            del self.phi_x\n",
    "\n",
    "        self.A = A\n",
    "        self.norm_fn = normA_fn(A) if A is not None else normsq4\n",
    "        self.dot_fn = dotA_fn(A) if A is not None else dotsq4\n",
    "\n",
    "    def m_model(self, hi, hj, norms, dots):\n",
    "        out = torch.cat([hi, hj, norms, dots], dim=1)\n",
    "        # print(\"Before embedding to |psi> : \", out)\n",
    "        out = self.phi_e(out).squeeze(0)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def m_model_extended(self, hi, hj, norms, dots, xi, xj):\n",
    "        out = torch.cat([hi, hj, norms, dots, xi, xj], dim=1)\n",
    "        out = self.phi_e(out).squeeze(0)\n",
    "        w = self.phi_m(out)\n",
    "        out = out * w\n",
    "        return out\n",
    "\n",
    "    def h_model(self, h, edges, m, node_attr):\n",
    "        i, j = edges\n",
    "        agg = unsorted_segment_sum(m, i, num_segments=h.size(0))\n",
    "        agg = torch.cat([h, agg, node_attr], dim=1)\n",
    "        out = h + self.phi_h(agg)\n",
    "        return out\n",
    "\n",
    "    def x_model(self, x, edges, x_diff, m):\n",
    "        i, j = edges\n",
    "        trans = x_diff * self.phi_x(m)\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        # From https://github.com/vgsatorras/egnn\n",
    "        # This is never activated but just in case it explosed it may save the train\n",
    "        trans = torch.clamp(trans, min=-100, max=100)\n",
    "        agg = unsorted_segment_mean(trans, i, num_segments=x.size(0))\n",
    "        x = x + agg * self.c_weight\n",
    "        return x\n",
    "\n",
    "    def minkowski_feats(self, edges, x):\n",
    "        i, j = edges\n",
    "        x_diff = x[i] - x[j]\n",
    "        norms = self.norm_fn(x_diff).unsqueeze(1)\n",
    "        dots = self.dot_fn(x[i], x[j]).unsqueeze(1)\n",
    "        norms, dots = psi(norms), psi(dots)\n",
    "        return norms, dots, x_diff\n",
    "\n",
    "    def forward(self, h, x, edges, node_attr=None):\n",
    "        i, j = edges\n",
    "        norms, dots, x_diff = self.minkowski_feats(edges, x)\n",
    "\n",
    "        if self.include_x:\n",
    "            m = self.m_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n",
    "        else:\n",
    "            m = self.m_model(h[i], h[j], norms, dots) # [B*N, hidden]\n",
    "        if not self.last_layer:\n",
    "            x = self.x_model(x, edges, x_diff, m)\n",
    "        h = self.h_model(h, edges, m, node_attr)\n",
    "        return h, x, m\n",
    "\n",
    "class QLieEGNN(nn.Module):\n",
    "    r''' Implementation of LorentzNet.\n",
    "\n",
    "    Args:\n",
    "        - `n_scalar` (int): number of input scalars.\n",
    "        - `n_hidden` (int): dimension of latent space.\n",
    "        - `n_class`  (int): number of output classes.\n",
    "        - `n_layers` (int): number of QLieGEB layers.\n",
    "        - `c_weight` (float): weight c in the x_model.\n",
    "        - `dropout`  (float): dropout rate.\n",
    "    '''\n",
    "    def __init__(self, n_scalar, n_hidden, n_class = 2, n_layers = 6, c_weight = 1e-3, dropout = 0., A=None, include_x=False):\n",
    "        super(QLieEGNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Linear(n_scalar, n_hidden)\n",
    "        self.QLieGEBs = nn.ModuleList([QLieGEB(self.n_hidden, self.n_hidden, self.n_hidden, \n",
    "                                    n_node_attr=n_scalar, dropout=dropout,\n",
    "                                    c_weight=c_weight, last_layer=(i==n_layers-1), A=A, include_x=include_x)\n",
    "                                    for i in range(n_layers)])\n",
    "        self.graph_dec = nn.Sequential(nn.Linear(self.n_hidden, self.n_hidden),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(dropout),\n",
    "                                       nn.Linear(self.n_hidden, n_class)) # classification\n",
    "\n",
    "    def forward(self, scalars, x, edges, node_mask, edge_mask, n_nodes):\n",
    "        h = self.embedding(scalars)\n",
    "        \n",
    "        # print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        for i in range(self.n_layers):\n",
    "            h, x, _ = self.QLieGEBs[i](h, x, edges, node_attr=scalars)\n",
    "        \n",
    "        # print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\n",
    "        \n",
    "        h = h * node_mask\n",
    "        h = h.view(-1, n_nodes, self.n_hidden)\n",
    "        h = torch.mean(h, dim=1)\n",
    "        pred = self.graph_dec(h)\n",
    "        return pred.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum model\n",
    "\n",
    "#### Let's start with a default prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QLieEGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 0.9169079   1.3130671   0.57629734 -0.47118652]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.14373147 -0.01157023 -0.01360894 -0.06450406]\n",
      "h after (just the first particle): \n",
      " [-2.4968634  -1.5802901  -1.0678469  -0.01900774]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... taking any random nonsense transformation in the four-momentum vectors\n",
    "i.e.: multiplying by 0.1. Does the hidden rep stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 2.43436    1.324191  -3.4988296 -1.3810511]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.14373147 -0.01157023 -0.01360894 -0.06450406]\n",
      "h after (just the first particle): \n",
      " [-2.5144496  -0.36996183 -1.928361   -0.69840103]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=0.1 * atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not at all! What about Lorentz transformations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.02530114 -0.01923932 -0.2870935   0.00164617]\n",
      "h after (just the first particle): \n",
      " [ 0.91627324  1.3119451   0.5798764  -0.47567284]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(180000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [ 0.14373147 -0.01157023 -0.01360894 -0.06450406]\n",
      "h after (just the first particle): \n",
      " [-2.4969366  -1.5804261  -1.0679283  -0.01912272]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(180000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivariance holds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's do the predictions again for some other metric tensor J. \n",
    "#### This will illustrate the situation where we found an infinitesimal generator for some experimental data\n",
    "(i.e.: following Robin Walter's approach in LieGAN; the oracle-preserving latents from Roy Forestano et. al, or some other approach that we develop further - would be interesting). Once we have the generators, suppose that we solved for the metric tensor by solving the following eq. (as proposed in Robin's paper):\n",
    "\n",
    "\\begin{equation}\n",
    "L\\cdot J + J\\cdot L^{T} = 0\n",
    "\\end{equation}\n",
    "\n",
    "Here, the Lorentz transformations should not anymore preserve equivariance. To illustrate this, let's consider $J = diag(1,1,1,1)$, that is, we recover the Euclidean norm and dot-product. So, if our model is working, then **boosts** should **break equivariance**, but **rotations** should **preserve** it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J: \n",
      " tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "J = torch.eye(4)\n",
    "print(\"J: \\n\", J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    I will define a rotation matrix about the xy plane. Given that our QLie-EGNN has a new metric,\n",
    "    the Lorentz boosts now should break equivariance, but rotations in this case, should preserve\n",
    "    it.\n",
    "\"\"\"\n",
    "rot = torch.tensor([[np.cos(np.pi), -np.sin(np.pi), 0, 0],\n",
    "                    [np.sin(np.pi), np.cos(np.pi),  0, 0],\n",
    "                    [     0       ,       0      ,  1, 0],\n",
    "                    [     0       ,       0      ,  0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = QLieEGNN(n_scalar = 8, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 6,\\\n",
    "                       c_weight = 1e-3, A=J)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, the default forward pass using the Euclidean metric J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.46950197 -3.965734   -3.0378942  -1.9866586 ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=atom_positions, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the Lorentz boosted jets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.60887957 -3.9931903  -3.0501935  -2.342436  ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(torch.tensor(transformation_matrix(240000000)) @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariance is broken. What about a rotation about the xy plane?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h before (just the first particle): \n",
      " [-0.20670539  0.26581004 -0.09239267 -0.22357208]\n",
      "h after (just the first particle): \n",
      " [-0.46950197 -3.965734   -3.0378942  -1.9866586 ]\n"
     ]
    }
   ],
   "source": [
    "pred = model(scalars=nodes, x=(rot @ atom_positions.to(dtype=torch.float64).T).to(dtype=torch.float32).T, edges=edges, node_mask=atom_mask,\n",
    "                     edge_mask=edge_mask, n_nodes=n_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivariant again.\n",
    "I propose to work on this project, exploring how to improve the symmetry discovery. Also, besides incorporating arbitrary Lie invariances, Infrared Collinear (IRC) safety would be very interesting, and study how our model performs on tagging semi-visible jets for Beyond the Standard Model (BSM) discoveries, like was done in [6] for the Hidden Valley models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train again on some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jogi/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 283\n",
      " train samples: 10000\n",
      " val samples: 1250\n",
      " test samples: 1250\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:41,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 1/60 \t Batch 624/625 \t Loss 0.6946 \t Running Acc 0.512 \t Total Acc 0.512 \t Avg Batch Time 1.6992\n",
      "Time: train: 1061.98 \t Train loss 0.6946 \t Train acc: 0.5119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:10,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6911 \t Running Acc 4.190 \t Total Acc 0.530 \t Avg Batch Time 0.1123\n",
      "New best validation model, saving...\n",
      "Epoch 0/60 finished.\n",
      "Train time: 1061.98 \t Val time 70.18\n",
      "Train loss 0.6946 \t Train acc: 0.5119\n",
      "Val loss: 0.6907 \t Val acc: 0.5296\n",
      "Best val acc: 0.5296 at epoch 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 2/60 \t Batch 624/625 \t Loss 0.6899 \t Running Acc 0.542 \t Total Acc 0.542 \t Avg Batch Time 1.6580\n",
      "Time: train: 1036.24 \t Train loss 0.6899 \t Train acc: 0.5417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6828 \t Running Acc 4.715 \t Total Acc 0.596 \t Avg Batch Time 0.1112\n",
      "New best validation model, saving...\n",
      "Epoch 1/60 finished.\n",
      "Train time: 1036.24 \t Val time 69.49\n",
      "Train loss 0.6899 \t Train acc: 0.5417\n",
      "Val loss: 0.6819 \t Val acc: 0.5960\n",
      "Best val acc: 0.5960 at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 3/60 \t Batch 624/625 \t Loss 0.6763 \t Running Acc 0.574 \t Total Acc 0.574 \t Avg Batch Time 1.6583\n",
      "Time: train: 1036.42 \t Train loss 0.6763 \t Train acc: 0.5743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:16,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6666 \t Running Acc 4.810 \t Total Acc 0.608 \t Avg Batch Time 0.1220\n",
      "New best validation model, saving...\n",
      "Epoch 2/60 finished.\n",
      "Train time: 1036.42 \t Val time 76.23\n",
      "Train loss 0.6763 \t Train acc: 0.5743\n",
      "Val loss: 0.6647 \t Val acc: 0.6080\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:14,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 4/60 \t Batch 624/625 \t Loss 0.6675 \t Running Acc 0.587 \t Total Acc 0.587 \t Avg Batch Time 1.6557\n",
      "Time: train: 1034.79 \t Train loss 0.6675 \t Train acc: 0.5867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6675 \t Running Acc 4.804 \t Total Acc 0.607 \t Avg Batch Time 0.1117\n",
      "Epoch 3/60 finished.\n",
      "Train time: 1034.79 \t Val time 69.84\n",
      "Train loss 0.6675 \t Train acc: 0.5867\n",
      "Val loss: 0.6648 \t Val acc: 0.6072\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 5/60 \t Batch 624/625 \t Loss 0.6634 \t Running Acc 0.598 \t Total Acc 0.598 \t Avg Batch Time 1.6598\n",
      "Time: train: 1037.40 \t Train loss 0.6634 \t Train acc: 0.5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6671 \t Running Acc 4.778 \t Total Acc 0.604 \t Avg Batch Time 0.1117\n",
      "Epoch 4/60 finished.\n",
      "Train time: 1037.40 \t Val time 69.80\n",
      "Train loss 0.6634 \t Train acc: 0.5978\n",
      "Val loss: 0.6648 \t Val acc: 0.6040\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:30,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 6/60 \t Batch 624/625 \t Loss 0.6620 \t Running Acc 0.601 \t Total Acc 0.601 \t Avg Batch Time 1.6801\n",
      "Time: train: 1050.08 \t Train loss 0.6620 \t Train acc: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6659 \t Running Acc 4.791 \t Total Acc 0.606 \t Avg Batch Time 0.1117\n",
      "Epoch 5/60 finished.\n",
      "Train time: 1050.08 \t Val time 69.81\n",
      "Train loss 0.6620 \t Train acc: 0.6011\n",
      "Val loss: 0.6630 \t Val acc: 0.6056\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:28,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 7/60 \t Batch 624/625 \t Loss 0.6620 \t Running Acc 0.600 \t Total Acc 0.600 \t Avg Batch Time 1.6780\n",
      "Time: train: 1048.75 \t Train loss 0.6620 \t Train acc: 0.6002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6627 \t Running Acc 4.810 \t Total Acc 0.608 \t Avg Batch Time 0.1110\n",
      "Epoch 6/60 finished.\n",
      "Train time: 1048.75 \t Val time 69.38\n",
      "Train loss 0.6620 \t Train acc: 0.6002\n",
      "Val loss: 0.6600 \t Val acc: 0.6080\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:14,  1.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 8/60 \t Batch 624/625 \t Loss 0.6615 \t Running Acc 0.595 \t Total Acc 0.595 \t Avg Batch Time 1.6547\n",
      "Time: train: 1034.21 \t Train loss 0.6615 \t Train acc: 0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6628 \t Running Acc 4.766 \t Total Acc 0.602 \t Avg Batch Time 0.1116\n",
      "Epoch 7/60 finished.\n",
      "Train time: 1034.21 \t Val time 69.72\n",
      "Train loss 0.6615 \t Train acc: 0.5949\n",
      "Val loss: 0.6599 \t Val acc: 0.6024\n",
      "Best val acc: 0.6080 at epoch 2.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:37,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 9/60 \t Batch 624/625 \t Loss 0.6613 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.6924\n",
      "Time: train: 1057.77 \t Train loss 0.6613 \t Train acc: 0.6016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6637 \t Running Acc 4.886 \t Total Acc 0.618 \t Avg Batch Time 0.1117\n",
      "New best validation model, saving...\n",
      "Epoch 8/60 finished.\n",
      "Train time: 1057.77 \t Val time 69.82\n",
      "Train loss 0.6613 \t Train acc: 0.6016\n",
      "Val loss: 0.6609 \t Val acc: 0.6176\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:14,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 10/60 \t Batch 624/625 \t Loss 0.6626 \t Running Acc 0.599 \t Total Acc 0.599 \t Avg Batch Time 1.6559\n",
      "Time: train: 1034.91 \t Train loss 0.6626 \t Train acc: 0.5985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [00:45,  1.11it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 11/60 \t Batch 624/625 \t Loss 0.6601 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.6588\n",
      "Time: train: 1036.78 \t Train loss 0.6601 \t Train acc: 0.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6622 \t Running Acc 4.778 \t Total Acc 0.604 \t Avg Batch Time 0.1114\n",
      "Epoch 10/60 finished.\n",
      "Train time: 1036.78 \t Val time 69.63\n",
      "Train loss 0.6601 \t Train acc: 0.6035\n",
      "Val loss: 0.6594 \t Val acc: 0.6040\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:15,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 12/60 \t Batch 624/625 \t Loss 0.6614 \t Running Acc 0.599 \t Total Acc 0.599 \t Avg Batch Time 1.6568\n",
      "Time: train: 1035.50 \t Train loss 0.6614 \t Train acc: 0.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6601 \t Running Acc 4.880 \t Total Acc 0.617 \t Avg Batch Time 0.1120\n",
      "Epoch 11/60 finished.\n",
      "Train time: 1035.50 \t Val time 69.99\n",
      "Train loss 0.6614 \t Train acc: 0.5994\n",
      "Val loss: 0.6573 \t Val acc: 0.6168\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 13/60 \t Batch 624/625 \t Loss 0.6600 \t Running Acc 0.597 \t Total Acc 0.597 \t Avg Batch Time 1.6597\n",
      "Time: train: 1037.31 \t Train loss 0.6600 \t Train acc: 0.5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:12,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6595 \t Running Acc 4.759 \t Total Acc 0.602 \t Avg Batch Time 0.1157\n",
      "Epoch 12/60 finished.\n",
      "Train time: 1037.31 \t Val time 72.33\n",
      "Train loss 0.6600 \t Train acc: 0.5973\n",
      "Val loss: 0.6565 \t Val acc: 0.6016\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 14/60 \t Batch 624/625 \t Loss 0.6593 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.6608\n",
      "Time: train: 1037.99 \t Train loss 0.6593 \t Train acc: 0.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:14,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6572 \t Running Acc 4.816 \t Total Acc 0.609 \t Avg Batch Time 0.1195\n",
      "Epoch 13/60 finished.\n",
      "Train time: 1037.99 \t Val time 74.68\n",
      "Train loss 0.6593 \t Train acc: 0.6035\n",
      "Val loss: 0.6544 \t Val acc: 0.6088\n",
      "Best val acc: 0.6176 at epoch 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:28,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 15/60 \t Batch 624/625 \t Loss 0.6612 \t Running Acc 0.600 \t Total Acc 0.600 \t Avg Batch Time 1.6775\n",
      "Time: train: 1048.44 \t Train loss 0.6612 \t Train acc: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:40,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 16/60 \t Batch 624/625 \t Loss 0.6602 \t Running Acc 0.599 \t Total Acc 0.599 \t Avg Batch Time 1.6962\n",
      "Time: train: 1060.14 \t Train loss 0.6602 \t Train acc: 0.5993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6577 \t Running Acc 4.949 \t Total Acc 0.626 \t Avg Batch Time 0.1118\n",
      "New best validation model, saving...\n",
      "Epoch 15/60 finished.\n",
      "Train time: 1060.14 \t Val time 69.85\n",
      "Train loss 0.6602 \t Train acc: 0.5993\n",
      "Val loss: 0.6548 \t Val acc: 0.6256\n",
      "Best val acc: 0.6256 at epoch 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:46,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 17/60 \t Batch 624/625 \t Loss 0.6592 \t Running Acc 0.600 \t Total Acc 0.600 \t Avg Batch Time 1.7071\n",
      "Time: train: 1066.96 \t Train loss 0.6592 \t Train acc: 0.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6620 \t Running Acc 4.823 \t Total Acc 0.610 \t Avg Batch Time 0.1113\n",
      "Epoch 16/60 finished.\n",
      "Train time: 1066.96 \t Val time 69.58\n",
      "Train loss 0.6592 \t Train acc: 0.6001\n",
      "Val loss: 0.6589 \t Val acc: 0.6096\n",
      "Best val acc: 0.6256 at epoch 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:45,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 18/60 \t Batch 624/625 \t Loss 0.6605 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.7053\n",
      "Time: train: 1065.84 \t Train loss 0.6605 \t Train acc: 0.6015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6574 \t Running Acc 4.918 \t Total Acc 0.622 \t Avg Batch Time 0.1111\n",
      "Epoch 17/60 finished.\n",
      "Train time: 1065.84 \t Val time 69.45\n",
      "Train loss 0.6605 \t Train acc: 0.6015\n",
      "Val loss: 0.6545 \t Val acc: 0.6216\n",
      "Best val acc: 0.6256 at epoch 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:30,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 19/60 \t Batch 624/625 \t Loss 0.6599 \t Running Acc 0.601 \t Total Acc 0.601 \t Avg Batch Time 1.6806\n",
      "Time: train: 1050.35 \t Train loss 0.6599 \t Train acc: 0.6011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6560 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1113\n",
      "New best validation model, saving...\n",
      "Epoch 18/60 finished.\n",
      "Train time: 1050.35 \t Val time 69.59\n",
      "Train loss 0.6599 \t Train acc: 0.6011\n",
      "Val loss: 0.6535 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199it [05:30,  1.65s/it]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "625it [18:03,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 25/60 \t Batch 624/625 \t Loss 0.6581 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.7336\n",
      "Time: train: 1083.50 \t Train loss 0.6581 \t Train acc: 0.6042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:10,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6520 \t Running Acc 4.962 \t Total Acc 0.627 \t Avg Batch Time 0.1130\n",
      "Epoch 24/60 finished.\n",
      "Train time: 1083.50 \t Val time 70.63\n",
      "Train loss 0.6581 \t Train acc: 0.6042\n",
      "Val loss: 0.6489 \t Val acc: 0.6272\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 26/60 \t Batch 624/625 \t Loss 0.6587 \t Running Acc 0.604 \t Total Acc 0.604 \t Avg Batch Time 1.6579\n",
      "Time: train: 1036.16 \t Train loss 0.6587 \t Train acc: 0.6041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6507 \t Running Acc 4.873 \t Total Acc 0.616 \t Avg Batch Time 0.1119\n",
      "Epoch 25/60 finished.\n",
      "Train time: 1036.16 \t Val time 69.94\n",
      "Train loss 0.6587 \t Train acc: 0.6041\n",
      "Val loss: 0.6477 \t Val acc: 0.6160\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:58,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 27/60 \t Batch 624/625 \t Loss 0.6557 \t Running Acc 0.608 \t Total Acc 0.608 \t Avg Batch Time 1.7249\n",
      "Time: train: 1078.07 \t Train loss 0.6557 \t Train acc: 0.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:11,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6506 \t Running Acc 4.861 \t Total Acc 0.614 \t Avg Batch Time 0.1139\n",
      "Epoch 26/60 finished.\n",
      "Train time: 1078.07 \t Val time 71.17\n",
      "Train loss 0.6557 \t Train acc: 0.6078\n",
      "Val loss: 0.6474 \t Val acc: 0.6144\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:35,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 28/60 \t Batch 624/625 \t Loss 0.6554 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.6883\n",
      "Time: train: 1055.16 \t Train loss 0.6554 \t Train acc: 0.6020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6497 \t Running Acc 4.892 \t Total Acc 0.618 \t Avg Batch Time 0.1118\n",
      "Epoch 27/60 finished.\n",
      "Train time: 1055.16 \t Val time 69.85\n",
      "Train loss 0.6554 \t Train acc: 0.6020\n",
      "Val loss: 0.6464 \t Val acc: 0.6184\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 29/60 \t Batch 624/625 \t Loss 0.6553 \t Running Acc 0.606 \t Total Acc 0.606 \t Avg Batch Time 1.6606\n",
      "Time: train: 1037.90 \t Train loss 0.6553 \t Train acc: 0.6060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6495 \t Running Acc 4.873 \t Total Acc 0.616 \t Avg Batch Time 0.1111\n",
      "Epoch 28/60 finished.\n",
      "Train time: 1037.90 \t Val time 69.46\n",
      "Train loss 0.6553 \t Train acc: 0.6060\n",
      "Val loss: 0.6462 \t Val acc: 0.6160\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 30/60 \t Batch 624/625 \t Loss 0.6550 \t Running Acc 0.608 \t Total Acc 0.608 \t Avg Batch Time 1.6584\n",
      "Time: train: 1036.50 \t Train loss 0.6550 \t Train acc: 0.6076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6494 \t Running Acc 4.867 \t Total Acc 0.615 \t Avg Batch Time 0.1118\n",
      "Epoch 29/60 finished.\n",
      "Train time: 1036.50 \t Val time 69.90\n",
      "Train loss 0.6550 \t Train acc: 0.6076\n",
      "Val loss: 0.6463 \t Val acc: 0.6152\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 31/60 \t Batch 624/625 \t Loss 0.6568 \t Running Acc 0.602 \t Total Acc 0.602 \t Avg Batch Time 1.6586\n",
      "Time: train: 1036.65 \t Train loss 0.6568 \t Train acc: 0.6023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6491 \t Running Acc 4.937 \t Total Acc 0.624 \t Avg Batch Time 0.1112\n",
      "Epoch 30/60 finished.\n",
      "Train time: 1036.65 \t Val time 69.53\n",
      "Train loss 0.6568 \t Train acc: 0.6023\n",
      "Val loss: 0.6461 \t Val acc: 0.6240\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [18:54,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 32/60 \t Batch 624/625 \t Loss 0.6560 \t Running Acc 0.607 \t Total Acc 0.607 \t Avg Batch Time 1.8156\n",
      "Time: train: 1134.77 \t Train loss 0.6560 \t Train acc: 0.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [18:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 33/60 \t Batch 624/625 \t Loss 0.6555 \t Running Acc 0.606 \t Total Acc 0.606 \t Avg Batch Time 1.7293\n",
      "Time: train: 1080.84 \t Train loss 0.6555 \t Train acc: 0.6061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6490 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1118\n",
      "Epoch 32/60 finished.\n",
      "Train time: 1080.84 \t Val time 69.88\n",
      "Train loss 0.6555 \t Train acc: 0.6061\n",
      "Val loss: 0.6460 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:17,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 34/60 \t Batch 624/625 \t Loss 0.6564 \t Running Acc 0.603 \t Total Acc 0.603 \t Avg Batch Time 1.6595\n",
      "Time: train: 1037.21 \t Train loss 0.6564 \t Train acc: 0.6032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:09,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6490 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1113\n",
      "Epoch 33/60 finished.\n",
      "Train time: 1037.21 \t Val time 69.59\n",
      "Train loss 0.6564 \t Train acc: 0.6032\n",
      "Val loss: 0.6460 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "625it [17:16,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> train \t Epoch 35/60 \t Batch 624/625 \t Loss 0.6552 \t Running Acc 0.607 \t Total Acc 0.607 \t Avg Batch Time 1.6584\n",
      "Time: train: 1036.49 \t Train loss 0.6552 \t Train acc: 0.6069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [01:13,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> val \t Loss 0.6490 \t Running Acc 4.968 \t Total Acc 0.628 \t Avg Batch Time 0.1174\n",
      "Epoch 34/60 finished.\n",
      "Train time: 1036.49 \t Val time 73.38\n",
      "Train loss 0.6552 \t Train acc: 0.6069\n",
      "Val loss: 0.6460 \t Val acc: 0.6280\n",
      "Best val acc: 0.6280 at epoch 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [02:44,  1.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[252], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m### training and testing\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m test(model, res, model_path, log_path)\n",
      "Cell \u001b[0;32mIn[250], line 93\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, res, N_EPOCHS, model_path, log_path)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, res, N_EPOCHS, model_path, log_path):\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m### training and validation\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m---> 93\u001b[0m         train_res \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime: train: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train loss \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Train acc: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m],train_res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;66;03m# if epoch % args.val_interval == 0:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m             \n\u001b[1;32m     97\u001b[0m         \u001b[38;5;66;03m# if (args.local_rank == 0):\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[250], line 37\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, epoch, loader, partition, N_EPOCHS)\u001b[0m\n\u001b[1;32m     34\u001b[0m edges \u001b[38;5;241m=\u001b[39m [a\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m edges]\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device, dtype)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m---> 37\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscalars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matom_positions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43matom_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                 \u001b[49m\u001b[43medge_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_nodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m predict \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m     41\u001b[0m correct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(predict \u001b[38;5;241m==\u001b[39m label)\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 171\u001b[0m, in \u001b[0;36mQLieEGNN.forward\u001b[0;34m(self, scalars, x, edges, node_mask, edge_mask, n_nodes)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# print(\"h before (just the first particle): \\n\", h[0].cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers):\n\u001b[0;32m--> 171\u001b[0m     h, x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQLieGEBs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# print(\"h after (just the first particle): \\n\", h[0].cpu().detach().numpy())\u001b[39;00m\n\u001b[1;32m    175\u001b[0m h \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m node_mask\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[251], line 135\u001b[0m, in \u001b[0;36mQLieGEB.forward\u001b[0;34m(self, h, x, edges, node_attr)\u001b[0m\n\u001b[1;32m    133\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_model_extended(h[i], h[j], norms, dots, x[i], x[j])\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdots\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [B*N, hidden]\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_layer:\n\u001b[1;32m    137\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_model(x, edges, x_diff, m)\n",
      "Cell \u001b[0;32mIn[251], line 89\u001b[0m, in \u001b[0;36mQLieGEB.m_model\u001b[0;34m(self, hi, hj, norms, dots)\u001b[0m\n\u001b[1;32m     87\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([hi, hj, norms, dots], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# print(\"Before embedding to |psi> : \", out)\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphi_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     90\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi_m(out)\n\u001b[1;32m     91\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m*\u001b[39m w\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[172], line 92\u001b[0m, in \u001b[0;36mDressedQuantumNet.forward\u001b[0;34m(self, input_features)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# for batch in q_in:\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m q_in:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# print(quantum_net(elem, self.q_params, self.q_depth, self.n_qubits))\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m     q_out_elem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhstack(\u001b[43mquantum_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_qubits\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     93\u001b[0m     q_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((q_out, q_out_elem))\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# return the batch measurement of the PQC\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/qnode.py:1098\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_gradient_fn(shots\u001b[38;5;241m=\u001b[39moverride_shots, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape)\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1098\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_component\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m old_interface \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/qnode.py:1052\u001b[0m, in \u001b[0;36mQNode._execution_component\u001b[0;34m(self, args, kwargs, override_shots)\u001b[0m\n\u001b[1;32m   1049\u001b[0m full_transform_program\u001b[38;5;241m.\u001b[39mprune_dynamic_transform()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;66;03m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[0;32m-> 1052\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverride_shots\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_shots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/execution.py:616\u001b[0m, in \u001b[0;36mexecute\u001b[0;34m(tapes, device, gradient_fn, interface, transform_program, config, grad_on_execution, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform, device_vjp)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Exiting early if we do not need to deal with an interface boundary\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_interface_boundary_required:\n\u001b[0;32m--> 616\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43minner_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m post_processing(results)\n\u001b[1;32m    619\u001b[0m _grad_on_execution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/workflow/execution.py:297\u001b[0m, in \u001b[0;36m_make_inner_execute.<locals>.inner_execute\u001b[0;34m(tapes, **_)\u001b[0m\n\u001b[1;32m    294\u001b[0m transformed_tapes, transform_post_processing \u001b[38;5;241m=\u001b[39m transform_program(tapes)\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_tapes:\n\u001b[0;32m--> 297\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mdevice_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransformed_tapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     results \u001b[38;5;241m=\u001b[39m ()\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/modifiers/simulator_tracking.py:30\u001b[0m, in \u001b[0;36m_track_execute.<locals>.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(untracked_execute)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m, circuits, execution_config\u001b[38;5;241m=\u001b[39mDefaultExecutionConfig):\n\u001b[0;32m---> 30\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43muntracked_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(circuits, QuantumScript):\n\u001b[1;32m     32\u001b[0m         batch \u001b[38;5;241m=\u001b[39m (circuits,)\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/modifiers/single_tape_support.py:32\u001b[0m, in \u001b[0;36m_make_execute.<locals>.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m     30\u001b[0m     is_single_circuit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     circuits \u001b[38;5;241m=\u001b[39m (circuits,)\n\u001b[0;32m---> 32\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecution_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m is_single_circuit \u001b[38;5;28;01melse\u001b[39;00m results\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:593\u001b[0m, in \u001b[0;36mDefaultQubit.execute\u001b[0;34m(self, circuits, execution_config)\u001b[0m\n\u001b[1;32m    590\u001b[0m prng_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prng_keys()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(circuits))]\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebugger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprng_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcircuits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[1;32m    608\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:594\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    590\u001b[0m prng_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prng_keys()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(circuits))]\n\u001b[1;32m    592\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_workers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[0;32m--> 594\u001b[0m         \u001b[43m_simulate_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m            \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdebugger\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minterface\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_state_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprng_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c, _key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(circuits, prng_keys)\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    607\u001b[0m vanilla_circuits \u001b[38;5;241m=\u001b[39m [convert_to_numpy_parameters(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m circuits]\n\u001b[1;32m    608\u001b[0m seeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rng\u001b[38;5;241m.\u001b[39mintegers(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m31\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(vanilla_circuits))\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/default_qubit.py:841\u001b[0m, in \u001b[0;36m_simulate_wrapper\u001b[0;34m(circuit, kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_simulate_wrapper\u001b[39m(circuit, kwargs):\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/qubit/simulate.py:287\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(circuit, debugger, state_cache, **execution_kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m simulate_one_shot_native_mcm(\n\u001b[1;32m    283\u001b[0m         circuit, debugger\u001b[38;5;241m=\u001b[39mdebugger, rng\u001b[38;5;241m=\u001b[39mrng, prng_key\u001b[38;5;241m=\u001b[39mprng_key, interface\u001b[38;5;241m=\u001b[39minterface\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    286\u001b[0m ops_key, meas_key \u001b[38;5;241m=\u001b[39m jax_random_split(prng_key)\n\u001b[0;32m--> 287\u001b[0m state, is_state_batched \u001b[38;5;241m=\u001b[39m \u001b[43mget_final_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcircuit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mops_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterface\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     state_cache[circuit\u001b[38;5;241m.\u001b[39mhash] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/qubit/simulate.py:150\u001b[0m, in \u001b[0;36mget_final_state\u001b[0;34m(circuit, debugger, **execution_kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, MidMeasureMP):\n\u001b[1;32m    149\u001b[0m     prng_key, key \u001b[38;5;241m=\u001b[39m jax_random_split(prng_key)\n\u001b[0;32m--> 150\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mapply_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_state_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_state_batched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebugger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebugger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmid_measurements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmid_measurements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprng_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Handle postselection on mid-circuit measurements\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(op, qml\u001b[38;5;241m.\u001b[39mProjector):\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/pennylane/devices/qubit/apply_operation.py:356\u001b[0m, in \u001b[0;36mapply_cnot\u001b[0;34m(op, state, is_state_batched, debugger, **_)\u001b[0m\n\u001b[1;32m    353\u001b[0m sl_0 \u001b[38;5;241m=\u001b[39m _get_slice(\u001b[38;5;241m0\u001b[39m, control_axes, n_dim)\n\u001b[1;32m    354\u001b[0m sl_1 \u001b[38;5;241m=\u001b[39m _get_slice(\u001b[38;5;241m1\u001b[39m, control_axes, n_dim)\n\u001b[0;32m--> 356\u001b[0m state_x \u001b[38;5;241m=\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43msl_1\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_axes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m math\u001b[38;5;241m.\u001b[39mstack([state[sl_0], state_x], axis\u001b[38;5;241m=\u001b[39mcontrol_axes)\n",
      "File \u001b[0;32m~/anaconda3/envs/quantum/lib/python3.10/site-packages/autoray/autoray.py:30\u001b[0m, in \u001b[0;36mdo\u001b[0;34m(fn, like, *args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict, defaultdict\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m signature\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo\u001b[39m(fn, \u001b[38;5;241m*\u001b[39margs, like\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do function named ``fn`` on ``(*args, **kwargs)``, peforming single\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    dispatch to retrieve ``fn`` based on whichever library defines the class of\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    the ``args[0]``, or the ``like`` keyword argument if specified.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        <tf.Tensor: id=91, shape=(3, 3), dtype=float32>\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     backend \u001b[38;5;241m=\u001b[39m _choose_backend(fn, args, kwargs, like\u001b[38;5;241m=\u001b[39mlike)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    N_EPOCHS = 60\n",
    "\n",
    "    model_path = \"models/LieEQGNN/\"\n",
    "    log_path = \"logs/LieEQGNN/\"\n",
    "    # utils_lorentz.args_init(args)\n",
    "\n",
    "    ### set random seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    ### initialize cuda\n",
    "    # dist.init_process_group(backend='nccl')\n",
    "    device = 'cpu' #torch.device(\"cuda\")\n",
    "    dtype = torch.float32\n",
    "\n",
    "    ### load data\n",
    "    # dataloaders = retrieve_dataloaders( batch_size,\n",
    "    #                                     num_data=100000, # use all data\n",
    "    #                                     cache_dir=\"datasets/QMLHEP/quark_gluons/\",\n",
    "    #                                     num_workers=0,\n",
    "    #                                     use_one_hot=True)\n",
    "\n",
    "    ### create parallel model\n",
    "    # model = LorentzNet(n_scalar = 8, n_hidden = 72, n_class = 2,\\\n",
    "    #                    dropout = 0.2, n_layers = 1,\\\n",
    "    #                    c_weight = 1e-3)\n",
    "\n",
    "    model = QLieEGNN(n_scalar = 1, n_hidden = 4, n_class = 2,\\\n",
    "                       dropout = 0.2, n_layers = 1,\\\n",
    "                       c_weight = 1e-3)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    ### print model and dataset information\n",
    "    # if (args.local_rank == 0):\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(\"Model Size:\", pytorch_total_params)\n",
    "    for (split, dataloader) in dataloaders.items():\n",
    "        print(f\" {split} samples: {len(dataloader.dataset)}\")\n",
    "\n",
    "    ### optimizer\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-2)\n",
    "\n",
    "    ### lr scheduler\n",
    "    base_scheduler = CosineAnnealingWarmRestarts(optimizer, 4, 2, verbose = False)\n",
    "    lr_scheduler = utils_lorentz.GradualWarmupScheduler(optimizer, multiplier=1,\\\n",
    "                                                warmup_epoch=5,\\\n",
    "                                                after_scheduler=base_scheduler) ## warmup\n",
    "\n",
    "    ### loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    ### initialize logs\n",
    "    res = {'epochs': [], 'lr' : [],\\\n",
    "           'train_time': [], 'val_time': [],  'train_loss': [], 'val_loss': [],\\\n",
    "           'train_acc': [], 'val_acc': [], 'best_val': 0, 'best_epoch': 0}\n",
    "\n",
    "    ### training and testing\n",
    "    print(\"Training...\")\n",
    "    train(model, res, N_EPOCHS, model_path, log_path)\n",
    "    test(model, res, model_path, log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train-result-epoch9.json'"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "max(os.listdir(\"logs/LorentzNet/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/IAAAIjCAYAAACgdyAGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAACkI0lEQVR4nOzdd3hU1dbH8d+kEyDUNEIktEgvAmJEBJGOiIqCFGmKXgiKRBRRaRYsXBEVFEERRVEUpVxBumABBUEEpElHILQAoaZN3j/2OwkxCaTMZCaT7+d55pmZM2fOWSfsAOvsvde2pKampgoAAAAAABQKHs4OAAAAAAAA5ByJPAAAAAAAhQiJPAAAAAAAhQiJPAAAAAAAhQiJPAAAAAAAhQiJPAAAAAAAhQiJPAAAAAAAhQiJPAAAAAAAhQiJPAAAAAAAhQiJPAAABeDAgQOyWCyaOXOms0MpclavXi2LxaK5c+ded99+/fopIiLC8UEBAJAPJPIAANjBzJkzZbFY9Pvvv9v92LZENLvHl19+mWH/pKQkvfPOO2rSpIlKliypEiVKqEmTJnr33XeVnJyc5TmsVqs+/fRTtWnTRuXLl5e3t7eCgoLUtm1bTZs2TQkJCRn2t537zTffzHSsrH4WY8eOlcViUXBwsC5dupTpOxEREbrrrrvy8uMBAKDI8XJ2AAAAFAWVKlXS5cuX5e3tnedjPPHEE2rSpEmm7VFRUWmvL168qE6dOmnNmjW666671K9fP3l4eGjJkiV64oknNH/+fP3vf/+Tv79/2ncuX76se++9V0uXLtWtt96q4cOHKzg4WHFxcVqzZo0GDx6s3377TR999FGmc0+YMEGDBg3KcLxrOXHihN5//3099dRTefgJON706dNltVqdHQYAANdEIg8AQAGwWCzy8/PL1zGaN2+u+++//5r7xMTEaM2aNXr33Xc1ZMiQtO2DBg3SlClTNGTIED399NOaMmVK2mfDhg3T0qVLNWnSJA0dOjTD8Z566in9/fffWr58eaZzNWjQQJs3b9bUqVMVExOTo2to0KCBJkyYoMGDB6tYsWI5+k5Bys+NFgAACgpD6wEAKADZzZHfuXOn7r//fpUtW1Z+fn5q3LixFi5cmKdz/PPPP/roo4/UqlWrDEm8TXR0tO644w5NmzZNR44ckSQdPnxYH374odq3b58pibepXr26Bg8enGl7s2bN1KpVK73xxhu6fPlyjmIcPXq0jh8/rvfffz8XV5a1li1bqk6dOtq4caNuvfVWFStWTJUrV9bUqVOz3N9qteqVV15RxYoV5efnpzvvvFN79uzJsE9u58jv2LFDxYoVU58+fTJs//nnn+Xp6akRI0bk+roAALgeEnkAAJzkr7/+0i233KIdO3bo2Wef1ZtvvqnixYvrnnvu0bx58zLtf/78eZ06dSrTIzU1VZL0/fffKyUlJVNSebU+ffooOTlZS5YsyfCd3r175+kaxo4dm6vEvHnz5rlO/q/lzJkz6tixoxo1aqQ33nhDFStW1KBBgzRjxoxM+7722muaN2+ehg8frpEjR+rXX39Vr1698nX+mjVr6qWXXtKsWbPSbsBcvHhR/fr1U40aNfTiiy/m6/gAAGSFofUAADjJ0KFDdcMNN2jDhg3y9fWVJA0ePFi33XabRowYoXvvvTfD/gMGDMjyOMeOHVNISIi2b98uSapfv36257R9Ztt3586dkqQ6depk2C8xMVHx8fFp7y0Wi8qVK5fpeM2bN9cdd9yRNlc+J8Plx4wZoxYtWmjq1KkaNmzYdfe/lqNHj+rNN99MG9r/2GOPqWnTpho5cqQeeuihDEPlr1y5os2bN8vHx0eSVKZMGQ0dOlTbtm3LdP25ERMTowULFujRRx9Vs2bNNGbMGB08eFDr1q1L+3MFAMCe6JEHAMAJ4uLitGrVKnXr1i1DT/vp06fVrl07/f3332nD321Gjx6t5cuXZ3qULVtWkumxl6SSJUtme17bZ7Z9bcl6iRIlMuy3ePFiBQYGpj0qVaqU7THHjh2r2NjYbIe0/9vtt9+uO+64wy698l5eXnrsscfS3vv4+Oixxx7TiRMntHHjxgz79u/fPy2Jl8xNCEnat29fvmLw8PDQzJkzdeHCBXXo0EHvvfeeRo4cqcaNG+fruAAAZIdEHgAAJ9izZ49SU1M1atSoDAlzYGCgxowZI8lUeL9a3bp11bp160wPW3L67yQ9K7bPgoKCMnznwoULGfZr1qxZ2o2Ctm3bXvNa8pKY5zb5z06FChVUvHjxDNsiIyMlmboEV7vhhhsyvC9TpowkMzw/v6pWraqxY8dqw4YNql27tkaNGpXvYwIAkB2G1gMA4AS2Jc6GDx+udu3aZblPtWrVcnXMWrVqSZK2bNmiBg0aZLnPli1bJElVqlSRJNWoUUOStG3btgxD8gMDA9W6dWtJ0meffXbdc48ZM0YtW7bUBx98oNKlS193/9tvv10tW7bUG2+8of/85z/X3d8ePD09s9xuqzGQX8uWLZNkhvufPn1aISEhdjkuAAD/Ro88AABOYEukvb29s+xlb9269TWHyGelQ4cO8vT01KxZs7Ld59NPP5WPj4+6dOmS4Tuff/553i9GUosWLdSyZUu9/vrrue6V/+CDD/J83qNHj+rixYsZtu3evVuSclV9Pr+mTp2q5cuX65VXXlFiYmKG4f4AANgbiTwAAE4QFBSU1oN97NixTJ+fPHky18esWLGiHn74Ya1YsSLLKvJTp07VqlWr9Nhjj6UVrrvhhhs0YMAAff/995o8eXKWx81pj7UtMZ82bVqO9r86+b9y5UqOvvNvycnJGW4EJCYm6oMPPlBgYKAaNWqUp2Pm1v79+/X000+ra9eueu655/Tf//5XCxcu1Kefflog5wcAFD0MrQcAwI5mzJiRtrTb1Ww94FebMmWKbrvtNtWtW1cDBw5UlSpVdPz4ca1bt07//POP/vzzzwz7//TTT1kmvPXq1VO9evUkSRMnTtTOnTs1ePBgLVmyRO3bt5ckLV26VAsWLFCrVq00YcKEDN+fNGmS9u/fr8cff1xffvmlOnfurKCgIJ06dUq//PKL/ve//+nGG2+87rW3aNFCLVq00Jo1a667r82YMWN0xx135Hj/f6tQoYJef/11HThwQJGRkZozZ442b96sadOmZahY7yipqakaMGCAihUrlnbz5LHHHtM333yjoUOHqnXr1qpQoYLD4wAAFC0k8gAA2FF266m3bNky07ZatWrp999/17hx4zRz5kydPn1aQUFBatiwoUaPHp1p/3feeSfLY48ZMyYtkS9evLhWrFih9957T7NmzdLw4cN16dIlSVLfvn01Y8YMeXhkHJDn7++vJUuWaNasWZo1a5beeOMNxcfHq3Tp0qpfv77ee+899e3bN0fXP3bs2Fwl5i1btsx18n+1MmXK6JNPPtHjjz+u6dOnKzg4WJMnT9bAgQPzdLzcevfdd7V69Wp98803CgwMTNv+0UcfqU6dOho4cKAWLVpUILEAAIoOS6q9KrwAAACXFB8frxYtWmjv3r368ccfsy2EV9i0bNlSp06d0rZt25wdCgAABYo58gAAuLmAgAB9//33Kl++vDp27KiDBw86OyQAAJAPDK0HAKAICAkJ0b59+5wdRo7ExcUpMTEx2889PT0zDGN39zgAAPg3EnkAAOBS7rvvvmvOma9UqZIOHDhQZOIAAODfmCMPAABcysaNG3XmzJlsPy9WrJiaNWtWZOIAAODfSOQBAAAAAChEKHYHAAAAAEAhwhz5LFitVh09elQlS5aUxWJxdjgAAAAAADeXmpqq8+fPq0KFCvLwuHafO4l8Fo4eParw8HBnhwEAAAAAKGIOHz6sihUrXnMfEvkslCxZUpL5AQYEBDg5muwlJSVp2bJlatu2rby9vZ0dDiCJdgnXRLuEK6JdwhXRLuFqilKbjI+PV3h4eFo+ei0k8lmwDacPCAhw+UTe399fAQEBbt+oUXjQLuGKaJdwRbRLuCLaJVxNUWyTOZneTbE7AAAAAAAKERJ5AAAAAAAKERJ5AAAAAAAKEebIAwAAAMBVUlJSlJSU5OwwIDNH3svLS1euXFFKSoqzw8kXT09PeXl52WWJcxJ5AAAAAPh/Fy5c0D///KPU1FRnhwKZtdVDQkJ0+PBhuyTAzubv76/Q0FD5+Pjk6zgk8gAAAAAg0xP/zz//yN/fX4GBgW6ROBZ2VqtVFy5cUIkSJeThUXhnhqempioxMVEnT57U/v37Vb169XxdD4k8AAAAAMgM405NTVVgYKCKFSvm7HAgk8gnJibKz8+vUCfyklSsWDF5e3vr4MGDadeUV4X7JwEAAAAAdkZPPBzFXjcjSOQBAAAAAChESOQBAAAAAChESOQBAAAAABlERERo0qRJzg4D2SCRBwAAAIBCymKxXPMxduzYPB13w4YNevTRR/MVW8uWLfXkk0/m6xjIGlXrAQAAAKCQOnbsWNrrOXPmaPTo0dq1a1fathIlSqS9Tk1NVUpKiry8rp8GBgYG2jdQ2BU98gAAAABwDRcvZv+4ciXn+16+nLN9cyMkJCTtUapUKVkslrT3O3fuVMmSJfX999+rUaNG8vX11c8//6y9e/eqS5cuCg4OVokSJdSkSROtWLEiw3H/PbTeYrHoww8/1L333it/f39Vr15dCxcuzF2w//LNN9+odu3a8vX1VUREhN58880Mn7/33nu68cYbFRISotDQUN1///1pn82dO1d169ZVsWLFVK5cObVu3VoXc/vDK8RI5AEAAADgGkqUyP7RtWvGfYOCst+3Q4eM+0ZEZL2fvT377LN67bXXtGPHDtWrV08XLlxQx44dtXLlSv3xxx9q3769OnfurEOHDl3zOOPGjVO3bt20ZcsWdezYUb169VJcXFyeYtq4caO6deumBx98UFu3btXYsWM1atQozZw5U5L0+++/64knntDYsWO1fv16LV68WLfffrskMwqhR48eGjBggHbs2KHVq1frvvvuU2pqap5iKYwYWg8AAAAAbuzFF19UmzZt0t6XLVtW9evXT3v/0ksvad68eVq4cKGGDBmS7XH69eunHj16SJLGjx+vd955R+vXr1f79u1zHdPEiRN15513atSoUZKkyMhIbd++XRMmTFC/fv106NAhFS9eXHfddZdSU1MVEBCgRo0aSTKJfHJysu677z5VqlRJklS3bt1cx1CYkcgDAAAAwDVcuJD9Z56eGd+fOJH9vh7/Gg994ECeQ8qVxo0bZ3h/4cIFjR07VosWLUpLii9fvnzdHvl69eqlvS5evLgCAgJ04loXfA07duxQly5dMmxr1qyZJk2apJSUFLVp00aVKlVStWrV1KpVK911113q2rWr/P39Vb9+fd15552qW7eu2rVrp7Zt2+r+++9XmTJl8hRLYcTQ+kJs/35p48Ygbdvm7EgAAAAA91W8ePYPP7+c71usWM72tX/8GQ86fPhwzZs3T+PHj9dPP/2kzZs3q27dukpMTLzmcby9vTO8t1gsslqtdo9XkkqWLKlNmzbp888/V3BwsMaOHav69evr7Nmz8vT01PLly/X999+rVq1aevfdd3XjjTdq//79DonFFZHIF2Lvv++hl16K0mef8ccIAAAAIGd++eUX9evXT/fee6/q1q2rkJAQHSio4QH/r2bNmvrll18yxRUZGSnP/x/m4OXlpdatW+vFF1/U5s2bdeDAAa1atUqSuYnQrFkzjRs3Tn/88Yd8fHw0b968Ar0GZ2JofSEWHGyejx2zODcQAAAAAIVG9erV9e2336pz586yWCwaNWqUw3rWT548qc2bN2fYFhoaqqeeekpNmjTRSy+9pO7du2vdunWaPHmy3nvvPUnSd999p3379um2226Tl5eXfvrpJ1mtVt1444367bfftHLlSrVt21ZBQUH67bffdPLkSdWsWdMh1+CKSOQLsZAQU5Xx+HEnBwIAAACg0Jg4caIGDBigW2+9VeXLl9eIESMUHx/vkHPNnj1bs2fPzrDtpZde0gsvvKCvvvpKo0eP1ksvvaTQ0FC9+OKL6tevnySpdOnS+vbbbzV27FhduXJF1atX1xdffKHatWtrx44d+vHHHzVp0iTFx8erUqVKevPNN9Xh38sCuDES+UIsJMQ80yMPAAAAoF+/fmmJsCS1bNkyyyXZIiIi0oao20RHR2d4/++h9lkd5+zZs9eMZ/Xq1df8vGvXrur67/X7/t9tt92m1atXy2q1Kj4+XgEBAfL4/2qBNWvW1JIlS655bHfH5OpCjB55AAAAACh6SOQLMVuPfFycRQkJzo0FAAAAAFAwnJ7IT5kyRREREfLz81PTpk21fv36a+5/9uxZRUdHKzQ0VL6+voqMjNTixYvTPj9//ryefPJJVapUScWKFdOtt96qDRs2OPoynKJsWcnLyxSloFceAAAAAIoGp86RnzNnjmJiYjR16lQ1bdpUkyZNUrt27bRr1y4FBQVl2j8xMVFt2rRRUFCQ5s6dq7CwMB08eFClS5dO2+eRRx7Rtm3bNGvWLFWoUEGfffaZWrdure3btyssLKwAr87xLBZpwICtuuWW2ipVinIHAAAAAFAUOLVHfuLEiRo4cKD69++vWrVqaerUqfL399eMGTOy3H/GjBmKi4vT/Pnz1axZM0VERKhFixaqX7++JOny5cv65ptv9MYbb+j2229XtWrVNHbsWFWrVk3vv/9+QV5agenY8YB6905VqVLOjgQAAAAAUBCc1o2bmJiojRs3auTIkWnbPDw81Lp1a61bty7L7yxcuFBRUVGKjo7WggULFBgYqJ49e2rEiBHy9PRUcnKyUlJS5Ofnl+F7xYoV088//5xtLAkJCUq4apK5bemFpKQkJSUl5ecyHcoWmyvHiKKHdglXRLuEK6JdwhUV9XaZlJSk1NRUWa1Wh62rjtyxVcu3/bkUdlarVampqUpKSpKnp2eGz3Lze+e0RP7UqVNKSUlRcHBwhu3BwcHauXNnlt/Zt2+fVq1apV69emnx4sXas2ePBg8erKSkJI0ZM0YlS5ZUVFSUXnrpJdWsWVPBwcH64osvtG7dOlWrVi3bWF599VWNGzcu0/Zly5bJ398/fxfqYCdOFNOrr25RmTJXVLXqOWeHA6RZvny5s0MAMqFdwhXRLuGKimq79PLyUkhIiC5cuKDExERnh4OrnD9/3tkh2EViYqIuX76sH3/8UcnJyRk+u3TpUo6PU6gmVlutVgUFBWnatGny9PRUo0aNdOTIEU2YMEFjxoyRJM2aNUsDBgxQWFiYPD09ddNNN6lHjx7auHFjtscdOXKkYmJi0t7Hx8crPDxcbdu2VUBAgMOvK6+SkpI0YMAhzZlTQwMHpujxxwv/HSoUfklJSVq+fLnatGkjb29vZ4cDSKJdwjXRLuGKinq7vHLlig4fPqwSJUpkGuUL50hNTdX58+dVsmRJWSwWZ4eTb1euXFGxYsV0++23Z2pjtpHhOeG0RL58+fLy9PTU8X+VWz9+/LhCbOuq/UtoaKi8vb0zDEGoWbOmYmNjlZiYKB8fH1WtWlVr1qzRxYsXFR8fr9DQUHXv3l1VqlTJNhZfX1/5+vpm2u7t7e3yf4GVKWOmBBw/7ilvb8/r7A0UnMLw+4Oih3YJV0S7hCsqqu0yJSVFFotFHh4e8vBw+gJfkNKG09v+XAo7Dw8PWSyWLH/HcvM757SfhI+Pjxo1aqSVK1embbNarVq5cqWioqKy/E6zZs20Z8+eDHMjdu/erdDQUPn4+GTYt3jx4goNDdWZM2e0dOlSdenSxTEX4mRlylyRJMXGOjkQAAAAAIVWy5Yt9eSTT6a9j4iI0KRJk675HYvFovnz5+f73PY6TlHi1FsaMTExmj59uj755BPt2LFDgwYN0sWLF9W/f39JUp8+fTIUwxs0aJDi4uI0dOhQ7d69W4sWLdL48eMVHR2dts/SpUu1ZMkS7d+/X8uXL9cdd9yhGjVqpB3T3dh65EnkAQAAgKKnc+fOat++fZaf/fTTT7JYLNqyZUuuj7thwwY9+uij+Q0vg7Fjx6pBgwaZth87dkwdOnSw67n+bebMmRmWLS/snDpHvnv37jp58qRGjx6t2NhYNWjQQEuWLEkrgHfo0KEMwyfCw8O1dOlSDRs2TPXq1VNYWJiGDh2qESNGpO1z7tw5jRw5Uv/884/Kli2rrl276pVXXnHboUFX98inppq15QEAAAAUDQ8//LC6du2qf/75RxUrVszw2ccff6zGjRurXr16uT5uYGCgvUK8ruymViN7Tp9kMGTIEB08eFAJCQn67bff1LRp07TPVq9erZkzZ2bYPyoqSr/++quuXLmivXv36rnnnsswZ75bt27au3evEhISdOzYMU2ePFml3HiR9dKlTY98YqJ05oyTgwEAAADcSGqqdPGicx7/v+radd11110KDAzMlDdduHBBX3/9tR5++GGdPn1aPXr0UFhYmPz9/VW3bl198cUX1zzuv4fW//3332kF2mrVqpXlygYjRoxQZGSk/P39VaVKFY0aNSptSbWZM2dq3Lhx+vPPP2WxWGSxWNJi/vfQ+q1bt6pVq1YqVqyYAgMD9eSTT+rChQtpn/fr10/33HOP/vvf/yo0NFTlypVTdHR0vpZNPHTokLp06aISJUooICBA3bp1y1DP7c8//9Qdd9yhkiVLKiAgQI0aNdLvv/8uSTp48KA6d+6sMmXKqHjx4qpdu7YWL16c51hyolBVrUdm3t5WlSmTqjNnLIqNlcqWdXZEAAAAgHu4dEkqUcI5575wQSpe/Pr7eXl5qU+fPpo5c6aef/75tMruX3/9tVJSUtSjRw9duHBBjRo10ogRIxQQEKBFixbpoYceUtWqVXXzzTdf9xxWq1X33XefgoOD9dtvv+ncuXMZ5tPblCxZUjNnzlSFChW0detWDRw4UCVLltQzzzyj7t27a9u2bVqyZIlWrFghSVl2uF68eFHt2rVTVFSUNmzYoNjYWD3yyCN6/PHH9cknn6Tt98MPPyg0NFQ//PCD9uzZo+7du6tBgwYaOHDg9X9oWVyfLYlfs2aNkpOTFR0dre7du2v16tWSpF69eqlhw4Z6//335enpqc2bN6eN+o6OjlZiYqJ+/PFHFS9eXNu3b1cJBzccEnk38NprKfL39xIjUgAAAICiZ8CAAZowYYLWrFmjli1bSjLD6rt27apSpUqpVKlSGj58eNr+jz/+uJYuXaqvvvoqR4n8ihUrtHPnTi1dulQVKlSQJI0fPz7TvPYXXngh7XVERISGDx+uL7/8Us8884yKFSumEiVKyMvL65pD6WfPnq0rV67o008/VfHixVWrVi298cYb6tGjh9544420adhlypTR5MmT5enpqRo1aqhTp05auXJlnhL5lStXauvWrdq/f7/Cw8MlSZ9++qlq166tDRs2qEmTJjp06JCefvpp1ahRQ5JUvXr1tO8fOnRIXbt2Vd26dSXpmium2QuJvBvo3z9VbloCAAAAAHAaf3/TM+6sc+dUjRo1dOutt2rGjBlq2bKl9uzZo59++kkvvviiJLOs3vjx4/XVV1/pyJEjSkxMVEJCgvxzeJIdO3YoPDw8LYmXlOVKY3PmzNE777yjvXv36sKFC0pOTlZAQEDOL+T/z1W/fn0Vv2o4QtOmTWW1WrVr1660RL527doZpliHhoZq69atuTrX1ecMDw9PS+IlqVatWipdurR27NihJk2aKCYmRo888ohmzZql1q1b64EHHlDVqlUlSU888YQGDRqkZcuWqXXr1uratWue6hLkhtPnyAMAAACAK7JYzPB2ZzxyW8T64Ycf1jfffKPz58/r448/VtWqVdWiRQtJ0oQJE/T2229rxIgR+uGHH7R582a1a9dOiYmJdvtZrVu3Tr169VLHjh313Xff6Y8//tDzzz9v13Nc7d/FzC0WS4Zlyu1t7Nix+uuvv9SpUyetWrVKtWrV0rx58yRJjzzyiPbt26eHHnpIW7duVePGjfXuu+86LBaJRN4tHD4sffed9Ntvzo4EAAAAgDN069ZNHh4emj17tj799FMNGDAgbb78L7/8oi5duqh3796qX7++qlSpot27d+f42DVr1tThw4d17NixtG2//vprhn3Wrl2rSpUq6fnnn1fjxo1VvXp1HTx4MMM+Pj4+SklJue65/vzzT128eDFt22+//SYPDw/deOONOY45N2zXd/jw4bRt27dv19mzZ1WrVq20bZGRkRo2bJiWLVum++67Tx9//HHaZ+Hh4frPf/6jb7/9Vk899ZSmT5/ukFhtSOTdwNy5HurcWXrnHWdHAgAAAMAZSpQooe7du2vkyJE6duyY+vXrl/ZZ9erVtXz5cq1du1Y7duzQY489lqEi+/W0bt1akZGR6tu3r/7880/99NNPev755zPsU716dR06dEhffvml9u7dq3feeSetx9omIiJC+/fv1+bNm3Xq1CklJCRkOlevXr3k5+envn37atu2bfrhhx80YsQI9e7dO21YfV6lpKRo8+bNGR47duxQ69atVbduXfXq1UubNm3S+vXr1adPH7Vo0UKNGzfW5cuXNWTIEK1evVoHDx7UL7/8og0bNqhmzZqSpCeffFJLly7V/v37tWnTJv3www9pnzkKibwbCAkxa1PExjo5EAAAAABO8/DDD+vMmTNq165dhvnsL7zwgm666Sa1a9dOLVu2VEhIiO65554cH9fDw0Pz5s3T5cuXdfPNN+uRRx7RK6+8kmGfu+++W8OGDdOQIUPUoEEDrV27VqNGjcqwT9euXdW+fXvdcccdCgwMzHIJPH9/fy1dulRxcXFq0qSJunXrphYtWthlqPqFCxfUsGHDDI/OnTvLYrFowYIFKlOmjG6//Xa1bt1aVapU0Zw5cyRJnp6eOn36tPr06aPIyEh169ZNHTp00Lhx4ySZGwTR0dGqWbOm2rdvr8jISL333nv5jvdaLKmpOV2hsOiIj49XqVKldO7cuVwXZyhISUlJWrx4sYoV66R27bxUs6a0fbuzo0JRZ2uXHTt2zDR3CXAW2iVcEe0Srqiot8srV65o//79qly5svz8/JwdDmSWhouPj1dAQIA8PAp/P/S12lhu8tDC/5OAgoPpkQcAAACAooJE3g2EhprnM2ekK1ecGwsAAAAAwLFI5N1A6dKSr695nYuaFQAAAACAQohE3g1YLFJIiHl91YoQAAAAAAA35OXsAGAftqKRVao4Nw4AAACgsKMeOBzFXm2LRN5N9Orl7AgAAACAws3T01OSlJiYqGLFijk5GrijS5cuSVK+V4UgkQcAAAAASV5eXvL399fJkyfl7e3tFsudFXZWq1WJiYm6cuVKof7zSE1N1aVLl3TixAmVLl067aZRXpHIu4kjR6RNm0zhu+bNnR0NAAAAUPhYLBaFhoZq//79OnjwoLPDgUwCfPnyZRUrVkwWi8XZ4eRb6dKlFWIrcJYPJPJu4rvvpP/8R+rcmUQeAAAAyCsfHx9Vr15diYmJzg4FkpKSkvTjjz/q9ttvz/dwdGfz9vbOd0+8DYm8m7CtJR8b69w4AAAAgMLOw8NDfn5+zg4DMnULkpOT5efnV+gTeXsqvJMMkAHLzwEAAABA0UAi7yZsPfLHj0tWq3NjAQAAAAA4Dom8mwgONs9JSVJcnHNjAQAAAAA4Dom8m/DxkcqVM6+ZJw8AAAAA7otE3o3YhtczTx4AAAAA3BdV693ImDFSYqJUp46zIwEAAAAAOAqJvBu5/35nRwAAAAAAcDSG1gMAAAAAUIjQI+9Gjh2Tfv9dKl5catXK2dEAAAAAAByBHnk3snq1dPfd0ksvOTsSAAAAAICjkMi7kZAQ88zycwAAAADgvkjk3YgtkWf5OQAAAABwXyTybsS2jvy5c9Lly86NBQAAAADgGCTybqRUKcnX17w+fty5sQAAAAAAHINE3o1YLOm98gyvBwAAAAD3RCLvZih4BwAAAADujXXk3czIkdLFi1KTJs6OBAAAAADgCCTybubuu50dAQAAAADAkRhaDwAAAABAIUKPvJs5cUL69VfJx0dq397Z0QAAAAAA7I1E3s389pvUpYvUuDGJPAAAAAC4I4bWuxlb1XqWnwMAAAAA90Qi72Zs68gfPy5Zrc6NBQAAAABgfyTybiYoyDwnJ0unTzs3FgAAAACA/ZHIuxkfH6l8efM6Nta5sQAAAAAA7I9E3g0xTx4AAAAA3BeJvBuyzZOnRx4AAAAA3A+JvBuKiZG++EJq0cLZkQAAAAAA7M3pifyUKVMUEREhPz8/NW3aVOvXr7/m/mfPnlV0dLRCQ0Pl6+uryMhILV68OO3zlJQUjRo1SpUrV1axYsVUtWpVvfTSS0pNTXX0pbiM9u2lBx+UKlVydiQAAAAAAHvzcubJ58yZo5iYGE2dOlVNmzbVpEmT1K5dO+3atUtBtvLrV0lMTFSbNm0UFBSkuXPnKiwsTAcPHlTp0qXT9nn99df1/vvv65NPPlHt2rX1+++/q3///ipVqpSeeOKJArw6AAAAAADsz6mJ/MSJEzVw4ED1799fkjR16lQtWrRIM2bM0LPPPptp/xkzZiguLk5r166Vt7e3JCkiIiLDPmvXrlWXLl3UqVOntM+/+OKLa/b0JyQkKCEhIe19fHy8JCkpKUlJSUn5ukZHssX27xhPnZLWrrXIYpE6dy46IxHgGrJrl4Az0S7himiXcEW0S7iaotQmc3ONllQnjTlPTEyUv7+/5s6dq3vuuSdte9++fXX27FktWLAg03c6duyosmXLyt/fXwsWLFBgYKB69uypESNGyNPTU5I0fvx4TZs2TcuWLVNkZKT+/PNPtW3bVhMnTlSvXr2yjGXs2LEaN25cpu2zZ8+Wv7+/fS64AG3dWk6jRt2msLDzmjJllbPDAQAAAABcx6VLl9SzZ0+dO3dOAQEB19zXaT3yp06dUkpKioKDgzNsDw4O1s6dO7P8zr59+7Rq1Sr16tVLixcv1p49ezR48GAlJSVpzJgxkqRnn31W8fHxqlGjhjw9PZWSkqJXXnkl2yRekkaOHKmYmJi09/Hx8QoPD1fbtm2v+wN0pqSkJC1fvlxt2rRJG6EgSVWqSKNGSefPl1DHjh2dGCGKouzaJeBMtEu4ItolXBHtEq6mKLVJ28jwnHDq0PrcslqtCgoK0rRp0+Tp6alGjRrpyJEjmjBhQloi/9VXX+nzzz/X7NmzVbt2bW3evFlPPvmkKlSooL59+2Z5XF9fX/n6+mba7u3tXSgay7/jvOEG8xwfb1FSkrcK4aACuIHC8vuDooV2CVdEu4Qrol3C1RSFNpmb63NaIl++fHl5enrq+PHjGbYfP35cISEhWX4nNDRU3t7eacPoJalmzZqKjY1VYmKifHx89PTTT+vZZ5/Vgw8+KEmqW7euDh48qFdffTXbRN7dBARIfn7SlSvS8eNS5crOjggAAAAAYC9OW37Ox8dHjRo10sqVK9O2Wa1WrVy5UlFRUVl+p1mzZtqzZ4+sVmvatt27dys0NFQ+Pj6SzLwCD4+Ml+Xp6ZnhO+7OYpFCQ83rY8ecGwsAAAAAwL6cuo58TEyMpk+frk8++UQ7duzQoEGDdPHixbQq9n369NHIkSPT9h80aJDi4uI0dOhQ7d69W4sWLdL48eMVHR2dtk/nzp31yiuvaNGiRTpw4IDmzZuniRMn6t577y3w63Mm26CG2FjnxgEAAAAAsC+nzpHv3r27Tp48qdGjRys2NlYNGjTQkiVL0grgHTp0KEPvenh4uJYuXaphw4apXr16CgsL09ChQzVixIi0fd59912NGjVKgwcP1okTJ1ShQgU99thjGj16dIFfnzPZEnl65AEAAADAvTi92N2QIUM0ZMiQLD9bvXp1pm1RUVH69ddfsz1eyZIlNWnSJE2aNMlOERZOjz8ude8u3XyzsyMBAAAAANiT0xN5OMYddzg7AgAAAACAIzh1jjwAAAAAAMgdeuTd1Jkz0po1UnKydP/9zo4GAAAAAGAvJPJuau9e6d57pQoVSOQBAAAAwJ0wtN5N2daRP35cslqdGwsAAAAAwH5I5N1UUJBksUgpKdKpU86OBgAAAABgLyTybsrbWypf3ryOjXVuLAAAAAAA+yGRd2MhIeb52DHnxgEAAAAAsB8SeTdmmydPjzwAAAAAuA8SeTdGjzwAAAAAuB+Wn3Njjz4qdeok3XSTsyMBAAAAANgLibwba9bM2REAAAAAAOyNofUAAAAAABQi9Mi7sfh4aeVK6dIlqVcvZ0cDAAAAALAHEnk3duyYdN99UsmSJPIAAAAA4C4YWu/GbMvPnT8vXbzo3FgAAAAAAPZBIu/GSpaUihUzr48fd24sAAAAAAD7IJF3YxZLeq88a8kDAAAAgHsgkXdzISHmOTbWuXEAAAAAAOyDRN7N2RJ5euQBAAAAwD2QyLs529B6euQBAAAAwD2w/Jyb69tXatFCql/f2ZEAAAAAAOyBRN7NNWliHgAAAAAA98DQegAAAAAAChESeTd38aL07bfSjBnOjgQAAAAAYA8MrXdz585JXbtKnp5mvrynp7MjAgAAAADkBz3ybi4oSLJYpJQU6dQpZ0cDAAAAAMgvEnk35+UlBQaa1yxBBwAAAACFH4l8ERASYp6PHXNuHAAAAACA/CORLwJCQ80zPfIAAAAAUPiRyBcB9MgDAAAAgPsgkS8C6JEHAAAAAPfB8nNFQPfu0k03SXXqODsSAAAAAEB+kcgXAQ0amAcAAAAAoPBjaD0AAAAAAIUIiXwRcOWK9M030vvvOzsSAAAAAEB+MbS+CEhKku6/37x+6CGpRAnnxgMAAAAAyDt65IuAEiUkf3/z+vhx58YCAAAAAMgfEvkiwGJJX4KOteQBAAAAoHAjkS8iQkLMM2vJAwAAAEDhRiJfRNgSeXrkAQAAAKBwI5EvImxD6+mRBwAAAIDCjUS+iGBoPQAAAAC4B5afKyLuvVeqUcM8AAAAAACFF4l8EVGrlnkAAAAAAAo3lxhaP2XKFEVERMjPz09NmzbV+vXrr7n/2bNnFR0drdDQUPn6+ioyMlKLFy9O+zwiIkIWiyXTIzo62tGXAgAAAACAQzm9R37OnDmKiYnR1KlT1bRpU02aNEnt2rXTrl27FBQUlGn/xMREtWnTRkFBQZo7d67CwsJ08OBBlS5dOm2fDRs2KCUlJe39tm3b1KZNGz3wwAMFcUkuKSlJWrDAzJEfNEjy9HR2RAAAAACAvHB6Ij9x4kQNHDhQ/fv3lyRNnTpVixYt0owZM/Tss89m2n/GjBmKi4vT2rVr5e3tLcn0wF8tMDAww/vXXntNVatWVYsWLRxzEYWAh4fUvbtktUoPPCAFBzs7IgAAAABAXjg1kU9MTNTGjRs1cuTItG0eHh5q3bq11q1bl+V3Fi5cqKioKEVHR2vBggUKDAxUz549NWLECHlm0c2cmJiozz77TDExMbJYLFkeMyEhQQkJCWnv4+PjJUlJSUlKSkrKzyU6lC22nMYYGOil48ctOnQoSWXLOjIyFGW5bZdAQaBdwhXRLuGKaJdwNUWpTebmGp2ayJ86dUopKSkK/lf3cHBwsHbu3Jnld/bt26dVq1apV69eWrx4sfbs2aPBgwcrKSlJY8aMybT//PnzdfbsWfXr1y/bOF599VWNGzcu0/Zly5bJ398/dxflBMuXL8/Rfv7+LSSV1nff/a6jR084NigUeTltl0BBol3CFdEu4Ypol3A1RaFNXrp0Kcf7On1ofW5ZrVYFBQVp2rRp8vT0VKNGjXTkyBFNmDAhy0T+o48+UocOHVShQoVsjzly5EjFxMSkvY+Pj1d4eLjatm2rgIAAh1yHPSQlJWn58uVq06ZN2jSDa3n/fU/t3y+FhzdRx46pBRAhiqLctkugINAu4Ypol3BFtEu4mqLUJm0jw3PCqYl8+fLl5enpqePHj2fYfvz4cYWEhGT5ndDQUHl7e2cYRl+zZk3FxsYqMTFRPj4+adsPHjyoFStW6Ntvv71mHL6+vvL19c203dvbu1A0lpzGabuXcfKklwrBZaGQKyy/PyhaaJdwRbRLuCLaJVxNUWiTubk+py4/5+Pjo0aNGmnlypVp26xWq1auXKmoqKgsv9OsWTPt2bNHVqs1bdvu3bsVGhqaIYmXpI8//lhBQUHq1KmTYy7AyV57zUNPP327vv4667n//2a7N3LsmAODAgAAAAA4lNPXkY+JidH06dP1ySefaMeOHRo0aJAuXryYVsW+T58+GYrhDRo0SHFxcRo6dKh2796tRYsWafz48ZnWiLdarfr444/Vt29feXkVuhkEOXLwoEV//11G27fnLpGPjXVgUAAAAAAAh3J6htu9e3edPHlSo0ePVmxsrBo0aKAlS5akFcA7dOiQPDzS7zeEh4dr6dKlGjZsmOrVq6ewsDANHTpUI0aMyHDcFStW6NChQxowYECBXk9Biogw89wPHMhZIt+xo1SxolS9uiOjAgAAAAA4ktMTeUkaMmSIhgwZkuVnq1evzrQtKipKv/766zWP2bZtW6WmundBt/REPmf7V6tmHgAAAACAwsvpQ+uRd5Urm+eDB3PWIw8AAAAAKPxI5AsxW4/8kSNSQsL1909Nlb7+Wnr3XenyZQcHBwAAAABwCBL5QiwwUPL1TVZqqkWHDl1/f4tF6t9feuIJk/wDAAAAAAofEvlCzGKRgoMvSZL278/Zd0JDzTNL0AEAAABA4UQiX8gFBeUukWcJOgAAAAAo3EjkC7ncJvL0yAMAAABA4UYiX8jldmg9PfIAAAAAULiRyBdyeZ0jTyIPAAAAAIUTiXwhFxR0UVLue+QZWg8AAAAAhZOXswNA/th65E+dki5ckEqUuPb+rVpJ33wjVa1aAMEBAAAAAOyORL6QK148WWXKpOrMGYv275fq1r32/pUqmQcAAAAAoHBiaL0biIgwzwcOODMKAAAAAEBBIJF3AxERqZJyPk/+22+ld96Rzp51XEwAAAAAAMdgaL0bqFw5d4l8dLSpWn/bbdJNNzkwMAAAAACA3dEj7wZsQ+tZgg4AAAAA3B+JvBuoVCl3PfK2JehI5AEAAACg8CGRdwNXz5FPTb3+/rYeedaSBwAAAIDCh0TeDdiG1p8/L8XFXX9/euQBAAAAoPAikXcDxYqlJ+c5GV5v25ceeQAAAAAofEjk3UTlyuY5J4k8xe4AAAAAoPBi+Tk3UbmytG5dzhL5W281a8nbkn8AAAAAQOFBIu8mctMjX6GCdO+9jo0HAAAAAOAYDK13E7ZE/sABp4YBAAAAAHAwEnk3kZseeUmaP196+23pxAmHhQQAAAAAcACG1ruJq3vkrVbJ4zq3aJ55Rvr7b6lhQykoyOHhAQAAAADshB55NxEeLnl6SgkJOatGzxJ0AAAAAFA4kci7CS8vqWJF85ol6AAAAADAfZHIu5HczJOnRx4AAAAACicSeTeSm0SeHnkAAAAAKJxI5N0IPfIAAAAA4P5I5N0IPfIAAAAA4P5Yfs6N5CaRb9RImjdPuuEGx8YEAAAAALAvEnk3YkvkDx+WkpIkb+/s9y1fXrrnngIJCwAAAABgRwytdyMhIZKvr2S1Sv/84+xoAAAAAACOQCLvRjw8pIgI8zonw+v/9z9p0iTTgw8AAAAAKBwYWu9mKleWdu3KWSI/dqy0aZNUvboUHu7w0AAAAAAAdkCPvJvJyxJ0VK4HAAAAgMKDRN7N5GZovW0JOtaSBwAAAIDCg0TezdAjDwAAAADujUTezeQlkadHHgAAAAAKDxJ5N2NL5GNjpcuXr72vbWg9PfIAAAAAUHiQyLuZsmWlkiXN6wMHrr0vQ+sBAAAAoPAhkXczFkvOh9fXri3Nny999ZXDwwIAAAAA2AnryLuhypWlLVuun8iXLi116VIgIQEAAAAA7IQeeTdk65G/3tB6AAAAAEDhQ4+8G8pN5fpt26TFi6XISOmeexwaFgAAAADADpzeIz9lyhRFRETIz89PTZs21fr166+5/9mzZxUdHa3Q0FD5+voqMjJSixcvzrDPkSNH1Lt3b5UrV07FihVT3bp19fvvvzvyMlxKbhL5pUulESOkWbMcGxMAAAAAwD6c2iM/Z84cxcTEaOrUqWratKkmTZqkdu3aadeuXQoKCsq0f2Jiotq0aaOgoCDNnTtXYWFhOnjwoEqXLp22z5kzZ9SsWTPdcccd+v777xUYGKi///5bZcqUKcArc67cJPI33WSeN250XDwAAAAAAPtxaiI/ceJEDRw4UP3795ckTZ06VYsWLdKMGTP07LPPZtp/xowZiouL09q1a+Xt7S1JioiIyLDP66+/rvDwcH388cdp2yrbMtsiwvYjOXNGOndOKlUq+31tifzBg9Lp01K5cg4PDwAAAACQD05L5BMTE7Vx40aNHDkybZuHh4dat26tdevWZfmdhQsXKioqStHR0VqwYIECAwPVs2dPjRgxQp6enmn7tGvXTg888IDWrFmjsLAwDR48WAMHDsw2loSEBCUkJKS9j4+PlyQlJSUpKSnJHpfrELbY/h2jr69UvryXTp2yaPfuJDVokP0x/P2latW8tGePRevXJ6t161QHRoyiILt2CTgT7RKuiHYJV0S7hKspSm0yN9fotET+1KlTSklJUXBwcIbtwcHB2rlzZ5bf2bdvn1atWqVevXpp8eLF2rNnjwYPHqykpCSNGTMmbZ/3339fMTExeu6557RhwwY98cQT8vHxUd++fbM87quvvqpx48Zl2r5s2TL5+/vn80odb/ny5Zm2lSlzu06dKqNvv/1DR48eu+b3Q0Iaac+eivrii91KTPzbUWGiiMmqXQLORruEK6JdwhXRLuFqikKbvHTpUo73LVRV661Wq4KCgjRt2jR5enqqUaNGOnLkiCZMmJCWyFutVjVu3Fjjx4+XJDVs2FDbtm3T1KlTs03kR44cqZiYmLT38fHxCg8PV9u2bRUQEOD4C8ujpKQkLV++XG3atEmbamDz2Wee+vtvqWzZRurY0XrN4+zY4aGff5YuXqyhjh2rOzJkFAHXapeAs9Au4Ypol3BFtEu4mqLUJm0jw3PCaYl8+fLl5enpqePHj2fYfvz4cYWEhGT5ndDQUHl7e6cNo5ekmjVrKjY2VomJifLx8VFoaKhq1aqV4Xs1a9bUN998k20svr6+8vX1zbTd29u7UDSWrOKsWtU8HzrkKW9vzyy+la5JE/O8bZuHvL2dvpAB3ERh+f1B0UK7hCuiXcIV0S7haopCm8zN9Tkta/Px8VGjRo20cuXKtG1Wq1UrV65UVFRUlt9p1qyZ9uzZI6s1vYd59+7dCg0NlY+PT9o+u3btyvC93bt3q1KlSg64CteVm8r1t9wibdggbdni2JgAAAAAAPnn1O7XmJgYTZ8+XZ988ol27NihQYMG6eLFi2lV7Pv06ZOhGN6gQYMUFxenoUOHavfu3Vq0aJHGjx+v6OjotH2GDRumX3/9VePHj9eePXs0e/ZsTZs2LcM+RUFuEvnixaXGjU2RPAAAAACAa3PqHPnu3bvr5MmTGj16tGJjY9WgQQMtWbIkrQDeoUOH5OGRfq8hPDxcS5cu1bBhw1SvXj2FhYVp6NChGjFiRNo+TZo00bx58zRy5Ei9+OKLqly5siZNmqRevXoV+PU5ky2RP3BASk2VLBanhgMAAAAAsBOnF7sbMmSIhgwZkuVnq1evzrQtKipKv/766zWPedddd+muu+6yR3iF1g03mOT90iXp5EkpKOja+2/eLL33nhQYKL3ySoGECAAAAADIAyqbuSlfXykszLzOyfD6kyel6dOlOXMcGxcAAAAAIH9I5N1YRIR5zkkif9NN5nnvXunsWUdFBAAAAADILxJ5N5abgnflyqUn/ps2OSwkAAAAAEA+kci7sdwk8pLUqJF53rjRMfEAAAAAAPKPRN6N5TaRtw2vJ5EHAAAAANdFIu/G6JEHAAAAAPdDIu/GbIn8oUNSSsr197cl8qmpUkKC4+ICAAAAAOQdibwbCwuTvL2lpCTpyJHr71++vHTmjLRnj1m+DgAAAADgekjk3Zinp3TDDeZ1TofXly7tsHAAAAAAAHZAIu/mbMPrDxxwahgAAAAAADshkXdzuS14t3u3dOedUlSU42ICAAAAAOSdV16+dPjwYVksFlWsWFGStH79es2ePVu1atXSo48+atcAkT+5TeTLlJFWrZIsFun8ealkScfFBgAAAADIvTz1yPfs2VM//PCDJCk2NlZt2rTR+vXr9fzzz+vFF1+0a4DIn4gI85zTRD4wUAoPN5Xr//jDYWEBAAAAAPIoT4n8tm3bdPPNN0uSvvrqK9WpU0dr167V559/rpkzZ9ozPuRTbnvkJemmm8wz68kDAAAAgOvJUyKflJQk3/9fn2zFihW6++67JUk1atTQsWPH7Bcd8s2WyB85kvO14W3ryZPIAwAAAIDryVMiX7t2bU2dOlU//fSTli9frvbt20uSjh49qnLlytk1QORPUJDk72+Gyh86lLPv2BL5TZscFxcAAAAAIG/ylMi//vrr+uCDD9SyZUv16NFD9evXlyQtXLgwbcg9XIPFkvt58rZEfudO6cIFh4QFAAAAAMijPFWtb9mypU6dOqX4+HiVKVMmbfujjz4qf39/uwUH+6hcWdq+PeeJfHCwVL++6c2Pi5NKlHBsfAAAAACAnMtTIn/58mWlpqamJfEHDx7UvHnzVLNmTbVr186uASL/8lLwbvNmh4QCAAAAAMinPA2t79Kliz799FNJ0tmzZ9W0aVO9+eabuueee/T+++/bNUDkX14SeQAAAACAa8pTIr9p0yY1b95ckjR37lwFBwfr4MGD+vTTT/XOO+/YNUDkny2RP3Ag9989e9aekQAAAAAA8itPifylS5dUsmRJSdKyZct03333ycPDQ7fccosOHjxo1wCRf3npkT93TqpUSSpXTrp40TFxAQAAAAByL0+JfLVq1TR//nwdPnxYS5cuVdu2bSVJJ06cUEBAgF0DRP7ZqtafPJnzKvSlSklJSZLVKv35p8NCAwAAAADkUp4S+dGjR2v48OGKiIjQzTffrKioKEmmd75hw4Z2DRD5V7q0eUi5G15/003meeNGOwcEAAAAAMizPCXy999/vw4dOqTff/9dS5cuTdt+55136q233rJbcLCfvAyvt60nTyIPAAAAAK4jT8vPSVJISIhCQkL0zz//SJIqVqyom2++2W6Bwb4qV5b++INEHgAAAAAKuzz1yFutVr344osqVaqUKlWqpEqVKql06dJ66aWXZLVa7R0j7CA/PfLbt0uXLtk/JgAAAABA7uWpR/7555/XRx99pNdee03NmjWTJP38888aO3asrly5oldeecWuQSL/8pLIV6ggBQVJJ06Ygnf/XwoBAAAAAOBEeUrkP/nkE3344Ye6++6707bVq1dPYWFhGjx4MIm8C8pLIm+xSL17S5cvmyr2AAAAAADny1MiHxcXpxo1amTaXqNGDcXFxeU7KNjf1Yl8aqpJ0nPizTcdFxMAAAAAIPfyNEe+fv36mjx5cqbtkydPVr169fIdFOzPtpb8+fMS91oAAAAAoPDKU4/8G2+8oU6dOmnFihVpa8ivW7dOhw8f1uLFi+0aIOyjWDEpJESKjTVryZcrl/PvXr5s5sg3bCj5+josRAAAAABADuSpR75FixbavXu37r33Xp09e1Znz57Vfffdp7/++kuzZs2yd4ywk7zMk5ekatVMobvNm+0eEgAAAAAgl/K8jnyFChUyFbX7888/9dFHH2natGn5Dgz2V7mytG5d7hP5evWko0fNevJNmzomNgAAAABAzuSpRx6Fk22efG4Tedt68ps22TUcAAAAAEAekMgXIXkdWm9L5DdutG88AAAAAIDcI5EvQvKayN90k3netk26csW+MQEAAAAAcidXc+Tvu+++a35+9uzZ/MQCB7Ml8gcOSFar5JHD2zg33GCq3J8+LW3dKjVp4rAQAQAAAADXkatEvlSpUtf9vE+fPvkKCI4THm6S94QEswxdhQo5+57FYobXL1tm5smTyAMAAACA8+Qqkf/4448dFQcKgLe3SeYPHjTD63OayEvSww9L7dpJLVo4Lj4AAAAAwPXlefk5FE6VK6cn8s2a5fx73bo5LiYAAAAAQM5R7K6IyWvBOwAAAACAayCRL2KuLniXW3//LX3+uenRBwAAAAA4B4l8EZOfHvnBg6XevU3ROwAAAACAc5DIFzH5SeQbNTLPGzfaLx4AAAAAQO64RCI/ZcoURUREyM/PT02bNtX69euvuf/Zs2cVHR2t0NBQ+fr6KjIyUosXL077fOzYsbJYLBkeNWrUcPRlFAoREeb58GEpOTl3373pJvNMIg8AAAAAzuP0qvVz5sxRTEyMpk6dqqZNm2rSpElq166ddu3apaCgoEz7JyYmqk2bNgoKCtLcuXMVFhamgwcPqnTp0hn2q127tlasWJH23svL6ZfqEkJDJV9fs5b84cPpPfQ5YeuR37JFSkyUfHwcEyMAAAAAIHtOz24nTpyogQMHqn///pKkqVOnatGiRZoxY4aeffbZTPvPmDFDcXFxWrt2rby9vSVJEbZu5qt4eXkpJCTEobEXRh4eUqVK0u7dZnh9bhL5KlWk0qWls2elv/6SGjZ0VJQAAAAAgOw4NZFPTEzUxo0bNXLkyLRtHh4eat26tdatW5fldxYuXKioqChFR0drwYIFCgwMVM+ePTVixAh5enqm7ff333+rQoUK8vPzU1RUlF599VXdcMMNWR4zISFBCQkJae/j4+MlSUlJSUpKSrLHpTqELbbcxhgR4anduz20Z0+ymjdPzdV3Gzb01A8/eGjDhmTVqZO776JoyGu7BByJdglXRLuEK6JdwtUUpTaZm2t0aiJ/6tQppaSkKDg4OMP24OBg7dy5M8vv7Nu3T6tWrVKvXr20ePFi7dmzR4MHD1ZSUpLGjBkjSWratKlmzpypG2+8UceOHdO4cePUvHlzbdu2TSVLlsx0zFdffVXjxo3LtH3ZsmXy9/e3w5U61vLly3O1v4dHPUmVtWLFXgUHZ/1zzk6pUrUkVde8eYcVHLwlV99F0ZLbdgkUBNolXBHtEq6IdglXUxTa5KVLl3K8r9OH1ueW1WpVUFCQpk2bJk9PTzVq1EhHjhzRhAkT0hL5Dh06pO1fr149NW3aVJUqVdJXX32lhx9+ONMxR44cqZiYmLT38fHxCg8PV9u2bRUQEOD4i8qjpKQkLV++XG3atEmbZpAT27d7aMkSydOzujp2rJKrc4aHSw89lKwmTSoqJKRibkNGEZDXdgk4Eu0Sroh2CVdEu4SrKUpt0jYyPCecmsiXL19enp6eOn78eIbtx48fz3Z+e2hoqLy9vTMMo69Zs6ZiY2OVmJgonywqsJUuXVqRkZHas2dPlsf09fWVr69vpu3e3t6ForHkNs5q1czzwYMe8vbO3cIFN92UXr0euJbC8vuDooV2CVdEu4Qrol3C1RSFNpmb63Pq8nM+Pj5q1KiRVq5cmbbNarVq5cqVioqKyvI7zZo10549e2S1WtO27d69W6GhoVkm8ZJ04cIF7d27V6Ghofa9gEIqP2vJAwAAAACcy+nryMfExGj69On65JNPtGPHDg0aNEgXL15Mq2Lfp0+fDMXwBg0apLi4OA0dOlS7d+/WokWLNH78eEVHR6ftM3z4cK1Zs0YHDhzQ2rVrde+998rT01M9evQo8OtzRbZEPjZWunw5999ft04aO1a6anU/AAAAAEABcfoc+e7du+vkyZMaPXq0YmNj1aBBAy1ZsiStAN6hQ4fk4ZF+vyE8PFxLly7VsGHDVK9ePYWFhWno0KEaMWJE2j7//POPevToodOnTyswMFC33Xabfv31VwUGBhb49bmismWlkiWl8+elgwelGjVy9/1vvpHefFMaPFhq3doxMQIAAAAAsub0RF6ShgwZoiFDhmT52erVqzNti4qK0q+//prt8b788kt7heaWLBbTK79lixlen9tEvlEj87xxo/1jAwAAAABcm9OH1sM5IiLMc17myduK3f35p5ScbLeQAAAAAAA5QCJfROWn4F316mZo/pUr0o4d9o0LAAAAAHBtJPJFVH4SeQ8PqWFD85rh9QAAAABQsEjki6j8LkHHPHkAAAAAcA4S+SLKXon8tm32iQcAAAAAkDMuUbUeBc+WyJ85I507J5Uqlbvvd+xokvjcVrwHAAAAAOQPPfJFVIkSUvny5nVeeuXLlJFq15Y8Pe0bFwAAAADg2kjkizBbr/yBA04NAwAAAACQCyTyRVh+58mvWSM99JD0xhv2iwkAAAAAcG0k8kVYfhP5Q4ekzz6TFi60X0wAAAAAgGsjkS/C7FW5/o8/pJQU+8QEAAAAALg2EvkiLCLCPOc1kb/xRql4cenSJWnXLruFBQAAAAC4BhL5IuzqHvnU1Nx/39NTatDAvN60yW5hAQAAAACugUS+CIuIkPz9TY/6qlV5O8ZNN5nnadOkkyftFhoAAAAAIBsk8kWYj4/Uv795PXFi3o7Ru7fk6yv99JM0cqT9YgMAAAAAZI1Evoh78knJYpEWL5Z27Mj992++WVq/XmrTRnr9dbuHBwAAAAD4FxL5Iq5aNalLF/P6rbfydox69aRly6Ry5cz71FTppZekvXvtEyMAAAAAIB2JPPTUU+b500+lEyfyf7zPPpNGjzaF8GbNyv/xAAAAAADpSOShZs3MEPmEBOm99/J/vJYtpdtvly5ckPr0kR56SIqPz/9xAQAAAAAk8pCZIx8TY16/9550+XL+jhcebqrgv/iiWaLus8+khg3NXHoAAAAAQP6QyEOS1LWrVKmSWULus8/yfzxPT2nUKOnHH81x9+0zPf/vv5//YwMAAABAUUYiD0mSl5c0dKh5PXGiZLXa57i33ipt3ix17y6lpEiRkfY5LgAAAAAUVSTySPPww1JAgLRzp7Rkif2OW7q09MUXZmj9nXemb4+Ntd85AAAAAKCoIJFHmoAAaeBA8/rNN+17bItFatw4/f3evaZ3/vHHpStX7HsuAAAAAHBnJPLI4IknzPz2VavMkHhHWbJEOn9emjzZVMzfvt1x5wIAAAAAd0IijwxuuEF64AHzeuJEx50nOlr6/nspKEjaulVq0sQUxgMAAAAAXBuJPDJ56inz/MUX0pEjjjtP+/bSli1Sq1bSpUvSXXdJGzY47nwAAAAA4A5I5JFJ48ZS8+ZScrIZ+u5IwcHSd99Jd9xhhtq3b2+K7QEAAAAAskYijyzZeuWnTpUuXHDsuYoVkxYskJo2lRo1ksLDHXs+AAAAACjMSOSRpc6dpWrVpLNnpY8/dvz5SpY0BfD+9z+peHHHnw8AAAAACisSeWTJw0MaNsy8njRJSklx/DlLl5Z8fc3r1FTpnXdYax4AAAAA/o1EHtnq108qW1bat88MfS9Ir70mDR0qtW0rxcUV7LkBAAAAwJWRyCNb/v7SoEHm9ZtvFuy5u3WTQkPN0nQdOphCeAAAAAAAEnlcR3S05OMjrV0r/fprwZ23alVp+XKpXDlp/XozZ//SpYI7PwAAAAC4KhJ5XFNoqNSzp3k9cWLBnrt2bWnpUlMIb80a6f77pcTEgo0BAAAAAFwNiTyuy1b07ptvpAMHCvbcjRpJixaZJeq+/17q3dsUwgMAAACAoopEHtdVr57Upo1ktUpvv13w52/eXJo/3yTz7dpJFkvBxwAAAAAAroJEHjny1FPm+cMPzdryBa1tW2nvXunhhwv+3AAAAADgSkjkkSNt25o56xcumGTeXlJSpKlTpZEjr1/MLjQ0/fXJk9J779kvDgAAAAAoLEjkkSMWixQTY16//baUlJT/Y+7caYbNDxpk1o1v0yZna8Zfviy1aGEq6r/2Wv7jAAAAAIDChEQeOdarlxQcLP3zj/T113k/TnKyScAbNJDWrTNV6UuVMkvctWghHTly7e8XKyb1729ejxwpTZ6c91gAAAAAoLAhkUeO+fqaXnBJevPNvFWP37JFuuUWk4AnJEgdOkh//SX99JMZOr9tm9SsmbR797WP8/TT0qhR5vXjj0szZ+Y+FgAAAAAojEjkkSuDBpke8U2bpB9/zPn3EhOlsWPNcnIbN0plykiffGKWlgsPl+rWlX75RapWTTp4ULrtNrPftYwbJw0dal4//LA0b16eLwsAAAAACg0SeeRK+fJS377m9Ztv5uw7v/8uNW5sEu/kZOnee6Xt26U+fTIuJVe5sknmb7rJFLNr2VJatSr741os0ltvmSTeajU3GS5fzvOlAQAAAEChQCKPXHvySfP8v/9dewj8lSvSs89KTZtKW7dKgYHSnDnSN99IISFZfycoSPrhB+mOO0yF/A4dpLlzsz+HxSK9/74UESEVLy7t25fXqwIAAACAwoFEHrl2441S587m9VtvZb3P2rWmmN3rr5ve8h49zFz4bt0y9sJnJSBAWrxYuu8+MyS/Wzfpgw+y39/bW1q2TNq1yyyRBwAAAADuzCUS+SlTpigiIkJ+fn5q2rSp1q9ff839z549q+joaIWGhsrX11eRkZFavHhxlvu+9tprslgsetLWjQy7eOop8/zJJ9KpU+nbL140Pfa33WYS69BQaf58afZs0yOfU35+0ldfSY8+aorq/ec/0ssvZ19gr3p1ycsrr1cDAAAAAIWH0xP5OXPmKCYmRmPGjNGmTZtUv359tWvXTidOnMhy/8TERLVp00YHDhzQ3LlztWvXLk2fPl1hYWGZ9t2wYYM++OAD1atXz9GXUeTcfruZy375sjR1qtn2ww9SvXpmnfnUVLNE3F9/SV265O0cnp7m2M8/b96PGmVuElit2X8nMdFUsE9MzNs5AQAAAMDVOT2RnzhxogYOHKj+/furVq1amjp1qvz9/TVjxows958xY4bi4uI0f/58NWvWTBEREWrRooXq16+fYb8LFy6oV69emj59usqUKVMQl1KkWCzpvfKTJ5se81atzBz18HBpyRJpxgxTnT6/53n5ZWnSJPP+nXekhx7KOlFPTTU3GPr3NyMFkHsXL0qff26KDQIAAABwTU4djJyYmKiNGzdq5MiRads8PDzUunVrrVu3LsvvLFy4UFFRUYqOjtaCBQsUGBionj17asSIEfL09EzbLzo6Wp06dVLr1q318ssvXzOOhIQEJSQkpL2Pj4+XJCUlJSkpKSk/l+hQtticFeM990gVK3rpn38saXPYH3ssRa+8YlVAgGTPsAYPlkqXtuiRRzw1e7ZFp09b9eWXKSpePON+Dzzgod9+89Srr6aqV69keXvbLwZ3t3On9OCDXtq+3aLatVP1yy/J8vfP/XGc3S6BrNAu4Ypol3BFtEu4mqLUJnNzjU5N5E+dOqWUlBQFBwdn2B4cHKydO3dm+Z19+/Zp1apV6tWrlxYvXqw9e/Zo8ODBSkpK0pgxYyRJX375pTZt2qQNGzbkKI5XX31V48aNy7R92bJl8s9LJlPAli9f7rRzd+hQWdOn11Nw8EUNGfKH6tY9rZ9/dsy5SpeWRo4M0uuvN9HSpV5q2vSsXnjhVwUEpDf48HBPlSrVWvv3++nZZ7fqzjsPOyYYN/PzzxU0eXJDXbliKhH+9ZdF999/VEOGbM7zMZ3ZLoHs0C7himiXcEW0S7iaotAmL126lON9Lamp2ZUPc7yjR48qLCxMa9euVVRUVNr2Z555RmvWrNFvv/2W6TuRkZG6cuWK9u/fn9YDP3HiRE2YMEHHjh3T4cOH1bhxYy1fvjxtbnzLli3VoEEDTbKNz/6XrHrkw8PDderUKQUEBNjxiu0rKSlJy5cvV5s2beTtpK7n1FRpyxYpMlIqVqxgzvnrrxZ16eKpM2csqlEjVYsXJ6tixfTP33zTQyNHeqpatVRt2ZJMEbxrSEyURozw0JQp5nepZUurHn7Yqr59PWW1WjRjRrJ6987dXxGu0C6Bf6NdwhXRLuGKaJdwNUWpTcbHx6t8+fI6d+7cdfNQp6Y45cuXl6enp44fP55h+/HjxxWSzULjoaGh8vb2zjCMvmbNmoqNjU0bqn/ixAnddNNNaZ+npKToxx9/1OTJk5WQkJDhu5Lk6+srX1/fTOfy9vYuFI3F2XE2blyw52veXPrpJ6ldO2nnTotatPDWsmVSjRrm8yFDpP/+V9qzx6JvvvFW794FG19hceiQWdrPdr/sueekceM85OXloX37pDFjpMcf91JUVPrPNjec3S6BrNAu4Ypol3BFtEu4mqLQJnNzfU4tdufj46NGjRpp5cqVadusVqtWrlyZoYf+as2aNdOePXtkvap0+e7duxUaGiofHx/deeed2rp1qzZv3pz2aNy4sXr16qXNmzdnSuJRONWuLf3yixkJcPiwWe7ul1/MZyVKpBfie/llKSXFeXG6qiVLpIYNTRJfpoz03XfSK6+kL+H3/POmeOHFi9IDD0i5GOUDAAAAwMGcXrU+JiZG06dP1yeffKIdO3Zo0KBBunjxovr37y9J6tOnT4ZieIMGDVJcXJyGDh2q3bt3a9GiRRo/fryio6MlSSVLllSdOnUyPIoXL65y5cqpTp06TrlGOEalStLPP5sRAadPm576gQNNxfXoaJOgRkRIcXHOjtR1pKSYnvaOHc3PpXFjadMmqVOnjPt5eprq9cHB0rZt0tChzokXAAAAQGZOnz3cvXt3nTx5UqNHj1ZsbKwaNGigJUuWpBXAO3TokDw80u83hIeHa+nSpRo2bJjq1aunsLAwDR06VCNGjHDWJcCJAgOlVaukxx83S859+KH0zTemJ37bNqlCBWdH6DpOnpR69pRWrDDvBw2S3npLymJWiSQpJESaPVtq3dr8XFu0ENMUAAAAABfg9ERekoYMGaIhQ4Zk+dnq1aszbYuKitKvv/6a4+NndQy4j5IlpZkzTW/8kCHS5s2mR376dGnKFOnWW50dofOtXWvmwx85Ivn7S9OmSb16Xf97rVqZHvyxY6X//Mf04OdlvjwAAAAA+3H60HrAXpo1k37/XZo82SxVt3mz2da9uzRqlKmwX9Skpppe9xYtTBJfo4a0YUPOknibF15gvjwAAADgSkjk4VY8PU1v/O7d0sMPm21ffWWG2j/6qJSc7Nz4CtK5c9L990sxMea6H3zQJPG1auXuOMyXBwAAAFwLiTzcUmCgmdf9669mrrdk3t90k/Tjj86NrSD8+acZBv/tt5K3txmlMHu2qeifF7b58haL+Tl+/rl94wUAAACQcyTycGtNm0p//CH5+Jj3W7eaYea9eklHjzo3NkeZOVO65RZpzx7phhtMZf/oaJOE54dtvrwkPfaYtGtXvkMFAAAAkAck8nB7ISGmqr0kBQWZhHb2bOnGG6U335SSkpwbnz09+aTUv7905YrUoYNZWu7mm+13/H/Pl7982X7HBgAAAJAzJPIoEoYPl/z8pBMnzDDzpk2lCxfM9vr1pZUrnR1h/m3cKL39trlR8dJL0nffSeXK2fccV8+X37qV+fKu7u+/pbp1pXHjJKvV2dEAAADAXkjkUSSEhJhid5I0Z45Zju2jj6Ty5aUdO8xa6d26SadPOzfO/Pjvf81zr16m59zDQb/dISEmmbdYzBJ/zJd3XZMmmQKFY8dKDz0kJSQ4O6KibcsW6ZNPTO2Oc+ecHQ0AACjMSORRZDzzjFSqlNSgganiPmCAqW4/ZIhJer/+2vTUb9/u7Ehz78ABE79kRhk42p13SqNHm9fMl3dNiYnSl1+mv589W2rfXjp71mkhFWl//indeqvUr58UFWWWyKxQwfwuPf649N570g8/SLGxRXOpTAAAkDsk8igywsLMWupvv51e/K5MGendd82ybBER0t69plDc4sVODTXX3npLSkmR2rQxUwUKwqhR0h132H++fGysWTKwMN5QcSWLF0txcSZZXLJEKllSWr1aatZMOnTI2dHl3OnTZqpIhw5mJE1hdOyY1Lmz+V2JiDB/F9m2r1plpvtER5v6E6GhUtmy5s/pkUdMHY/Fi6X9+5keAQAA0pHIo0gpXjzr7TfdJK1fLzVvLp0/L911lxmqXhh6xuLizDQBqWB64208PU0vb37ny6emSn/9Jb36qrmJEhoqde9u5nYPHly4pzs406efmudevaR27aSffjJJ/fbt5ue8ebNTw7uufftMT/UNN5jRH0uWSC1bprf1wuLyZalLF+nwYalGDbOKxj//mJERv/4qffyxGS3UubNUtaoZHXT2bPr0n+HDpU6dpCpVzPKRjRqZ7wAAgKKNRB5F0q+/mnnDVwsMlFasML1gqanS00+bYbBXrjgjwpybOtX09NWvb3rkC9K/58vPnp2z7yUnS2vWSDExUvXqUp060nPPSb/9Zj6/8UbT+/j++1JkpLnGlBTHXYe7OX3aFDuUpD59zHP9+qbd16ljeoKbN5eWLnVejNnZsMHUq6he3fRUX7okNWxoeuSTkszv5+OPF47VJqxWqW9fc03lypk/k9KlzWelSpmpPP36Sa+/Li1caJaMvHjRDMP/8kuz3GO3bubPzMfH3BTYtMlMCxoxgh56AACKMhJ5FDnHjkm33WYqea9fn/EzHx9p2jQz/N7Dw/Rq3nGHGe7tiq5ckd55x7wePjz/a8XnRU7ny1+4IH3zjUksg4NN7+pbb5npDL6+UseO0gcfSEePSjt3mvnCdeqYEQeDBkmNG0s//1xgl1WoffWVSXQbNDA/Q5vwcPMzvPNO8+fRqZNr9HBbrSbJbdHCLJf49ddmW/v2ZkWJjRvN5y++aPafPFlq21Y6edK5cV/P2LHmWry9pW+/NT3u1+PnJ9WrZ0aljB1rinNu3WoS/N27zZQWSXrjDalnT9e/0QgAAByDRB5FTmioqeAtmbm3/2axSE88YYbyli5tejGbNDFDYl3N559Lx49LFSua//g7i22+/IULUs+eXkpIMH+1HD1qkvOOHU2P5P33S7NmmeS8XDnTW/nNN9KpU9KiRWZlgdBQc8yWLc3P/J13zJ/D5s2mF7l3b1PrANmzDau39cZfrVQpM+e6Tx8zyuGRR8yNGGdMI0lIMDcS6tQxQ8t//FHy8jKxbdkiff+9mTdusZgba6NGSfPnmyHmq1eb30tXnSLw+efpf79Mmybdfnv+juflZUYpvPii+fP19jZJfps25vcJAAAULSTyKJKee84kBt99Z4aqZqVNGzPUOzLSzGm97TZp7tyCjfNarNb0JeeefNL8x95ZbPPlg4KkrVsteuWVW3TrrZ4KC5P+8x+TkCUmStWqSU89ZRK22Fhp5kzpvvtMYpYVLy8zjHr3bmngQJPQff65GXr/+ussp5aV3bvNzSdPT6lHj6z38fExP3tb7+5LL5kh3omJBRPjmTOmJkJEhLmRsGOHFBBgprPs32+WaKtbN+vvdulirq9qVengQVMUzrZig6v45Rcz/F2Snn3W/Gzt6aGHzI3GUqXMCItbbzU1BQAAQNFBIo8iqXr19CTn5Zez3y8y0iTz7dqZuboPPGCG5LvC3NRFi8wQ9IAAk+RKpnf+8GHnxJM+Xz5VW7YE6vffPWSxmKW2Xn3VFFnbvdvcfGje3CTpORUYaHo11683hdouXjQJUt26hW+FAUf77DPz3Lat+TPJjsVienenTzdJ/6efmqH2jlzf/OBBc9MpPNzcTIuNNaNJ/vtf027feMO8v57atU1baNvW/F526yY9/7xr/F7u3y/de6+5KXLvvdIrrzjmPK1amRsG4eFmOktUlJmLDwAAigYSeRRZzz9vkpl588ww3uyULm167p980rwfO9YMY794sQCCvAZbb/xjj5lkfs0a0+NdrZq0YIFzYmrdWpo6NUXNmh3R1KnJOnrUVN9+9lmpZs38z+Fv3NgkL598YubZ//23ST47dzaFwoo6q9VMXZCyHlaflUceMe27RAlT7LF5czMCxV5SUkwPeo8ephf97bfN707duubmwd69ZpRGQEDujlu2rLmZZVupYfx401vvyBsR13PunFnx4uRJsxLGrFlm5I+j1K5tfrYNG0onTpgaAwsXOu58AADAdZDIo8iqWdP0sEvX7zXz8jKF2T76yAxhnzvXJDzO6v1ev94MT/f2Nsu+ff+9KQx24YLpCezaNecV5O2tf/9UPf307xowIPWaPcJ55eFhktTdu00S5+VlEtHatc3NGWffYHGmn3+WDhwwSXGXLjn/Xvv2pj2FhJjCarfccu2bW9dy8aJZG/3FF81IljJlTG/xl1+apL51azMs/M8/zRBxH5+8nUcyf/YTJpiE2dfXtINbbjFto6AlJ0sPPmhGnlSoYBLq7Ja7tKcKFcxNvPbtTVX7e++Vpkxx/HkBAIBzkcijSHvhBalSJVOoLScGDDBJSmCgKcTWpIm0bp1jY8zKhAnmuWdPKSzMJLcpKaY30FbErHdvM2zaXQUEmJ/D1q1miHVioumVvfFGkzQ6o3ibs9l64++/XypWLHffbdjQ9O7WqmWKCd52m+mhv57YWFOwcNgw8/tQqpSpij9mjLRsmXT+vFSypFnPftMmaflyk+Dbc4WF3r3NTYywMDPd5Oabzc2tghQTY25Q+PtL//ufiaWglCxpzjlwoBmVMWSIucnlClMNAACAY5DIo0irW9cM7f3Pf3L+ndtuM3NR69Uzc9JbtjRDvQvK3r1mKSvJDEmWTGL0449m+8cfS4MHm0T26adNRXh3VqOGSaDmz5cqVzZJaI8e5uaMPYeIu7rLl82yc1LOh9X/W6VKJiFu0cIk4B06ZGzbqammMN306aaAW7VqZpWB+++XJk2Sfv/d3ESqWNH0Tk+ebKrKnzlj5u43bJjPi7yGxo3N+W+91Qxx79TJzLkviBs6U6ZI775rXs+aZYbVFzQvL7NCxPjx5v2bb5o/A5anAwDAPeWi3BTgnjw9zfPRo2a+cOXK6Y+ICPNcpkzGHsRKlcxc7YceMglkv37Stm3Sa6+lH89R3nrL9LTVqWOGE9vcckv668mTzRzi9u2l8uUdG48rsFjMUPK2bU3tgFdfNcONmzc365BXqeLsCB1v4UIpPt60zebN836cMmWkpUul/v2lL74wbfvHH82877VrpdOnM+5vsZgbYrfdZirI33abdMMN+bqUPAsJMSNmhgyRPvxQGjHC3Ej48EPHreqwdKmZ3iKZdnfffY45T05YLNLIkaYN9OtnqvkfPWpqZpQr57y4AACA/ZHIA//v77+zH44bEGCq1dsK3p07J/30k9kWGWl6/v77X7ME1FdfOS6ZP3VKmjHDvN62zQxh/uOPzMm6xZK+hrXNgQPmP/j2HNLsaooVM0uq9eplRins2WPW716xwvTcuzPbsPrevfNfYM3X1/SgV6pkbk7Z2pxkfsZNm6Yn7lFRZji9q/D1NSscNGxoEuwvvjBV3W2jFexp+3ZTMT8lRerb19w4cAU9e5q58/fea244RkWZv9uqVnV2ZAAAwF5I5IH/V726SQAOHDBLSNkex4+bns6r5xz/+aeplG5TsqQpNPftt9Izz5hhrY7w3ntmCLXNQw/lrKdt82YzBaBHDzMM2JGVtF1BlSqmF7l1a5Ns2ZL5evWcHZljHD9uphdIpk3Yg4eH6WG2TV1o0sQk7w0bOq53214sFjO9pFYtU9By0yYpKspLjz0WrDZt7BP/yZOmJkV8vBkB8cEHrnWTrGVLk8R37GhuUkZFmXn0TZs6OzIAAGAPJPLA/6tQIX099qtdumSS+8DA9G1Wq0lo9u+Xzp4184ltJk40lbhffdW+8V26ZHpHbcaPN8Noc2LLFpNwTJ1qqorPmJG7ddwLo9BQM7y+bVszaqFlS5OQ3nyzsyOzvy++ML3CTZuaYn/21LeveRRGLVuaehb33CP9+adFL798i955J1WtWpl20a5d3qZdJCSY3u79+833v/024zQXV1GrlilgeNdd0saNpm7E7Nnm5wEAAAo3N++XA/LP39/8h/jqRL5lS9PLd+aMeWzaZNbELlvWfP7226bX3l5SUkzSYeuNnzQp50m8ZIqfff65GfI/a5bUvbtJRtxd+fJmznRUlPlzat3a9NS7G9uwenv1xruTiAjTM/2f/6SoZMlExcdbNH++6bGvWtUU7IuONvPI4+Ovf7zUVHPD75dfzJSC775z7ToUISHS6tUmmb982czhf/ttZ0cFAADyi0QeyKfSpU3v/EMPmcJSN99s/sN8zz1mTvsbb5hewfx4+WVTTVwylahtxbVyo0cPs0yYj4/pQbznHtPL7+5KlzbLoN1xhxk50b69ee8utm0zN5K8vc0NGmRWvLj0zjtWzZz5vdatS9bLL5vpFl5eZhWI994zvw/lypntL79sfmdTUjIf69VXzY0TT09TTK5mzQK/nFwrUUKaN08aNMjciHjySXPzIinJ2ZEBAIC8IpEH7MjX1xSVqlLFDMfv2NEUwLr5ZlOA6sCBvB3XVqSqePH8rQ3fpYvpQfT3N8PMO3TIOC3AXZUoIS1aZP48Ll829Q0WLHB2VPZh643v1Mm1e4Zdgaen1KhRqp5/3ky7OH3atIPoaNMzn5xsiliOGmV+Z4ODzY2zGTPMUoZz50rPP2+ONXmy1KaNc68nN7y8TH2MN94wc/nfe8/c1IqLc3ZkAAAgL0jkATsrW9YkB8WLm169mjXNf5y/+MIUDhsxwsyrv57k5PTX779vnp94wiSl+dGmjemRDggwvbj2KPyVmmoqxFut6duuXMm6R9NZihUzvZJdu0qJieb5yy+dHVX+pKSYKRMSw+rzIiBAuvtuk5T//bfpnX//fTP/PSDAJPpz5kgPPyyFh6ePeBg6VPrPf5wbe15YLNLTT5u/n0qUMNNObr5Z2rHD2ZEBAIDcIpEHHKBOHTNnXjL/SR47VmrVysxLf+MN0/v37rvZD209d84MBX/3XbN299q1Zkj844/bJ75mzcxQ/QULJD+/vB3jn3/MNfbrZ5Ypq15d2ro1/fNJkzxUvbr0zjumor8r8PExyftDD5kkuGfPjEurFTY//CAdOWLWfu/UydnRFH5VqpgE/dtvTRL/88/S6NGmiKCHh7lR1bGj41alKCidO0vr1pn6AXv3SrfcIi1e7OyoAABAbpDIAw5y331miK5kKsyPH2+GtdesaZKEl17KuJSczenTZn14WxIxfrzZ3ru3qcRuL3XrmlEDkulR/+9/pcOHr/2dLVtMohMZaXoo+/aVPvnEfM/HJ71nLzVVmjPHQ/v3m97LihXNSIR//rFf/Hnl5SXNnGmuIzXV9LZOnuzsqPLGdrPowQdds2p6YeblZW54jRtnKr+fPGlunMybZ4boF3Z16kjr15uaAPHxphjem2+a3wkAAOD6SOQBBxo71gzdTUgwQ7kbNTLJ8NSp5j/NAQFmv9RUsz021lTE37jRzHeeOTO9p+yppxwX5zvvmCG3zZubIfKSGRXwv/+ZYmo2J0+a9bL//tv0UN58s/Tss9Ly5aYq/IMPmv0sFumXX5L13nump/7cOTMSoXJlc0Pijz8cdy054eFh5gjHxJj3jz8uvf66c2PKrQsXTM+xxLD6glC2rPnd9PFxdiT2ExhofncfecT8HTR8uDRgQNFY0QIAgMKORB5wIA8PU4ysZk0zBLprVzM897HHMiZfc+dK9eubXrJt28ya9j/+aArSpaaaYdO1ajkuzvvuMwn3wYMmmW/a1CQud9+dcej5rbeaitcLF5oiWb/9Zqp4t25tCuhdzd/fVMneudMM4W/Rwsz7//xz11j+ymIxoxBGjzbvn33WjKAoLD2S8+ZJFy+aaRq33OLsaFBY+fhI06aZ30kPD3PzsFUr6fhxZ0cGAACuhUQecLCAAGn+fLPm9Nq1Wc9z37LFJJanT5t5qz/9ZJbCmjnTfP70046NMTzc3DioW9eMCli/3txwiIyUgoLS9ytWTHrrLTPHtlSpnB3bw8PcEFi92hT/69lTGjYs/fPt280IBWcshWexmKHTtt74l182Ix8KQzJvG1bfp4+5DiCvLBZTSPP779P/nmrSRNq82dmRAQCA7JDIAwUgMlKaPdv8h3naNDM8/WovvWTWAn/pJemXX0zRrffeM5XfmzQx81gdLSTEJNvjxpl574cOSbt2mZ5qe2nc2PTI16+fvm3CBNNzf8MNpkc8NtZ+58upZ55Jnyf/1ltm/vzVFfhdzZEj0sqV5nXv3s6NBe6jbVszyiYy0tS9aNYsffoGAABwLSTyQAHp2DG9cN2QIaaY3dUaNJBeeMEMq790KT2xHD684Hpcy5Y1Q8379DG99AXh5pvNKITTp02PeKVKZp7u7t0Fc36b6Gjp44/NCIJp00whv6uXAHQln39uRg00b27qDgD2cuONprhf27bm76GuXc0NxsIwSgUAgKKERB4oQCNGSN26mQSxa9fsq8TPnGkS28qVzfx1dzZokCme9/XXUlSUWeP9449NvYBXXinYWPr1MyMnvLykzz4zxfuyWyLQWVJTMw6rB+ytTBlp0SKz4oRkbu716OGc6S8AACBrJPJAAbJYTPG4+vWlEyeke+/NvARdSoo0caJ5HRNjkkp35+Ul3X+/mZu7dq3Uvr1JoJ3RI969uxlO7OsrffON6yXzmzdLf/1l4rv/fmdHA3fl5SVNmmRGp3h5SXPmmCk+rrCEJAAAIJEHClzx4qb4XblyZpm5xx7LOGx1/nxp714zzL1/f2dF6TxRUWbJvYULM87P37nT3PwoCJ07mz8HX1+T1LtSMm/rje/SRSpd2qmhoAgYONDUYyhf3vx91aSJmUcPAACci0QecIKICOmrryRPT7M83aRJZntqqin+JkmDB5ukvyiyWEwy7etr3icnm6G9N95oeggLohBd+/aul8wnJ5uh/xLD6lFwbr/drGRRp44pRtmihann8c03ZnRIYqKzIwQAoOghkQecpFUr6c03zevhw6UVK0zF+t9+M8njkCHOjc+VnDhhkvuzZ80IhubNpa1bHX9eV0vmly0zP4vAQFOM7HqSkkyydbU//zQ/u0OHpHPnXLs6P1xH5cpm2kvnzlJCgqlfcf/9Jrn39zc32e65x4yi+eQTk/jHxzs7agAA3FcRmH0LuK4nnpD++MP8x7d7d6lmTbO9Tx8pONi5sbmSChVMYjB5slmibu1a6aabTA2B0aMdO3LBlsx36ZKezH/5peTt7bhzZsc2rL5nz5ydf8wYU7Ssa9f0bd26ZVwRwGKRSpY064fXqiUtWZL+2euvS2fOmCTtoYeKRr0GZK9kSfO78NFHZtWNHTvM48IF06Z275YWLMj4nQoVzN9rtkeNGuY5JKTgVuMAAMAd8d8ywIksFmnqVPOf4fXrTY+8JD31lHPjckVeXtKTT5pewCeekObNk954wxTh+uknxy6XZ0vm77nHJPM9ekhffFGwyfy5c+lJUk6G1f/xh/n5BAVJu3aZZFwytRmCgszohsREM50jPt48ypTJeIwZM9KT/qVLzTQQZ9zAgOvw8DDz5gcONO9TU6UjR0wNC1tiv2OHeR8bKx09ah4rV2Y8TlCQNGUKBRsBAMgrEnnAyfz8THLYqJF0/Lh0993pSRcyq1jR/Lz+9z8z/SA8XAoLc/x5O3RIT+a/+abgk/m5c6UrV0yvecOG1943KUkaMMCsgHDbbRnb09q16a+vXDE3CGyPf/eQPvqotG+fNH26uWGSmGhGI/j42O+6ULhZLOZ3smJFqXXrjJ+dOWMS+n8n+fv3myki3btLH35YNIt6AgCQXyTygAsIC5O+/94UvRszxtnRFA6dO0t33GGSBY//r/Zx/rxJrh9+2BQStDdnJvNXrx1/vSHJ//2vWaaubFnp3Xez38/Pzzyym8ZhGxnSoYMZnj9vnnn++mvzPeBaypQxq1BERWXcfuWK9PjjJokfMMAMzX/8cefECABAYUWxO8BFNGxo5spXqeLsSAqPEiUyDqkfM8YUw7vlFmnTJsecs0MHk9D6+KQn844ugHfggPTjjyaB79Xr2vvu3CmNG2deT5pkn1oLd91llgP085O++85cM5BXfn5m9Ylhw8z7J56Qxo93bkwAABQ2JPIA3EaNGqZo2++/m/Wux441S7bZW8eOBZvMf/aZeW7Vygxhzo7VKj3yiKkq3r691Lu3/WJo184UzitTxtwsAfLDYjGrdowebd4//7w0cqSZcw8AAK6PRB6A23j0UdMj3b27SWrHjTPJ7+HD9j9XQSXzqakZh9Vfy5Ej0rFjZqTC1Kn2rwreqpUZHdC+vX2Pi6LJYjG/oxMmmPevvWaG2LMkIgAA1+cSifyUKVMUEREhPz8/NW3aVOvXr7/m/mfPnlV0dLRCQ0Pl6+uryMhILV68OO3z999/X/Xq1VNAQIACAgIUFRWl77//3tGXAcAFhISYgmyff26Wy/rpJ6l+fWn1avufq2NHU3jPlsz37Gn/ZH79eunvv81a3ffdd+19w8PNGvHffy9VqmTfOGwCAtJf//23mTN/7pxjzoWiYfjw9BtPU6aYefOOGEkDAIA7cXoiP2fOHMXExGjMmDHatGmT6tevr3bt2unEiRNZ7p+YmKg2bdrowIEDmjt3rnbt2qXp06cr7Kqy1RUrVtRrr72mjRs36vfff1erVq3UpUsX/fXXXwV1WQCcrGdPswRbkyYmQahWzTHn6dQpPZmfO9f+ybytN/6++0xP+/X4+5tK9Y5mtZok/ttvpTZtTNFBIK8ee8y0dU9PUyvkwQfNKgkAACBrTk/kJ06cqIEDB6p///6qVauWpk6dKn9/f82YMSPL/WfMmKG4uDjNnz9fzZo1U0REhFq0aKH69eun7dO5c2d17NhR1atXV2RkpF555RWVKFFCv/76a0FdFgAXULWq9PPPpjf+6rnlx4/b9zyOSuZty71J1x5W/8UX0jvvFOyQZA8Pk3CVKydt2CDdead0+nTBnR/up3dvsyKCbYRLly7SpUvOjgoAANfk1OXnEhMTtXHjRo0cOTJtm4eHh1q3bq1169Zl+Z2FCxcqKipK0dHRWrBggQIDA9WzZ0+NGDFCnlmsN5WSkqKvv/5aFy9eVNS/18D5fwkJCUpISEh7Hx8fL0lKSkpSkqPLUeeDLTZXjhFFj6u1S4vFFMGzhbNwoUUPPeSp//7XqkcesdptHnnbttJXX1nUrZun5s61yGq1ataslHwtTbdwoUVxcV6qUCFVzZsnZ3lz4PhxKTraS2fOWOTvn6y+fQuuWlidOtKyZVL79l764w+LWrZM1ZIlyQoKKrAQcszV2iWydtdd0rx5Ft1/v6eWLLGofXur5s1LyTClw53QLuGKaJdwNUWpTebmGi2pqc6rEXv06FGFhYVp7dq1GZLsZ555RmvWrNFvv/2W6Ts1atTQgQMH1KtXLw0ePFh79uzR4MGD9cQTT2jMVQtwb926VVFRUbpy5YpKlCih2bNnq2PHjlnGMXbsWI2zrdd0ldmzZ8vf398OVwrAVbz11k1as8asWRcVdVTR0ZtVooT9/mHYsCFYr7/eRMnJnoqKOqr27Q8oKcmilBQPJSd7KDnZ8v/PHv+/Lf19+sNs27atvA4dCtA99/ytfv22Z3m+N95orLVrw1Slylm98caP8vIq+L/SDx8uqdGjb9WZM34KD4/XuHFrVbZswvW/CGRj+/ayevnlW3TpkreqVz+j0aPXqWRJ9/8PHACgaLt06ZJ69uypc+fOKeA6d7ELXSIfGRmpK1euaP/+/Wk98BMnTtSECRN07NixtP0SExN16NAhnTt3TnPnztWHH36oNWvWqFatWpmOmVWPfHh4uE6dOnXdH6AzJSUlafny5WrTpo2889PtB9iRq7dLq1V65x0PPf+8h5KSLAoPT9Wnn6aoWTP7/VW4aJFF3bt7KjHRPt39mzYlqU6dzNvnz7eoWzcveXqmau3aZDVsaJfT5cnu3VK7dl46csSi7t3NaARX4urtEplt2iR16uSl06ctql07Vd9/n6yQEGdHZV+0S7gi2iVcTVFqk/Hx8SpfvnyOEnmnDq0vX768PD09dfxfE1aPHz+ukGz+tQ4NDZW3t3eGYfQ1a9ZUbGysEhMT5ePjI0ny8fFRtf+vbtWoUSNt2LBBb7/9tj744INMx/T19ZWvr2+m7d7e3oWisRSWOFG0uHK7fPpp6Y47zJJxe/ZYdOedXhozxqxlncUMnVy75x5pwQJpzBjp8mXJ29vM+/Xxyf51dp/Vqyc1bJj553jmjPTEE+b1iBEW3Xyzc3/WtWtLa9ZITz0lvf++h7y9nV6CJUuu3C6RUdOmpk21bi399ZdFd97prRUrpBtucHZk9ke7hCuiXcLVFIU2mZvrc2oi7+Pjo0aNGmnlypW65557JElWq1UrV67UkCFDsvxOs2bNNHv2bFmtVnl4mP8o7t69W6GhoWlJfFasVmuGXncARVvjxqbHLzpamjXLJN3NmpmibfbQvr1j11sfPlyKjZVuvFEaNcpx58mNqlWl+fMzbjt/3iwDCORF7dpmCck77zTLHTZvLq1c6bhVKBwlMVE6elT6559/Pzx1/Hgj/e9/nipTxizvmNWjVKn018WLK9+1PVJSzBJ/SUnm2fawbc/La9v4ztTUjI+cbMtqn6we19vPJqvX1/v836+z4rwxrAUrOdlDO3ZU1a5dHna5uQ3kV0qKfdqkn5/5f5+7cGoiL0kxMTHq27evGjdurJtvvlmTJk3SxYsX1b9/f0lSnz59FBYWpldffVWSNGjQIE2ePFlDhw7V448/rr///lvjx4/XE7auKUkjR45Uhw4ddMMNN+j8+fOaPXu2Vq9eraVLlzrlGgG4ppIlzZJXbdpIf/5pvyTe0fbvN3FbLNJHH5l/mFzRu+9KEyaYxKt6dWdHg8KqWjWz+kTr1mYKR/Pm0vLlynK6iTNcviwdOZJVkp7+yH6lDA9JFfXzzzk/n4eH+bvr6sTelkzbEvPrPReVhBR55SnJRX7BAEn2apPlypHI21X37t118uRJjR49WrGxsWrQoIGWLFmi4OBgSdKhQ4fSet4lKTw8XEuXLtWwYcNUr149hYWFaejQoRoxYkTaPidOnFCfPn107NgxlSpVSvXq1dPSpUvVpk2bAr8+AK7voYfMw+bYMWnSJGnsWKlYMWdFlb3Klc2Sbz/8YEYRuKIrV6SpU6XDh6UWLaRXXzVJxMWL0oULZvpB7dpm37VrzecXLpjPbfvYXn/wQfryeytWSO3amakHXl7m+erH6NHSgAFm361bpUcfNft5eXnqypUmWrnSQxERZnj2TTdJVao446eD3AoPl3780dx027rVtKlZs6RKlcyNLD8/ydc3/dnLK/e91larFB8vxcXl/HHiRM6XXfT1NctgXv0IDk7RX39tV8WKtXTxoqfi45Xl49w585ySYuI8d8487M38rpgpRrl9bbGkP6SM73Py2bW+k5v9bP7955/dZ9fa71rbrrW9sLNarTpy5IjCwsIy/B8ccBZ7tckSJewYlAtweiIvSUOGDMl2KP3q1aszbYuKirrmmvAfffSRvUIDUMSkppqkccUK81i6VCpf3tlRZdaggXm4Kj8/c6OhdWuTePXrl/HzSpXSE/lTp6Tvvsv+WBcupL9OSjKJTEKCefzbxYvpr8+ckdL/qfCQVEFX/9Px8sumLoJkhm0/+qhJ8G+4wSSOV79meoDzBQdLq1dLHTpI69dLnTplv6+HR8bk/t+Jvp+fufFz/nx6Un7mjGlbeeHvnzlJ//ejfPnMiV9SklWLF+9Tx4415O197fGiqamm9//fSf6lS+kJte3m1tXP2b2+epuXl/mZuWtiitxJSkrR4sWb1LFjiMvWO0HRQpvMmksk8gDgKiwWM/9882Yzh75VK5PQu8La6Js2mf9w16vn7EhyJijIJPNPPSUdOmSGAJcoYZ4rV07fr2FD6cMPzfar97G9vvpGSqtWZq5xUlLmR3JyxkJotWqZooNJSdLly8n66aftKlmyto4c8dShQ1LNmun77t1rksTsvP669Mwz5rVtWDJJT8ErW9b8Pj76qOmhT0gwoz8SEsyfv43VahLcS5dyfw5/f3OenDzKlzdJeunSjm8PFouJzd9fble9HwCQeyTyAPAv7dqZJKFVK9ObfMcdZp63M//znJAg9e5teo6/+kq6917nxZIb5cpJM2dee5/wcOnhh3N2PF9fKTQ0Z/uWLy/dfbd5nZSUqpIl96tjx5pZ9nzWry99/rm54XD14/Bh6ezZjH/2a9dK/ftLDz4ode+ePrIABaNkSemLLzJvT0nJmNhf/ZzVtsREc6yrE/MyZVy35gQAAFcjkQeALNSsaZa+atVK2r5datlSWrVKqlDBOfG88oq0Y4fp5W7RwjkxuLPQUKlnz6w/i483IyFsvvrK3FB56SXzqFPHJPTdu1PUz5k8PdN7rAEAcHdMMgCAbERGmmQ+PFzatUvq29c5cfz5pykGJ0lTppieQxScgICMyeHLL5ve+7vvNvOMt20zSwBGRpplDY8dc16sAACgaCCRB4BrqFrVJPO3326qpxe05GQz7Dw52Qyn79q14GNARiVLmt77BQvMsmIzZpjpGJ6e5v3/L7oiydQIILEHAAD2RiIPANdRubIphHb1UmWJiQVz7rfekjZuNMW0pkyhwJqrKVPGzJdfssQk7F99Zap/S+bmS/fuUliYqbPwwQemQj8AAEB+kcgDQA5cnUAvXGgqou/Z49hz7t9v1kWXpIkTc17kDc4RGChFRaW/j42VqlUzVe5Xr5b+8x/TW3/77dKECaZSPgAAQF6QyANALqSkmPnQe/eaAni7dzvuXDfcYObGd+mSeR12uL6KFU2F+/37zfJ1DRuaZdF++sksZTdjRvq+yckZl08DAAC4FhJ5AMgFT09p2TLTI3/kiEnmd+60z7FTU82xv/46/VxPPinNm8eQ+sIsIsIk7ps2SQcOSO++K7VpY27Q2CxebHrr+/Qxf/7x8c6KFgAAFAYk8gCQS8HBZqh03bpmXnSLFtJff+X9eKmpJpGLijJF0x5/XLp8Of1zknj3UamSNGSIuWFz883p25cvl+LipFmzpG7dpPLlTVuYPFk6eNB58QIAANdEIg8AeRAYaNaVb9BAOnHC9Mxv2ZK7Y6Smmvn2TZpInTpJv/0mFStmKqIXVDE9uIa33pJ+/FEaPtwsY5eUZJL9xx83Pfr79zs7QgAA4EpI5AEgj8qXl1aulBo1MtXIZ87M+XfXrZNuuskMr9640axTPny4SdgmTpRKlXJY2HBBXl5S8+amCN6uXWa6xoQJZlutWmblBJv5880QfQAAUHR5OTsAACjMypaVVqyQ3ntPevbZnH/P31/avFkqUcIMtY6JMb38gCTdeKN5DB+ecXTGjh1Sjx6Sj49pc716OS9GAADgPPTIA0A+lS4tPfdcxvXDd+xI/zwlRfryS2n8+PRt9eub+dAHDpjK9CTxyI6PT/prPz9T/T4+Xurd2yTyZ88WXCyHD0tDh0qTJlFlHwAAZyKRBwA7SkkxlcebNjXLjH3+uVSnjulFHTs2Y+Gy3r2lcuWcFioKocqVzVz6sWPNqgazZ5ubQj/95NjzXrhgll2MjJTeeUcaNsxU3rdaHXteAACQNRJ5ALCjhARTyf78een2202yvnOnVKaMNHq0eQbyw8tLGjNG+vlnqUoV6dAhU2xxzBj7nyslxax3X7269PLL0pUr5iZVQIB0113po1AAAEDB4p9gALAjf39p0SKpdWvzvlw5M6T+wAHphRdMAgTYwy23mDoL/fubnnFHJNWTJkkPPyzFxkpVq0rffGMKNe7YIT35ZPp+W7aYGwoAAKBgkMgDgJ3ZkvkVK0wV+pEjSeDhGCVLmh7zpUul559P337mjFneMC9SUtJfP/KI6Y1/803pr7+k++6TLBapQgUztF+SLl2SHnhAqlvXrNyQ1/MCAICcI5EHAAfw8ZHuvNMkWoCjtW1rhtxLZnrHHXeYpPvUqZwf4/RpU8iuTZv0ZLxUKdP7HhMj+fpm/b2zZ83Ik/h4MzrgnntMDz4AAHAcEnkAANzIunXS9u1mvfl69aRly669f2Ki9NZbUrVqppDdDz9kLJ5n63nPToUKZv/XXjM3sBYuNAUe587N96UAAIBskMgDAOBGWraUfvtNqlnTFF5s187MZ79yJeN+qanSvHlS7dqmx/3sWZP4r1hhCjXmhqenNGKE9Pvvpor+6dNmuH2vXtLly3a6MAAAkIZEHgAAN9OwoUmqBw82799+W2rSRNq61bw/cSJ9+P2ePVJwsPThh9KmTWZKSF7VrSutX2/m63t4mITezy//1wMAADIikQcAwA35+0tTpkjffScFBUnbtpn13yUzpz0uziTZL7wg/f23qU5/vWH0OeHjY5aqW7tW+ugjUxxPMksynj+f/+MDAAASeQAA3FqnTmZ5uO7dpenTzTZPT+nTT6Vdu6SXXnJMUcamTaWwsPT3w4aZYfdXz78HAAB5QyIPAICbCw6WvvxSqlw5fVuDBtINNxTM+c+eTV+OsUULafhw6cKFgjk3AADuiEQeAAA4VOnSZlTAww+bIntvvilVqWKeL11ydnQAABQ+JPIAAMDhAgJMQb3//U+qWlU6edL0zFepIm3Y4OzoAAAoXEjkAQBAgbnrLmnnTmnGDCkiQrJazVJ5AAAg50jkAQBAgfLykvr3N8X2Vq6USpQw21NTpS5dpGnTpMRE58YIAIArI5EHAABO4eNj1p63WbjQPB57TLrxRunjj6XkZOfFBwCAqyKRBwAALqFdO+ntt02V/QMHpAEDzLD7WbOklBRnRwcAgOsgkQcAAC7Bz0964glp3z7pv/+VypeX9uyR+vSRateWjh51doQAALgGEnkAAOBS/P2lp54y686/+qpUtqxUvLgUGursyAAAcA0k8gAAwCWVKCE9+6xJ6GfPliwWs/38ealZM+n556VNm0yRPAAAihISeQAA4NICAkzxO5v33pPWrpXGj5caNZIqV5ZiYqRffjHL2QEA4O5I5AEAQKEyaJD0+edS165mGP7Bg9Jbb0m33SaFhUk//+zsCAEAcCwSeQAAUKgEBEg9e0pz50onT0rz5kkPPSSVKiWdOCFFRqbvu3y5tGCBdPmy8+IFAMDevJwdAAAAQF75+0v33GMeiYnSH39IQUHpn48fL61ebYrldewo3XefeQ4IcFLAAIACl5qaXmfFXdAjDwAA3IKPj9S0afr71FQzh/6GG6SLF6Wvv5Z69JACA6W77jIF9AAA7m3OHKl7d+ncOWdHYl8k8gAAwC1ZLGY9+gMHpA0bpJEjzbD7xERp0SJp1qyM+ycmOiVMAICD/P23NHCguZH7wQfOjsa+SOQBAIBbs1ikxo3NMPudO6Vt26SxY6XBg9P3OXzY9NT36SMtXiwlJTktXACAnSxdapYsvf12s7qJO2GOPAAAKDIsFql2bfO42sKFUny86aWfNUsqW1a6/34zFL95c8nT0znxAgDybsgQKSJCuukmycvNMl965AEAQJE3aJBZh37IEFMsLy5OmjZNuuMOKTxc+v13Z0eY2aVL0q5dzo4CAFzbXXdJFSo4Owr7I5EHAABFnoeHdOut0rvvSkeOmGXrHn5YKl1aOnNGuvHG9H1//FHautVpoSolRfrwQ6laNalGDenZZ01hPwCAtHu31K6ddOiQsyNxLDcbYAAAAJA/Xl5S69bmMWWKtGWLVLJk+udPPmmWuatXT3rqKTP83tvb8XGlpkrffy8984z011/p2zdvlpKTCyYGAFlLTZX27pXWrzePceOkUqXMZ199ZW6+lS2b/ihXLv31rbea97bjuNsyaQXp8mXpgQfM39tPPil9+62zI3Icl+iRnzJliiIiIuTn56emTZtq/fr119z/7Nmzio6OVmhoqHx9fRUZGanFixenff7qq6+qSZMmKlmypIKCgnTPPfdoF2PPAABALvn6Sk2apL+/csUsZ+fjY/6j2Lev6Rl/5x2zxJ2jbNsmtWkjdepkkvgyZaSJE02C8O23JPFAQYuLM6tfjBkjtW8vlS8vVa8u9eolvf22tHFj+r47d5pRPnPmSO+/L73yiim81q+fdPfd0vbt6ftOmSIFBEiVK5vlMx9/3P2WTXOkJ580fzcHBZmfpTtzeo/8nDlzFBMTo6lTp6pp06aaNGmS2rVrp127dikoKCjT/omJiWrTpo2CgoI0d+5chYWF6eDBgypdunTaPmvWrFF0dLSaNGmi5ORkPffcc2rbtq22b9+u4sWLF+DVAQAAd+LnJ82fb4bbf/CBNGmSGb45dKj04ovmfe/e9j/v8ePSypXmBsITT0jPPWeS+aulpkqzZ0vdupHYF0W7dkkzZ5q2Urq0eZQqZap1N2xo9klISP+8RAkzpQTXd/GitGmTSdRDQsy2L7+UoqMz7ufjY37WTZuaRNKma1eTmMfFZXycPm2ebceUzPvz583jwAFz3u++kz7/3PTcI3uzZ5vaJhaL9NlnUmiosyNyLKcn8hMnTtTAgQPVv39/SdLUqVO1aNEizZgxQ88++2ym/WfMmKG4uDitXbtW3v//r1RERESGfZYsWZLh/cyZMxUUFKSNGzfq9ttvd8yFAACAIqNMGTM3/cknpU8+kd54Q9q3L314bH6dO2d69Fq1Mu/vvNOc44EHTAXmrIwda24mfPON9MUXZjQB3N/27aZI4w8/ZP35hAnpifyWLdLNN5vXHh4m0S9VKj3xf+QRcyOoqLNaTe/5qlVmmPy2bWbb9OnmZySZZL1mTfPztD3q1TPJ/L9ltVJGdoYNkx580CT0hw5JI0aYhP7226VRo6TRoxl6n5Vdu6RHHzWvX3jBjGByd05N5BMTE7Vx40aNHDkybZuHh4dat26tdevWZfmdhQsXKioqStHR0VqwYIECAwPVs2dPjRgxQp7ZrA1z7v/Ho5QtWzbLzxMSEpSQkJD2Pj4+XpKUlJSkJBdeSNYWmyvHiKKHdglXRLuEo3h6SgMGmPXnlyyx6M47U9PWoH/tNQ/t3WvRU0+lqEaNzN/Nql0mJkrTpnnolVc8lJAg7diRrOBg89mTT9q+l3UsDRta5OvrqXnzLOrSxao5c1Lk72+nC4VLSUxMTxgDAqSffvKSh4fUvn2qbrklVefO6f8fFtWoYVVSkqmGeO6cRT4+nkpMtMhqNSNLzpxJP2779ilp7fHIkSTNmOGhAQOsCgsr6Ct0nv37pYEDPfXjjxmHK4SFperyZauSkqySTNL+55+Zv5/ff2b8/EzvvW1ofatW0tChnpo920N79liVnJySvxMUQtf7N/zSJen++7108aJFLVpY9dxzKfn+c3CW3Pw/xZKa6rw6p0ePHlVYWJjWrl2rqKiotO3PPPOM1qxZo99++y3Td2rUqKEDBw6oV69eGjx4sPbs2aPBgwfriSee0JgxYzLtb7Vadffdd+vs2bP6+eef/6+9Ow+PqkrzOP6rbEUISSAsIciaZleBZgsBHGSRgA4tiq2tDAZGxCXwgAytNgqEFsF2o8VRtNWGVgQUZ6CVERDDJkuQZUCQRbHpAU1CACUJiSEhdeaP06lKSUCgQ26KfD/Pc5+k7j2peqvyplLvPcstN47U1FRNnz79nP0LFy5UTf4DAgCAS3TmTJBGj05SXl6YXC6jhIRM3X7712rd+lS57Y2RtmyJ0zvvtFdmZi1JUuPGeZo4cbvi43Mv+nF3766vmTO768yZEF133XE98cTnCg8/WxFPqVwlJdL339dQ3bqFDNO+ws6edWnbtoZataq5Skpceuqpzd5jGzc2Ups2P6h+/R8v6r6KioKUnx96zta8eY6aNDktSfrv/26pt9++VkFBHnXrdkxJSX9Xp07ZV/XvOS8vVA88cJMKCkJVo8ZZDRp0WG3bfq9WrU6pbt1CR2PbvDlOnTodV82a9u+5qChIYWEeR2OqKk6cqKEZM3rohx/cmj17nWJizvz8D1VRBQUFuueee5STk6OoqKgLtg24Qr5169YqLCzU4cOHvT3wL774op577jllZmae0/6hhx7SihUrtHHjRjVu3LjcOMrrkW/SpIlOnDjxsy+gk4qLi7V69WrddNNN3mkGgNPIS1RF5CWckJ7u0nPPBemjj3yVT9++Hv32tx7172909qzNy8jIJD3xRJjS02272FijqVM9GjXKo5DLGDu5aZNLv/pVsPLyXEpI8Oijj0pUZimhCmGM9D//49KUKcH68kuXWrY0Sknx6N57PX4r/FcnOTn26gExMRU79PnIEemtt4I0f36QMjPtHbtcRocOnVWTJhX3OKVK3y9LSpI0e3aYNm705W+LFkb33edRcrLHO1LkajN5cpC2bnXpjTdKFB/vdDTlM0YaOjRY0dHSyy+XeFfHv1pdzP/wH3+UDh2Srr++koOrYLm5uapXr95FFfKODq2vV6+egoODdezYMb/9x44dU8Oyqz6UERcXp9DQUL9h9O3atVNWVpaKiooUVmZiytixY7V8+XJt2LDhvEW8JLndbrnLmUgWGhoaEB/4AiVOVC/kJaoi8hKV6YYb7LZvn53f/u670tq1QVq7NkipqXbBulOn3LrzTreKilyqWVOaNEmaNMmlyMhgSeVPGfw5N95oF8ZLSpK2bg3SzTcHacsWXdZJgfJ8+62dw7tpk2/foUMuPfJIsFJTg/X++9LAgRXzWBfi8Ujr10vvvCNt2OBbR6Cyi5ozZ+zaBH/4gx2hEBUlxcf7tl/+Urrnnku/302bpFmz7CUHPf/oeG3QQLrvPun++11q0eLKvpcNGRKs228P0r59dmHHv/xFOnzYpSefDNYf/hCsrCwF/NQNY6S337bz29u1s/tmzrRTZoKq8NCD7dulTz6x+ZaeHqQFC6RevZyO6sr76f/wslNMQkOlzp0dCqwCXcpnFEczNCwsTF26dFFaWpp3n8fjUVpaml8PfVm9evXSoUOH5PH4hpJ89dVXiouL8xbxxhiNHTtWS5cu1Zo1a9SiRYsr+0QAAADOo317u5r4N9/Yee6Rkfba85JUu/YZPfywR6NHS19/ba89XRE92t26SevW2dWwU1IqroiXbDGZkSGFh9sF/44elV59VWrd2vaKdezoa5uba4ulirRvn/S739lF//r1k+bNs6/txx87swiYMfYSgCX/mLqcmyvt2mX3Pf+8vX54WT162LhHj7ZF4+LFdkG1Eyf8X6vDh+3lzTwe2/799+1rPXOmnT9dWdq3t5dTy8iwr3VCgr1kWtkifuFCuzhbIMnMlG691V4CLjnZjqaQbEFYhWt4SVLXrtJnn9k8KF0ILzXV9xwqknNjty+soMCegHnqKd/fXrVjHLZ48WLjdrvN/Pnzzb59+8yYMWNM7dq1TVZWljHGmBEjRpjHH3/c2/7IkSMmMjLSjB071hw8eNAsX77cNGjQwMyYMcPb5qGHHjLR0dFm3bp1JjMz07sVFBRcVEw5OTlGksnJyanYJ1vBioqKzLJly0xRUZHToQBe5CWqIvISVcnp0/ZraV6eOXPl8rIiPsr87W/GTJxoTNk/ny1bjPnuO/92JSXG7N7tv69vX2O6dzdm4UL/n79ckycbY0sLu0VHG3P//ca8844xaWm+dh6PMceO/fOPdz6nTxtTXOy7vWWLMf/1X8YUFBizb58xy5cbM2eOMePH26+lCgr84//pNnOmr+2PPxrz+OPGHDx45Z5HeS7m/fLHH33f79ljY3e7jRkxwphNm+zrX1V5PMYsWmRMTIyNOyzMmFmzjDl71unILl1Ojn3NS/MnMdH+vf4zzp41Zvt2Y55/3ph//VdjmjXzf20++8yYJUuMOXCg8l6z8nLyvvvsc46NNSY7u3LiqAyXUoc6XsgbY8zLL79smjZtasLCwkz37t1Nenq691ifPn1McnKyX/vNmzebhIQE43a7TXx8vHn66afN2TKZJKncbd68eRcVD4U8cPnIS1RF5CWqosrOy6wsYwYONOarry6+/bhxxoSG2g/Mr79+aY/33Xe2uCstMq65xhZMJ09e3M8XFNiCq2y8K1YYExJizK9+ZYuJsgVlWW++aYv8P/3JnmCoSGvWGBMfb8yzz176zxYXG7N5sz3xMH26McnJxtxwg31tJGNatnS+CL7UvPzsM2M6dfI/IdG6tTEPPGDMggUX//uuDNnZxtxxhy/OX/7SnogIdAsX2nwvfU6XmkP79/sK96ioc08wbdvma3vPPb79NWoY07mzMSNHGvPCC8Z88knFnLD7qZ/m5Ntv28d3ufxP4F0NAq6Qr2oo5IHLR16iKiIvURVVdl6WFjANGxqzd+/52+XkGDN1qjEREb4P7DfdZMzOnZf+mMeOGfP739tes9L7Cg835sEHyz+hUFJizNq1xvz7v/sKit/+1ne8uPjne988HmOSknyP16fPxZ+8uJBTp4wZM8a/WK3IX11BQdXoWbycvPR4jNm61ZhRo+zvt2wRuGGDr92hQ7ZodOJkxf79xtSvb2MKCTEmNfXKFJ1O+fvfjbnxRjs65EJKe9xzc337pk07d6TLkCG2ON+xw7/nffp0Y7p1M6ZmzXML/uBgYwoLfW0XLbIn1bZuPf9Jt4tRNif37fM9dmrq5d9nVXUpdaiji90BAACgcvznf0oHD0p79kh9+tjFssouDuXx2LnQTz8tnTxp93XrZhdc69//8h6zQQNpyhTp0Uel996TZs+288dfe03q2VNq1cq227/fLlr37rt2lfZSzZpJjRr5boeESPXrX/gxXS47t/zll6UnnrAL4l1/vZ1D/B//YedAX6qPP5YeeMAu9CdJDz0kPfPM5d3X+YSH2y0QuVx2vnL37vZ3vG6dncO9ZYvNoVKzZ0uvvGJ/h71727ndN9xg11WoyHUcytOypZ1T3qCBXeDualgYraxmzaQ1a/zXiXjnHalpU7vuxtq1vt9LTo5dw+G222y7gQOlnTvtQpk33mh/H8HnWWtz6lS7lZTYdRy++MK+p+zZYxd+LLt++IsvStu22e9DQ6UuXeyifD172u08a5ufV0GB9Otf268DBkhPPnlpP3+1oZAHAACoBmJj7Qf5QYPsh+t+/eyK6KXrCwcF2QL45EmpTRtb0N9+e8UsIOd2S/feK40YYVeYnzdPuusue+zsWVvQnThhb0dH2w/rI0bYYu9yFh4LDrYLCw4dagvwTz6xC+QtXmwXHuzU6eLu5+RJ6ZFHbEEkSb/4hfTWW/ZECMoXHW0Xkbv11nOPFRVJNWpIx49LS5faTbKFZs+etrisyJXw09JsDrnd9kTB0qVS3br+xebVpOzf6pdfSvffb4vrn4qKkrKzfbd79pQ+/PDSHis42J4cadnSvk+UZ+BAmw+7d9vfeXq63V54QWrc2C7eWOr//s/uO98JBEkaPz5YX35pTwAsWHDhttUBhTwAAEA1ERMjffqpdMst0saN9gP8+vW2kJbs5dt27rQreV+JHlKXyxbBZQvhkBBbtB86ZL8OGWKLvYrQvLm0cqX90D9hgu09LCy8+J/PyrLFf1CQLeh///vAv+Sak/70JztSYscOe0Lns8/sZfZycqQDB/xf27597QiIiAjfVrOm/dq0qR0pUuq996S8PP82ixfbky6PPWZHT0j+ozuudk2a2JNlb79tC/d/+Rdfj3unTpVTBM+YYb8aY3vvN22SNm+2W/v2vnbG2N764mJ7VYfSXvuEBN9VPIyREhI8+uCDIC1aZE9MVncU8gAAANVIVJQtbocOtUX9smW+Qr5zZ2eGHL/wwpW7dJzLZU8QJCXZnvkePXzHjh61BU9ZP/7oG+J+7bX20nrXX2+LCvzz3G7f0OrHH7dDtPfs8e8hlqS//c1/mkVZbdv6F/JPPWV7oH/K5bL3b4wzlyZ0UlSU9Je/SHPmSLVqOdt77XJJ8fF2GzHC7itzJXFlZNiRA6dP27/RTz6x+4OC7DD/ESOCFB8vjR5t9OtfS/XqVf5zqIoo5AEAAKqZiAjpo4/sXNeYGKejqZwiq0ED6d/+zXf74EHbM3n33fZ673Xq2CH0kybZKQalc7tHj77ysVVnwcHlT3VYvtz21Ofn262gwPd9VJR/2/797fz3su1q17bTQ0pPUlVX0dFOR1C+slNmrrlG+uEHae9eX6/9pk12uP3//q+dDx8fb9tSxPtQyAMAAFRDNWrYofTVVVqa7QWcN88W7tddZxcLk+yifwsWOBtfdXf99Rff9qWXrlwcqBwhIfaETqdOUkqK3ffdd7ao/8UvPH7z6WFdxvIhAAAAQGB7+GG7TkC7dnZY95o1dtj3zJm2uAfgrGuusQtfXspJneqEHnkAAABUSz172qG7zz9vF8KbPt3OvwaAqo5CHgAAANWW222vNw8AgYSh9QAAAAAABBAKeQAAAAAAAgiFPAAAAAAAAYRCHgAAAACAAEIhDwAAAABAAKGQBwAAAAAggFDIAwAAAAAQQCjkAQAAAAAIIBTyAAAAAAAEEAp5AAAAAAACCIU8AAAAAAABhEIeAAAAAIAAQiEPAAAAAEAAoZAHAAAAACCAUMgDAAAAABBAKOQBAAAAAAggFPIAAAAAAAQQCnkAAAAAAAJIiNMBVEXGGElSbm6uw5FcWHFxsQoKCpSbm6vQ0FCnwwEkkZeomshLVEXkJaoi8hJVTXXKydL6s7QevRAK+XLk5eVJkpo0aeJwJAAAAACA6iQvL0/R0dEXbOMyF1PuVzMej0cZGRmKjIyUy+VyOpzzys3NVZMmTXT06FFFRUU5HQ4gibxE1UReoioiL1EVkZeoaqpTThpjlJeXp0aNGiko6MKz4OmRL0dQUJAaN27sdBgXLSoq6qpPagQe8hJVEXmJqoi8RFVEXqKqqS45+XM98aVY7A4AAAAAgABCIQ8AAAAAQAChkA9gbrdb06ZNk9vtdjoUwIu8RFVEXqIqIi9RFZGXqGrIyfKx2B0AAAAAAAGEHnkAAAAAAAIIhTwAAAAAAAGEQh4AAAAAgABCIQ8AAAAAQAChkA9gr7zyipo3b64aNWooISFBn3/+udMhoRrZsGGDhgwZokaNGsnlcmnZsmV+x40xmjp1quLi4hQeHq4BAwbo66+/diZYVAuzZs1St27dFBkZqQYNGmjo0KE6ePCgX5vCwkKlpKSobt26qlWrloYNG6Zjx445FDGqg7lz56pDhw6KiopSVFSUEhMTtWLFCu9xchJOe+aZZ+RyuTRhwgTvPvISTkhNTZXL5fLb2rZt6z1OXvqjkA9Q7733niZOnKhp06Zp586d6tixo5KSkpSdne10aKgm8vPz1bFjR73yyivlHn/22Wc1Z84cvfbaa9q6dasiIiKUlJSkwsLCSo4U1cX69euVkpKi9PR0rV69WsXFxRo4cKDy8/O9bR555BF99NFHWrJkidavX6+MjAzdfvvtDkaNq13jxo31zDPPaMeOHdq+fbv69eunW2+9VV9++aUkchLO2rZtm15//XV16NDBbz95Cadce+21yszM9G4bN270HiMvf8IgIHXv3t2kpKR4b5eUlJhGjRqZWbNmORgVqitJZunSpd7bHo/HNGzY0Dz33HPefadOnTJut9ssWrTIgQhRHWVnZxtJZv369cYYm4OhoaFmyZIl3jb79+83ksyWLVucChPVUJ06dcybb75JTsJReXl5plWrVmb16tWmT58+Zvz48cYY3ivhnGnTppmOHTuWe4y8PBc98gGoqKhIO3bs0IABA7z7goKCNGDAAG3ZssXByADr8OHDysrK8svR6OhoJSQkkKOoNDk5OZKkmJgYSdKOHTtUXFzsl5dt27ZV06ZNyUtUipKSEi1evFj5+flKTEwkJ+GolJQU3XLLLX75J/FeCWd9/fXXatSokeLj4zV8+HAdOXJEEnlZnhCnA8ClO3HihEpKShQbG+u3PzY2VgcOHHAoKsAnKytLksrN0dJjwJXk8Xg0YcIE9erVS9ddd50km5dhYWGqXbu2X1vyElfanj17lJiYqMLCQtWqVUtLly5V+/bttWvXLnISjli8eLF27typbdu2nXOM90o4JSEhQfPnz1ebNm2UmZmp6dOn64YbbtDevXvJy3JQyAMArjopKSnau3ev39w6wClt2rTRrl27lJOTow8++EDJyclav36902Ghmjp69KjGjx+v1atXq0aNGk6HA3gNHjzY+32HDh2UkJCgZs2a6f3331d4eLiDkVVNDK0PQPXq1VNwcPA5qzQeO3ZMDRs2dCgqwKc0D8lROGHs2LFavny51q5dq8aNG3v3N2zYUEVFRTp16pRfe/ISV1pYWJhatmypLl26aNasWerYsaNeeuklchKO2LFjh7Kzs9W5c2eFhIQoJCRE69ev15w5cxQSEqLY2FjyElVC7dq11bp1ax06dIj3y3JQyAegsLAwdenSRWlpad59Ho9HaWlpSkxMdDAywGrRooUaNmzol6O5ubnaunUrOYorxhijsWPHaunSpVqzZo1atGjhd7xLly4KDQ31y8uDBw/qyJEj5CUqlcfj0ZkzZ8hJOKJ///7as2ePdu3a5d26du2q4cOHe78nL1EVnD59Wt98843i4uJ4vywHQ+sD1MSJE5WcnKyuXbuqe/fu+uMf/6j8/HyNGjXK6dBQTZw+fVqHDh3y3j58+LB27dqlmJgYNW3aVBMmTNCMGTPUqlUrtWjRQlOmTFGjRo00dOhQ54LGVS0lJUULFy7UX//6V0VGRnrnzEVHRys8PFzR0dG67777NHHiRMXExCgqKkrjxo1TYmKievTo4XD0uFr97ne/0+DBg9W0aVPl5eVp4cKFWrdunVatWkVOwhGRkZHetUNKRUREqG7dut795CWcMGnSJA0ZMkTNmjVTRkaGpk2bpuDgYN199928X5aDQj5A3XXXXTp+/LimTp2qrKwsderUSStXrjxncTHgStm+fbv69u3rvT1x4kRJUnJysubPn69HH31U+fn5GjNmjE6dOqXevXtr5cqVzMfDFTN37lxJ0o033ui3f968eRo5cqQkafbs2QoKCtKwYcN05swZJSUl6dVXX63kSFGdZGdn695771VmZqaio6PVoUMHrVq1SjfddJMkchJVE3kJJ3z77be6++67dfLkSdWvX1+9e/dWenq66tevL4m8/CmXMcY4HQQAAAAAALg4zJEHAAAAACCAUMgDAAAAABBAKOQBAAAAAAggFPIAAAAAAAQQCnkAAAAAAAIIhTwAAAAAAAGEQh4AAAAAgABCIQ8AAAAAQAChkAcAAI5zuVxatmyZ02EAABAQKOQBAKjmRo4cKZfLdc42aNAgp0MDAADlCHE6AAAA4LxBgwZp3rx5fvvcbrdD0QAAgAuhRx4AAMjtdqthw4Z+W506dSTZYe9z587V4MGDFR4ervj4eH3wwQd+P79nzx7169dP4eHhqlu3rsaMGaPTp0/7tfnzn/+sa6+9Vm63W3FxcRo7dqzf8RMnTui2225TzZo11apVK3344YdX9kkDABCgKOQBAMDPmjJlioYNG6bdu3dr+PDh+s1vfqP9+/dLkvLz85WUlKQ6depo27ZtWrJkiT799FO/Qn3u3LlKSUnRmDFjtGfPHn344Ydq2bKl32NMnz5dd955p7744gvdfPPNGj58uL7//vtKfZ4AAAQClzHGOB0EAABwzsiRI7VgwQLVqFHDb//kyZM1efJkuVwuPfjgg5o7d673WI8ePdS5c2e9+uqreuONN/TYY4/p6NGjioiIkCR9/PHHGjJkiDIyMhQbG6trrrlGo0aN0owZM8qNweVy6cknn9RTTz0lyZ4cqFWrllasWMFcfQAAfoI58gAAQH379vUr1CUpJibG+31iYqLfscTERO3atUuStH//fnXs2NFbxEtSr1695PF4dPDgQblcLmVkZKh///4XjKFDhw7e7yMiIhQVFaXs7OzLfUoAAFy1KOQBAIAiIiLOGepeUcLDwy+qXWhoqN9tl8slj8dzJUICACCgMUceAAD8rPT09HNut2vXTpLUrl077d69W/n5+d7jmzZtUlBQkNq0aaPIyEg1b95caWlplRozAABXK3rkAQCAzpw5o6ysLL99ISEhqlevniRpyZIl6tq1q3r37q13331Xn3/+ud566y1J0vDhwzVt2jQlJycrNTVVx48f17hx4zRixAjFxsZKklJTU/Xggw+qQYMGGjx4sPLy8rRp0yaNGzeucp8oAABXAQp5AACglStXKi4uzm9fmzZtdODAAUl2RfnFixfr4YcfVlxcnBYtWqT27dtLkmrWrKlVq1Zp/Pjx6tatm2rWrKlhw4bpxRdf9N5XcnKyCgsLNXv2bE2aNEn16tXTHXfcUXlPEACAqwir1gMAgAtyuVxaunSphg4d6nQoAABAzJEHAAAAACCgUMgDAAAAABBAmCMPAAAuiFl4AABULfTIAwAAAAAQQCjkAQAAAAAIIBTyAAAAAAAEEAp5AAAAAAACCIU8AAAAAAABhEIeAAAAAIAAQiEPAAAAAEAAoZAHAAAAACCA/D9wJYTubiKpLQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# log_path = \"logs/LorentzNet/train-result-epoch59.json\"\n",
    "# model_name = \"LorentzNet\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/phi_m/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_phi_m\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/phi_h/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_phi_h\"\n",
    "\n",
    "log_path = \"logs/LieEQGNN/phi_x/train-result-epoch51.json\"\n",
    "model_name = \"LieEQGNN_phi_x\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/phi_e/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_phi_e\"\n",
    "\n",
    "# log_path = \"logs/LieEQGNN/full_quantum/train-result-epoch59.json\"\n",
    "# model_name = \"LieEQGNN_full_quantum\"\n",
    "\n",
    "with open(log_path) as f:\n",
    "\n",
    "    data = json.load(f)\n",
    "    epochs = data[\"epochs\"]\n",
    "    train_loss = data[\"train_loss\"]\n",
    "    val_loss = data[\"val_loss\"]\n",
    "    train_acc = data[\"train_acc\"]\n",
    "    val_acc = data[\"val_acc\"]\n",
    "    \n",
    "    # Plotting Loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='--')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss', color='blue', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"figures/{}_loss.png\".format(model_name), dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plotting Accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(epochs, train_acc, label='Train Accuracy', color='green', linestyle='--')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy', color='green', linestyle='-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(model_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"figures/{}_accuracy.png\".format(model_name), dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# List of log paths and model names\n",
    "\n",
    "log_paths_and_model_names = [\n",
    "\n",
    "    (\"logs/LorentzNet/train-result-epoch59.json\", \"LorentzNet\"),\n",
    "    (\"logs/LieEQGNN/phi_m/train-result-epoch59.json\", \"LieEQGNN_phi_m\"),\n",
    "    (\"logs/LieEQGNN/phi_h/train-result-epoch48.json\", \"LieEQGNN_phi_h\"),\n",
    "    (\"logs/LieEQGNN/phi_x/train-result-epoch59.json\", \"LieEQGNN_phi_x\"),\n",
    "    (\"logs/LieEQGNN/phi_e/train-result-epoch50.json\", \"LieEQGNN_phi_e\"),\n",
    "    (\"logs/LieEQGNN/full_quantum/train-result-epoch18.json\", \"LieEQGNN_full_quantum\")\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for log_path, model_name in log_paths_and_model_names:\n",
    "\n",
    "    with open(log_path) as f:\n",
    "\n",
    "        data = json.load(f)\n",
    "        \n",
    "        epochs = data[\"epochs\"]\n",
    "        train_loss = data[\"train_loss\"]\n",
    "        val_loss = data[\"val_loss\"]\n",
    "        train_acc = data[\"train_acc\"]\n",
    "        val_acc = data[\"val_acc\"]\n",
    "\n",
    "        # Plotting Loss\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epochs, train_loss, label='Train Loss', color='blue', linestyle='--')\n",
    "        plt.plot(epochs, val_loss, label='Validation Loss', color='orange', linestyle='-')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(model_name)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"figures/{}_loss.png\".format(model_name), dpi=300)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "\n",
    "        # Plotting Accuracy\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epochs, train_acc, label='Train Accuracy', color='green', linestyle='--')\n",
    "        plt.plot(epochs, val_acc, label='Validation Accuracy', color='orange', linestyle='-')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(model_name)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(\"figures/{}_accuracy.png\".format(model_name), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
