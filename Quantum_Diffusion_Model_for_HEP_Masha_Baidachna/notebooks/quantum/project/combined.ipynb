{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\realc\\AppData\\Local\\Temp\\ipykernel_31004\\1040041042.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoded_data_channel_1 = torch.load(\"../../../data/Q1_16x16_1k_encoded.pt\")\n",
      "C:\\Users\\realc\\AppData\\Local\\Temp\\ipykernel_31004\\1040041042.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoded_data_channel_2 = torch.load(\"../../../data/Q2_16x16_1k_encoded.pt\")\n",
      "C:\\Users\\realc\\AppData\\Local\\Temp\\ipykernel_31004\\1040041042.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoded_data_channel_3 = torch.load(\"../../../data/Q3_16x16_1k_encoded.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8, 8, 4, 3)\n",
      "torch.Size([100, 8, 8, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\realc\\AppData\\Local\\Temp\\ipykernel_31004\\1040041042.py:25: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\Copy.cpp:305.)\n",
      "  scrambled_states = torch.tensor(scrambled_states, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from haar_noising_script import apply_haar_scrambling\n",
    "\n",
    "filename_channel_1 = \"../../../data/QG1_normalized_16x16_100k\"\n",
    "filename_channel_2 = \"../../../data/QG2_normalized_16x16_100k\"\n",
    "filename_channel_3 = \"../../../data/QG3_normalized_16x16_100k\"\n",
    "\n",
    "data_X_channel_1 = np.array(h5py.File(filename_channel_1, \"r\")['X'])\n",
    "data_X_channel_2 = np.array(h5py.File(filename_channel_2, \"r\")['X'])\n",
    "data_X_channel_3 = np.array(h5py.File(filename_channel_3, \"r\")['X'])\n",
    "\n",
    "data_X = np.stack([data_X_channel_1, data_X_channel_2, data_X_channel_3], axis=-1)\n",
    "\n",
    "encoded_data_channel_1 = torch.load(\"../../../data/Q1_16x16_1k_encoded.pt\")\n",
    "encoded_data_channel_2 = torch.load(\"../../../data/Q2_16x16_1k_encoded.pt\")\n",
    "encoded_data_channel_3 = torch.load(\"../../../data/Q3_16x16_1k_encoded.pt\")\n",
    "\n",
    "encoded_data = np.stack([encoded_data_channel_1, encoded_data_channel_2, encoded_data_channel_3], axis=-1)\n",
    "print(encoded_data.shape)\n",
    "\n",
    "num_samples = 100\n",
    "scrambled_states = apply_haar_scrambling(np.array(encoded_data), num_samples, seed=42)\n",
    "scrambled_states = torch.tensor(scrambled_states, dtype=torch.float32)\n",
    "print(scrambled_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train scrambled states shape: torch.Size([80, 256])\n",
      "Input shape to model: torch.Size([80, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (80x256 and 768x4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 112\u001b[0m\n\u001b[0;32m    110\u001b[0m inputs \u001b[38;5;241m=\u001b[39m train_scrambled_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(train_scrambled_states), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain scrambled states shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, train_encoded_data\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(train_encoded_data), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    115\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 43\u001b[0m, in \u001b[0;36mQuantumDiffusionModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape to model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# First linear layer\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape after fc1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantum_layer(x)  \u001b[38;5;66;03m# Quantum layer\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\realc\\OneDrive\\Documents\\GSOC\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (80x256 and 768x4)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.linalg\n",
    "import pennylane as qml\n",
    "\n",
    "# Assuming `encoded_data` and `scrambled_states` are already loaded and processed\n",
    "train_encoded_data, val_encoded_data, train_scrambled_states, val_scrambled_states = train_test_split(\n",
    "    encoded_data[:num_samples], scrambled_states, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "n_qubits = 4\n",
    "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev, interface='torch')\n",
    "def quantum_circuit(inputs, weights):\n",
    "    qml.templates.AngleEmbedding(inputs, wires=range(n_qubits))\n",
    "    qml.templates.StronglyEntanglingLayers(weights, wires=range(n_qubits))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "class QuantumLayer(nn.Module):\n",
    "    def __init__(self, n_qubits, n_layers):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        weight_shapes = {\"weights\": (n_layers, n_qubits, 3)}\n",
    "        self.qlayer = qml.qnn.TorchLayer(quantum_circuit, weight_shapes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.qlayer(x)\n",
    "\n",
    "class QuantumDiffusionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_qubits, n_layers):\n",
    "        super(QuantumDiffusionModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, n_qubits)  # Input dimension needs to be correct\n",
    "        self.quantum_layer = QuantumLayer(n_qubits, n_layers)\n",
    "        self.fc2 = nn.Linear(n_qubits, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Input shape to model: {x.shape}\")\n",
    "        x = torch.relu(self.fc1(x))  # First linear layer\n",
    "        print(f\"Shape after fc1: {x.shape}\")\n",
    "        x = self.quantum_layer(x)  # Quantum layer\n",
    "        print(f\"Shape after quantum layer: {x.shape}\")\n",
    "        x = torch.relu(self.fc2(x))  # Second linear layer\n",
    "        print(f\"Shape after fc2: {x.shape}\")\n",
    "        x = self.dropout(x)  # Dropout\n",
    "        x = self.fc3(x)  # Final layer\n",
    "        print(f\"Shape after fc3 (output): {x.shape}\")\n",
    "        return x\n",
    "\n",
    "def decode(encoded_data):\n",
    "    num_samples, encoded_height, encoded_width, num_channels, _ = encoded_data.shape\n",
    "    decoded_data = np.zeros((num_samples, 16, 16, 3))  # Adjusted for 3 channels\n",
    "\n",
    "    for sample in range(num_samples):\n",
    "        for i in range(encoded_height):\n",
    "            for j in range(encoded_width):\n",
    "                for c in range(num_channels):\n",
    "                    # Decode each channel\n",
    "                    if c == 0:\n",
    "                        decoded_data[sample, 2*i, 2*j] = encoded_data[sample, i, j, c, :]\n",
    "                    elif c == 1:\n",
    "                        decoded_data[sample, 2*i, 2*j+1] = encoded_data[sample, i, j, c, :]\n",
    "                    elif c == 2:\n",
    "                        decoded_data[sample, 2*i+1, 2*j] = encoded_data[sample, i, j, c, :]\n",
    "                    elif c == 3:\n",
    "                        decoded_data[sample, 2*i+1, 2*j+1] = encoded_data[sample, i, j, c, :]\n",
    "\n",
    "    return decoded_data\n",
    "\n",
    "def flip(decoded_data):\n",
    "    return 1 - decoded_data\n",
    "\n",
    "def calculate_statistics(data):\n",
    "    data = data.reshape(data.shape[0], -1)\n",
    "    mean = np.mean(data, axis=0)\n",
    "    covariance = np.cov(data, rowvar=False)\n",
    "    return mean, covariance\n",
    "\n",
    "def calculate_fid(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    diff = mu1 - mu2\n",
    "    covmean, _ = scipy.linalg.sqrtm(sigma1 @ sigma2, disp=False)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = diff @ diff + np.trace(sigma1) + np.trace(sigma2) - 2 * np.trace(covmean)\n",
    "    return fid\n",
    "\n",
    "n_layers = 6\n",
    "input_dim = 8 * 8 * 4 * 3  # Updated input dimension based on data\n",
    "hidden_dim = 128\n",
    "output_dim = input_dim  \n",
    "\n",
    "model = QuantumDiffusionModel(input_dim, hidden_dim, output_dim, n_qubits, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 50\n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "fid_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Reshape input correctly to fit into the model\n",
    "    inputs = train_scrambled_states.view(len(train_scrambled_states), -1)\n",
    "    print(f\"Train scrambled states shape: {inputs.shape}\")\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    loss = criterion(outputs, train_encoded_data.view(len(train_encoded_data), -1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_values.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_inputs = val_scrambled_states.view(len(val_scrambled_states), -1)\n",
    "        print(f\"Val scrambled states shape: {val_inputs.shape}\")\n",
    "        val_outputs = model(val_inputs)\n",
    "        val_loss = criterion(val_outputs, val_encoded_data.view(len(val_encoded_data), -1))\n",
    "        val_loss_values.append(val_loss.item())\n",
    "        \n",
    "        denoised_states = model(val_scrambled_states.view(len(val_scrambled_states), -1))\n",
    "        denoised_states = denoised_states.view(len(val_scrambled_states), 8, 8, 4, 3).detach().numpy()  # Adjusted for 3 channels\n",
    "        decoded_data = decode(denoised_states)\n",
    "        decoded_data = flip(decoded_data)\n",
    "\n",
    "        mu1, sigma1 = calculate_statistics(data_X[:len(decoded_data)])\n",
    "        mu2, sigma2 = calculate_statistics(decoded_data)\n",
    "        fid = calculate_fid(mu1, sigma1, mu2, sigma2)\n",
    "        fid_scores.append(fid)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, FID: {fid:.4f}')\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_values, label='Training Loss', color='blue')\n",
    "plt.plot(val_loss_values, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Loss', fontsize=14)\n",
    "plt.title('Training and Validation Loss Over Epochs', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(fid_scores, label='FID Score', color='green')\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('FID', fontsize=14)\n",
    "plt.title('FID Score Over Epochs', fontsize=16)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_images(model, num_images, input_dim=8*8*4*3):\n",
    "    model.eval()  \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(num_images):\n",
    "            random_noise = torch.randn(num_images, input_dim)\n",
    "            \n",
    "            generated_data = model(random_noise)\n",
    "            generated_data = flip(generated_data.view(num_images, 8, 8, 4, 3).detach().numpy())  # Adjusted for 3 channels\n",
    "            \n",
    "            decoded_images = decode(generated_data)\n",
    "            fig, axes = plt.subplots(1, 5, figsize=(10, 2))\n",
    "\n",
    "            for qubit in range(4):\n",
    "                im = axes[qubit].imshow(generated_data[i, :, :, qubit, 0], cmap='viridis')  # Showing first channel\n",
    "                axes[qubit].set_title(f\"Encoded Qubit {qubit+1} (Channel 1)\")\n",
    "                fig.colorbar(im, ax=axes[qubit])\n",
    "\n",
    "            im = axes[4].imshow(decoded_images[i, :, :, 0], cmap='viridis')  # Showing decoded first channel\n",
    "            axes[4].set_title(\"Decoded (Channel 1)\")\n",
    "            fig.colorbar(im, ax=axes[4])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return decoded_images\n",
    "\n",
    "num_samples_to_generate = 5\n",
    "new_images = generate_new_images(model, num_samples_to_generate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
