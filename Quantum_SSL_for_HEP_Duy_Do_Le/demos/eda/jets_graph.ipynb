{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_ssl.data_jets_graph import *\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 100000, Info: Data(y=[1], particleid=[18, 1], h=[18, 3], num_nodes=18), Sample...: \n",
      "tensor([[ 5.3666e-04,  3.8694e-01, -8.9608e-03],\n",
      "        [ 3.1963e-04, -2.2557e-01, -2.0012e-01],\n",
      "        [ 2.2936e-03, -3.2403e-02, -2.4649e-01],\n",
      "        [ 8.2497e-03,  2.0372e-01,  1.5876e-02],\n",
      "        [ 3.3865e-03, -1.8214e-01,  4.6523e-02]])\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "'knn_graph' requires 'torch-cluster'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m total_size)\n\u001b[1;32m     12\u001b[0m dataset\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m KNNGroup(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, attr_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNode degree: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;241m.\u001b[39mnum_edges\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39msample\u001b[38;5;241m.\u001b[39mnum_nodes\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Miniconda3/envs/py310_qml/lib/python3.10/site-packages/torch_geometric/data/dataset.py:292\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, (\u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39minteger))\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, Tensor) \u001b[38;5;129;01mand\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(idx, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(idx))):\n\u001b[1;32m    291\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices()[idx])\n\u001b[0;32m--> 292\u001b[0m     data \u001b[38;5;241m=\u001b[39m data \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/gsoc24/gsoc24-qml-workspace/src/qml_ssl/data_jets_graph.py:200\u001b[0m, in \u001b[0;36mKNNGroup.__call__\u001b[0;34m(self, data, self_loop)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattr_name):\n\u001b[1;32m    199\u001b[0m     attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattr_name)\n\u001b[0;32m--> 200\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mpyg_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     data\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m self_loop:\n\u001b[1;32m    204\u001b[0m         \u001b[38;5;66;03m# Create self-loops\u001b[39;00m\n",
      "File \u001b[0;32m~/Miniconda3/envs/py310_qml/lib/python3.10/site-packages/torch_geometric/nn/pool/__init__.py:171\u001b[0m, in \u001b[0;36mknn_graph\u001b[0;34m(x, k, batch, loop, flow, cosine, num_workers, batch_size)\u001b[0m\n\u001b[1;32m    168\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch_geometric\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mWITH_TORCH_CLUSTER_BATCH_SIZE:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_cluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn_graph\u001b[49m(x, k, batch, loop, flow, cosine,\n\u001b[1;32m    172\u001b[0m                                    num_workers)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch_cluster\u001b[38;5;241m.\u001b[39mknn_graph(x, k, batch, loop, flow, cosine,\n\u001b[1;32m    174\u001b[0m                                num_workers, batch_size)\n",
      "File \u001b[0;32m~/Miniconda3/envs/py310_qml/lib/python3.10/site-packages/torch_geometric/typing.py:104\u001b[0m, in \u001b[0;36mTorchCluster.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch-cluster\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: 'knn_graph' requires 'torch-cluster'"
     ]
    }
   ],
   "source": [
    "dataset = QG_Jets('../data/QG_Jets')\n",
    "print(f\"Length: {len(dataset)}, Info: {dataset[0]}, Sample...: \\n{dataset[0].h[:5]}\")\n",
    "\n",
    "\n",
    "total_size = 100000\n",
    "dataset = dataset[:total_size]\n",
    "\n",
    "train_size = int(0.6 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = int(0.2 * total_size)\n",
    "\n",
    "dataset.transform = KNNGroup(k=5, attr_name=\"h\")\n",
    "sample = dataset[10]\n",
    "print(sample)\n",
    "\n",
    "print(f'Node degree: {sample.num_edges / sample.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {sample.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {sample.has_self_loops()}')\n",
    "print(f'Is undirected: {sample.is_undirected()}')\n",
    "nx.draw(pyg.utils.to_networkx(dataset[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_ssl.data_jets_graph import *\n",
    "\n",
    "def generate_embeddings(model, data_loader):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the given data using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        data_loader (DataLoader): Data loader for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Embeddings and labels as numpy arrays.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(model.device)\n",
    "            emb = model.model(data)\n",
    "            embeddings.append(emb)\n",
    "            labels.append(data.y)\n",
    "    \n",
    "    embeddings = torch.cat(embeddings).cpu().numpy()\n",
    "    labels = torch.cat(labels).cpu().numpy()\n",
    "    \n",
    "    return embeddings, labels\n",
    "\n",
    "class Custom_GCN(pyg_nn.MessagePassing):\n",
    "    def __init__(self, out_channels, in_channels=8):\n",
    "        super().__init__(aggr='add')\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, h, particleid, edge_index):\n",
    "        return self.propagate(edge_index, h=h, particleid=particleid)\n",
    "\n",
    "    def message(self, h_i, h_j, particleid_i, particleid_j):\n",
    "        edge_feat = torch.cat([h_i, h_j, particleid_i, particleid_j], dim=-1)\n",
    "        return self.mlp(edge_feat)\n",
    "    \n",
    "class GCN_Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = Custom_GCN(hidden_dim)\n",
    "        self.conv2 = Custom_GCN(hidden_dim, in_channels=hidden_dim*2+2)\n",
    "        self.output_dim = 8\n",
    "        # self.classifier = pyg_nn.MLP([hidden_dim, hidden_dim, output_dim], bias=[False, True])\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, self.output_dim))\n",
    "\n",
    "    def forward(self, data):\n",
    "        h, particleid, edge_index, batch = data.h, data.particleid, data.edge_index, data.batch\n",
    "        \n",
    "        # First Custom_GCN layer\n",
    "        x = self.conv1(h=h, particleid=particleid, edge_index=edge_index)\n",
    "        x = x.relu()\n",
    "        # x = self.dropout(x)\n",
    "        \n",
    "        # Second Custom_GCN layer\n",
    "        x = self.conv2(h=x, particleid=particleid, edge_index=edge_index)\n",
    "        x = x.relu()\n",
    "        # x = self.dropout(h)\n",
    "        \n",
    "        # Global Pooling:\n",
    "        x = pyg_nn.global_mean_pool(x, batch)\n",
    "        \n",
    "        # Classifier:\n",
    "        # return self.classifier(x)\n",
    "        return self.readout(x)\n",
    "\n",
    "import pytorch_lightning as pl    \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device:\", device)\n",
    "batch_size = 64\n",
    "from pytorch_metric_learning import losses\n",
    "import torchmetrics\n",
    "\n",
    "class ModelPL_Contrastive(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=0.01):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        # self.criterion = losses.ContrastiveLoss(pos_margin=0.25, neg_margin=5.0)\n",
    "        self.criterion = losses.NTXentLoss(temperature=0.2)\n",
    "\n",
    "        self.train_loss = torchmetrics.MeanMetric()\n",
    "        self.val_loss = torchmetrics.MeanMetric()\n",
    "\n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=1),\n",
    "            'monitor': 'val_loss',\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        embeddings = self(data)\n",
    "        loss = self.criterion(embeddings, data.y)\n",
    "        self.train_loss.update(loss)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data, batch_idx):\n",
    "        embeddings = self(data)\n",
    "        loss = self.criterion(embeddings, data.y)\n",
    "        self.val_loss.update(loss)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, data, batch_idx):\n",
    "        embeddings = self(data)\n",
    "        loss = self.criterion(embeddings, data.y)\n",
    "        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class ModelPL_Classify(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.model.output_dim, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 2))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        from torchmetrics import AUROC, Accuracy \n",
    "        self.train_auc = AUROC(task='binary')\n",
    "        self.val_auc = AUROC(task='binary')\n",
    "        self.test_auc = AUROC(task='binary')\n",
    "        \n",
    "        self.train_acc = Accuracy(task='binary')\n",
    "        self.val_acc = Accuracy(task='binary')\n",
    "        self.test_acc = Accuracy(task='binary')\n",
    "\n",
    "    def forward(self, data):\n",
    "        embeddings = self.model(data)\n",
    "        return self.classifier(embeddings)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                        mode='min', factor=0.25, patience=1),\n",
    "            'monitor': 'val_loss', \n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        lr = self.optimizers().param_groups[0]['lr']\n",
    "        self.log('learning_rate', lr, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        logits = self(data)\n",
    "        loss = self.criterion(logits.squeeze(), data.y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.train_auc(F.softmax(logits.squeeze(), dim=1)[:, 1], data.y)\n",
    "        self.log(\"train_auc\", self.train_auc, on_step=False, on_epoch=True, prog_bar=False, batch_size=batch_size)\n",
    "        \n",
    "        self.train_acc(logits.argmax(dim=-1), data.y)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=False, batch_size=batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data, batch_idx):\n",
    "        logits = self(data)\n",
    "        loss = self.criterion(logits.squeeze(), data.y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.val_auc(F.softmax(logits.squeeze(), dim=1)[:, 1], data.y)\n",
    "        self.log(\"val_auc\", self.val_auc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.val_acc(logits.argmax(dim=-1), data.y)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "    def test_step(self, data, batch_idx):\n",
    "        logits = self(data)\n",
    "        \n",
    "        self.test_auc(F.softmax(logits.squeeze(), dim=1)[:, 1], data.y)\n",
    "        self.log(\"test_auc\", self.test_auc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.test_acc(logits.argmax(dim=-1), data.y)\n",
    "        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_ssl.utils import vmf_kde_on_circle, pca_proj, tsne_proj, plot_training\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = pyg_loader.DataLoader(train_dataset, batch_size=batch_size, num_workers = 12)\n",
    "val_loader = pyg_loader.DataLoader(val_dataset, batch_size=batch_size, num_workers = 12)\n",
    "test_loader = pyg_loader.DataLoader(test_dataset, batch_size=batch_size, num_workers = 12)\n",
    "\n",
    "GCN_encoder = GCN_Encoder()\n",
    "\n",
    "Graph_pl = ModelPL_Classify(model=GCN_encoder, learning_rate=0.01)\n",
    "\n",
    "logger = pl.loggers.CSVLogger(save_dir='logs', name='Graph_pl', version=0)\n",
    "\n",
    "summary_callback = pl.callbacks.ModelSummary(max_depth=8)\n",
    "callbacks = [summary_callback]\n",
    "\n",
    "# embeddings, labels = generate_embeddings(Contrastive_Graph_pl, val_loader)\n",
    "# pca_proj(embeddings, labels)\n",
    "# tsne_proj(embeddings, labels)\n",
    "# vmf_kde_on_circle(embeddings, labels)\n",
    "\n",
    "trainer_GCNN = pl.Trainer(max_epochs=20, \n",
    "                            devices=\"auto\",\n",
    "                            callbacks=callbacks,\n",
    "                            logger=logger,)\n",
    "\n",
    "train_result = trainer_GCNN.fit(Graph_pl, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "test_result = trainer_GCNN.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce number of node - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 15\n",
    "\n",
    "dataset.transform = T.Compose([TopKMomentum(k=k), KNNGroup(k=5, attr_name=\"h\")])\n",
    "sample = dataset[10]\n",
    "print(sample)\n",
    "\n",
    "print(f'Node degree: {sample.num_edges / sample.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {sample.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {sample.has_self_loops()}')\n",
    "print(f'Is undirected: {sample.is_undirected()}')\n",
    "nx.draw(pyg.utils.to_networkx(dataset[10]))\n",
    "plt.show()\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = pyg_loader.DataLoader(train_dataset, batch_size=batch_size, num_workers = 12)\n",
    "val_loader = pyg_loader.DataLoader(val_dataset, batch_size=batch_size, num_workers = 12)\n",
    "test_loader = pyg_loader.DataLoader(test_dataset, batch_size=batch_size, num_workers = 12)\n",
    "\n",
    "GCN_encoder = GCN_Encoder()\n",
    "\n",
    "Graph_pl = ModelPL_Classify(model=GCN_encoder, learning_rate=0.01)\n",
    "\n",
    "logger = pl.loggers.CSVLogger(save_dir='logs', name=f'graph_pl_{k}', version=0)\n",
    "\n",
    "summary_callback = pl.callbacks.ModelSummary(max_depth=8)\n",
    "callbacks = [summary_callback]\n",
    "\n",
    "# embeddings, labels = generate_embeddings(Contrastive_Graph_pl, val_loader)\n",
    "# pca_proj(embeddings, labels)\n",
    "# tsne_proj(embeddings, labels)\n",
    "# vmf_kde_on_circle(embeddings, labels)\n",
    "\n",
    "trainer_GCNN = pl.Trainer(max_epochs=10, \n",
    "                            devices=\"auto\",\n",
    "                            callbacks=callbacks,\n",
    "                            logger=logger,)\n",
    "\n",
    "train_result = trainer_GCNN.fit(Graph_pl, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "test_result = trainer_GCNN.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_metrics(logs_dir='logs', prefix='graph_pl_'):\n",
    "    metrics = []\n",
    "\n",
    "    # Iterate over directories in the logs_dir\n",
    "    for folder in os.listdir(logs_dir):\n",
    "        \n",
    "        if folder.startswith(prefix):\n",
    "            \n",
    "            k = int(folder[len(prefix):])  # Extract the value of k from the folder name\n",
    "            metrics_file = os.path.join(logs_dir, folder, 'version_0', 'metrics.csv')\n",
    "            \n",
    "            print(folder, k, metrics_file)\n",
    "            if os.path.exists(metrics_file):\n",
    "                df = pd.read_csv(metrics_file)\n",
    "                \n",
    "                if 'test_auc' in df.columns and 'test_acc' in df.columns:\n",
    "                    \n",
    "                    # Get the last row's test_auc and test_acc\n",
    "                    test_auc = df['test_auc'].dropna().values[-1]\n",
    "                    test_acc = df['test_acc'].dropna().values[-1]\n",
    "                    metrics.append((k, test_auc, test_acc))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    metrics = sorted(metrics, key=lambda x: x[0])  # Sort by k value\n",
    "    ks, test_aucs, test_accs = zip(*metrics)  # Unzip the list of tuples\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot test AUC\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(ks, test_aucs, marker='o', linestyle='-', color='b')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Test AUC')\n",
    "    plt.title('Test AUC vs k')\n",
    "\n",
    "    # Plot test accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(ks, test_accs, marker='o', linestyle='-', color='r')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Test Accuracy')\n",
    "    plt.title('Test Accuracy vs k')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "# logs_dir = 'logs'\n",
    "# prefix = 'graph_pl'\n",
    "metrics = extract_metrics()\n",
    "print(metrics)\n",
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qml_ssl.utils import vmf_kde_on_circle, pca_proj, tsne_proj, plot_training\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = pyg_loader.DataLoader(train_dataset, batch_size=batch_size, num_workers = 12)\n",
    "val_loader = pyg_loader.DataLoader(val_dataset, batch_size=batch_size, num_workers = 12)\n",
    "test_loader = pyg_loader.DataLoader(test_dataset, batch_size=batch_size, num_workers = 12)\n",
    "\n",
    "encoder = GCN_Encoder()\n",
    "\n",
    "Contrastive_Graph_pl = ModelPL_Contrastive(model=encoder, learning_rate=0.001)\n",
    "\n",
    "logger = pl.loggers.CSVLogger(save_dir='logs', name='Contrastive_Graph_pl')\n",
    "\n",
    "summary_callback = pl.callbacks.ModelSummary(max_depth=8)\n",
    "callbacks = [summary_callback]\n",
    "\n",
    "embeddings, labels = generate_embeddings(Contrastive_Graph_pl, val_loader)\n",
    "pca_proj(embeddings, labels)\n",
    "# tsne_proj(embeddings, labels)\n",
    "# vmf_kde_on_circle(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_GCNN = pl.Trainer(max_epochs=15, \n",
    "                            devices=\"auto\",\n",
    "                            callbacks=callbacks,\n",
    "                            logger=logger,)\n",
    "\n",
    "train_result = trainer_GCNN.fit(Contrastive_Graph_pl, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "test_result = trainer_GCNN.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, labels = generate_embeddings(Contrastive_Graph_pl, val_loader)\n",
    "pca_proj(embeddings, labels)\n",
    "# tsne_proj(embeddings, labels)\n",
    "# vmf_kde_on_circle(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbePL(pl.LightningModule):\n",
    "    def __init__(self, pretrained_model, num_classes, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(pretrained_model.output_dim, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, num_classes),\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        from torchmetrics import AUROC, Accuracy \n",
    "        self.train_auc = AUROC(task='binary')\n",
    "        self.val_auc = AUROC(task='binary')\n",
    "        self.test_auc = AUROC(task='binary')\n",
    "        \n",
    "        self.train_acc = Accuracy(task='binary')\n",
    "        self.val_acc = Accuracy(task='binary')\n",
    "        self.test_acc = Accuracy(task='binary')\n",
    "        \n",
    "        for param in self.pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings = self.pretrained_model(x)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        lr_scheduler = {\n",
    "            'scheduler': optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                        mode='min', factor=0.25, patience=1),\n",
    "            'monitor': 'val_loss', \n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, data, batch_idx):\n",
    "        lr = self.optimizers().param_groups[0]['lr']\n",
    "        self.log('learning_rate', lr, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        logits = self(data)\n",
    "        loss = self.criterion(logits.squeeze(), data.y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.train_auc(F.softmax(logits.squeeze(), dim=1)[:, 1], data.y)\n",
    "        self.log(\"train_auc\", self.train_auc, on_step=False, on_epoch=True, prog_bar=False, batch_size=batch_size)\n",
    "        \n",
    "        self.train_acc(logits.argmax(dim=-1), data.y)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=False, batch_size=batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data, batch_idx):\n",
    "        logits = self(data)\n",
    "        loss = self.criterion(logits.squeeze(), data.y)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.val_auc(F.softmax(logits.squeeze(), dim=1)[:, 1], data.y)\n",
    "        self.log(\"val_auc\", self.val_auc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.val_acc(logits.argmax(dim=-1), data.y)\n",
    "        self.log('val_acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "    def test_step(self, data, batch_idx):\n",
    "        logits = self(data)\n",
    "        \n",
    "        self.test_auc(F.softmax(logits.squeeze(), dim=1)[:, 1], data.y)\n",
    "        self.log(\"test_auc\", self.test_auc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "        \n",
    "        self.test_acc(logits.argmax(dim=-1), data.y)\n",
    "        self.log('test_acc', self.test_acc, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n",
    "\n",
    "pretrained_model = GCN_Encoder()\n",
    "# checkpoint_path = './logs/Contrastive_Graph_pl/version_0/checkpoints/epoch=14-step=14070.ckpt'\n",
    "pretrained_model.load_state_dict(Contrastive_Graph_pl.model.state_dict())\n",
    "\n",
    "num_classes = 2  # Adjust this based on your dataset\n",
    "logger = pl.loggers.CSVLogger(save_dir='logs', name='Contrastive_Graph_finetune_pl')\n",
    "# Create an instance of the LinearProbePL module\n",
    "linear_probe_model = LinearProbePL(pretrained_model=pretrained_model, num_classes=num_classes, learning_rate=0.001)\n",
    "\n",
    "trainer_linear_probe = pl.Trainer(max_epochs=15, \n",
    "                            devices=\"auto\",\n",
    "                            callbacks=callbacks,\n",
    "                            logger=logger,)\n",
    "\n",
    "train_result = trainer_linear_probe.fit(linear_probe_model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "test_result = trainer_linear_probe.test(dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metrics_from_csv(metrics_file, metrics={'val_loss', 'val_acc', 'val_auc'}):\n",
    "    df = pd.read_csv(metrics_file)\n",
    "\n",
    "    required_columns = metrics\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(\"The CSV file does not contain the required metrics.\")\n",
    "\n",
    "    df = df.sort_values('epoch')\n",
    "\n",
    "    df = df.fillna(method='ffill')\n",
    "\n",
    "    epochs = df['epoch']\n",
    "    val_loss = df['val_loss']\n",
    "    val_acc = df['val_acc']\n",
    "    val_auc = df['val_auc']\n",
    "\n",
    "    plt.figure(figsize=(5*len(metrics), 5))\n",
    "\n",
    "    plt.subplot(1, len(metrics), 1)\n",
    "    plt.plot(epochs, val_loss, marker='o', linestyle='-', color='b', label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(epochs, val_acc, marker='o', linestyle='-', color='r', label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(epochs, val_auc, marker='o', linestyle='-', color='g', label='Validation AUC')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('Validation AUC')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_metrics_from_csv('./logs/Contrastive_Graph_pl/version_0/metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39_qml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
