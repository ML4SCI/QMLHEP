{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install torch==2.2.0\n",
        "!pip install torch_geometric\n",
        "!pip install particle\n",
        "!pip install pennylane\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install torchvision==0.17.0\n",
        "!pip install qiskit==0.46.0\n",
        "!pip install torchquantum\n",
        "!pip install qiskit-ibm-runtime==0.18.0\n",
        "!pip install qiskit-aer==0.13.2\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
        "!pip install dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "!pip install energyflow\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "_4qoLSMk7-Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "gt_ntIZ7S00B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchdata\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchquantum as tq\n",
        "from torchquantum.layer.entanglement.op2_layer import Op2QAllLayer\n",
        "from torchquantum.layer.layers.layers import Op1QAllLayer, Op2QAllLayer\n",
        "from torchquantum.measurement import measure\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import warnings\n",
        "import dgl\n",
        "from dgl.data import DGLDataset\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "import pickle\n",
        "import multiprocessing\n",
        "import joblib\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.utils import add_self_loops, degree, softmax\n",
        "import torch.optim as optim\n",
        "\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "\n",
        "from particle import Particle\n",
        "import pennylane as qml\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kDgEhsePTK1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Download the jets along with storing them in a folder for future use, eliminating the need to download them again\n",
        "# main_dir = ''\n",
        "\n",
        "# import energyflow\n",
        "# data = energyflow.qg_jets.load(num_data=20000, pad=True, ncol=4, generator='pythia',\n",
        "#                         with_bc=False, cache_dir=main_dir+'/energyflow')"
      ],
      "metadata": {
        "id": "aCmapYGUTWG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jet_file_path = ''\n",
        "data = np.load(jet_file_path)"
      ],
      "metadata": {
        "id": "IZJZuPAvUJ-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/ML4SCI/QMLHEP/blob/main/Quantum_GNN_for_HEP_Roy_Forestano/utils/preprocess.py\n",
        "\n",
        "def preprocess_fixed_nodes(x_data,y_data,nodes_per_graph=10): #,masses):\n",
        "    print('--- Finding All Unique Particles ---')\n",
        "    unique_particles = np.unique(x_data[:,:,3])\n",
        "    x_data = torch.tensor(x_data)\n",
        "    y_data = torch.tensor(y_data)\n",
        "    print()\n",
        "    print('--- Inserting Masses ---')\n",
        "    masses = torch.zeros((x_data.shape[0],x_data.shape[1]))\n",
        "    for i,particle in tqdm(enumerate(unique_particles)):\n",
        "        if particle!=0:\n",
        "            mass = Particle.from_pdgid(particle).mass/1000\n",
        "            inds = torch.where(particle==x_data[:,:,3])\n",
        "            masses[inds]=mass # GeV\n",
        "    print()\n",
        "    print('--- Calculating Momenta and Energies ---')\n",
        "    #theta = torch.arctan(torch.exp(-X[:,:,1]))*2 # polar angle\n",
        "    pt        = x_data[:,:,0]     # transverse momentum\n",
        "    rapidity  = x_data[:,:,1]     # rapidity\n",
        "    phi       = x_data[:,:,2]     # azimuthal angle\n",
        "\n",
        "    mt        = (pt**2+masses**2).sqrt() # Transverse mass\n",
        "    energy    = mt*torch.cosh(rapidity) # Energy per multiplicity bin\n",
        "    e_per_jet = energy.sum(axis=1)  # total energy per jet summed across multiplicity bins\n",
        "\n",
        "    px = pt*torch.cos(phi)  # momentum in x\n",
        "    py = pt*torch.sin(phi)  # momentum in y\n",
        "    pz = mt*torch.sinh(rapidity)  # momentum in z\n",
        "\n",
        "    # three momentum\n",
        "    p  = torch.cat(( px[:,:,None],\n",
        "                     py[:,:,None],\n",
        "                     pz[:,:,None]), dim=2 )\n",
        "\n",
        "    p_per_jet        = (p).sum(axis=1)  # total componet momentum per jet\n",
        "    pt_per_Mbin      = (p_per_jet[:,:2]**2).sum(axis=1).sqrt()  # transverse momentum per jet\n",
        "    mass_per_jet     = (e_per_jet**2-(p_per_jet**2).sum(axis=1)).sqrt() # mass per jet\n",
        "    rapidity_per_jet = torch.log( (e_per_jet+p_per_jet[:,2])/(e_per_jet-p_per_jet[:,2]) )/2  # rapidity per jet from analytical formula\n",
        "    end_multiplicity_indx_per_jet = (pt!=0).sum(axis=1).int() # see where the jet (graph) ends\n",
        "\n",
        "    x_data = torch.cat( ( x_data[:,:,:3],\n",
        "                          x_data[:,:,4:],\n",
        "                          masses[:,:,None],\n",
        "                          energy[:,:,None],\n",
        "                          p), dim=2)\n",
        "\n",
        "    x_data_max = (x_data.max(dim=1).values).max(dim=0).values\n",
        "    x_data = x_data/x_data_max\n",
        "\n",
        "    print()\n",
        "    print('--- Calculating Edge Tensors ---')\n",
        "    N = x_data[:,0,3].shape[0]  # number of jets (graphs)\n",
        "    M = nodes_per_graph #x_data[0,:,3].shape[0]  # number of max multiplicty\n",
        "    connections = nodes_per_graph\n",
        "    edge_tensor = torch.zeros((N,M,M))\n",
        "    edge_indx_tensor = torch.zeros((N,2,connections*(connections-1) )) # M*(connections-1) is the max number of edges we allow per jet\n",
        "    edge_attr_matrix = torch.zeros((N,connections*(connections-1),1))\n",
        "#     fixed_edges_list = torch.tensor([ [i,j] for i in range(connections) for j in range(connections) if i!=j]).reshape(2,90)\n",
        "\n",
        "    for jet in tqdm(range(N)):\n",
        "        stop_indx = end_multiplicity_indx_per_jet[jet] #connections # stop finding edges once we hit zeros -> when we hit 10\n",
        "        if end_multiplicity_indx_per_jet[jet]>=connections:\n",
        "            for m in range(connections):\n",
        "#                 inds_edge = np.argsort((energy[jet,m]+energy[jet,:stop_indx])**2-torch.sum((p[jet,m,:stop_indx]+p[jet,:stop_indx,:])**2,axis=1))[:connections]\n",
        "#                 edge_tensor[jet,m,:] = (energy[jet,m]+energy[jet,:connections])**2-torch.sum((p[jet,m,:]+p[jet,:connections,:])**2,axis=1)\n",
        "#                 edge_tensor[jet,m,m] = 0.\n",
        "#                 edge_tensor[jet,m,m]=((energy[jet,m]+energy[jet,m])**2-torch.sum((p[jet,m,:]+p[jet,m,:])**2,axis=0))\n",
        "                # inds_edge = torch.sqrt( (phi[jet,m]-phi[jet,:])**2 + (rapidity[jet,m]-rapidity[jet,:])**2 ).argsort()[:connections]\n",
        "                # edge_tensor[jet,m,:] = torch.sqrt( (phi[jet,m]-phi[jet,inds_edge])**2 + (rapidity[jet,m]-rapidity[jet,inds_edge])**2 )\n",
        "                edge_tensor[jet,m,:] = torch.sqrt( (phi[jet,m]-phi[jet,:connections])**2 + (rapidity[jet,m]-rapidity[jet,:connections])**2 )\n",
        "#                 inds_edge = np.argsort( (energy[jet,m]+energy[jet,:stop_indx])**2-torch.sum((p[jet,m,:stop_indx]+p[jet,:stop_indx,:])**2,axis=1) )[:connections]\n",
        "#                 edge_tensor[jet,m,inds_edge] = (energy[jet,m]+energy[jet,inds_edge])**2-torch.sum((p[jet,m,:]+p[jet,inds_edge,:])**2,axis=1)\n",
        "            edges_exist_at = torch.where(edge_tensor[jet,:,:].abs()>0)\n",
        "\n",
        "#             edge_indx_tensor[jet,:,:(edge_tensor[jet,:,:].abs()>0).sum()] = fixed_edges_list\n",
        "            edge_indx_tensor[jet,:,:(edge_tensor[jet,:,:].abs()>0).sum()] = torch.cat((edges_exist_at[0][None,:],edges_exist_at[1][None,:]),dim=0).reshape((2,edges_exist_at[0].shape[0]))\n",
        "            edge_attr_matrix[jet,:(edge_tensor[jet,:,:].abs()>0).sum(),0]  =  edge_tensor[jet,edges_exist_at[0],edges_exist_at[1]].flatten()\n",
        "\n",
        "    end_edges_indx_per_jet = (edge_attr_matrix!=0).sum(axis=1).int()\n",
        "    keep_inds =  torch.where(end_edges_indx_per_jet>=connections)[0]\n",
        "\n",
        "    edge_tensor = edge_tensor/edge_tensor.max()\n",
        "    edge_attr_matrix = edge_attr_matrix/edge_attr_matrix.max()\n",
        "\n",
        "    graph_help = torch.cat( ( (energy.max(axis=1).values/e_per_jet).reshape(x_data[:,0,3].shape[0],1),\n",
        "                              (mass_per_jet).reshape(x_data[:,0,3].shape[0],1),\n",
        "                              (end_multiplicity_indx_per_jet).reshape(x_data[:,0,3].shape[0],1).int(),\n",
        "                              (end_edges_indx_per_jet).reshape(x_data[:,0,3].shape[0],1).int() ), dim=1)\n",
        "\n",
        "    return x_data[keep_inds,:nodes_per_graph], y_data[keep_inds].long(), edge_tensor[keep_inds], edge_indx_tensor[keep_inds].long(), edge_attr_matrix[keep_inds], graph_help[keep_inds], masses"
      ],
      "metadata": {
        "id": "lhY05LBQUeYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/bmdillon/JetCLR/blob/main/scripts/modules/jet_augs.py\n",
        "\n",
        "def distort_jets( batch, strength=0.1, pT_clip_min=0.1 ):\n",
        "    '''\n",
        "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
        "    dim 1 ordering: (pT, eta, phi)\n",
        "    Output: batch of jets with each constituents position shifted independently, shifts drawn from normal with mean 0, std strength/pT, same shape as input\n",
        "    '''\n",
        "    pT = batch[:,0]   # (batchsize, n_constit)\n",
        "    shift_eta = np.nan_to_num( strength * np.random.randn(batch.shape[0], batch.shape[2]) / pT.clip(min=pT_clip_min), posinf = 0.0, neginf = 0.0 )# * mask\n",
        "    shift_phi = np.nan_to_num( strength * np.random.randn(batch.shape[0], batch.shape[2]) / pT.clip(min=pT_clip_min), posinf = 0.0, neginf = 0.0 )# * mask\n",
        "    shift = np.stack( [ np.zeros( (batch.shape[0], batch.shape[2]) ), shift_eta, shift_phi ], 1)\n",
        "    return batch + shift\n",
        "\n",
        "def collinear_fill_jets( batch ):\n",
        "    '''\n",
        "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
        "    dim 1 ordering: (pT, eta, phi)\n",
        "    Output: batch of jets with collinear splittings, the function attempts to fill as many of the zero-padded args.nconstit\n",
        "    entries with collinear splittings of the constituents by splitting each constituent at most once, same shape as input\n",
        "    '''\n",
        "    batchb = batch.copy()\n",
        "    nc = batch.shape[2]\n",
        "    nzs = np.array( [ np.where( batch[:,0,:][i]>0.0)[0].shape[0] for i in range(len(batch)) ] )\n",
        "\n",
        "    for k in range(len(batch)):\n",
        "        nzs1 = np.max( [ nzs[k], int(nc/2) ] )\n",
        "        zs1 = int(nc-nzs1)\n",
        "        els = np.random.choice( np.linspace(0,nzs1-1,nzs1), size=zs1, replace=False )\n",
        "        rs = np.random.uniform( size=zs1 )\n",
        "        for j in range(zs1):\n",
        "            batchb[k,0,int(els[j])] = rs[j]*batch[k,0,int(els[j])]\n",
        "            batchb[k,0,int(nzs[k]+j)] = (1-rs[j])*batch[k,0,int(els[j])]\n",
        "            batchb[k,1,int(nzs[k]+j)] = batch[k,1,int(els[j])]\n",
        "            batchb[k,2,int(nzs[k]+j)] = batch[k,2,int(els[j])]\n",
        "\n",
        "    return batchb"
      ],
      "metadata": {
        "id": "-uyS1h4zU4XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuarkGluonGraphDataset(dgl.data.dgl_dataset.DGLDataset):\n",
        "\n",
        "  def __init__(self, dataset_name, raw_dir, save_dir, data_folder_name, datafile_name, labelsfile_name, datatype='particles', dataset_size=12500,\n",
        "               nodes_per_graph = 5, spectral_augmentation=False, irc_safety_aug=False, url=None, hash_key=..., force_reload=False, verbose=False, transform=None,\n",
        "              device='cpu'):\n",
        "    self.data_folder = data_folder_name\n",
        "    self.datafile_name = datafile_name\n",
        "    self.labelsfile_name = labelsfile_name\n",
        "    self.datatype = datatype\n",
        "    self.nodes_per_graph = nodes_per_graph\n",
        "    self.spectral_augmentation = spectral_augmentation\n",
        "    self.drop_ra_nodes = False\n",
        "    self.drop_cp_nodes = False\n",
        "    self.aug_ratio = None\n",
        "    self.irc_safety_aug = irc_safety_aug\n",
        "    self.device = device\n",
        "    self.dataset_size = dataset_size\n",
        "    self.augment = False\n",
        "    self.nodes_per_aug_graph = None\n",
        "    super().__init__(dataset_name, url, raw_dir, save_dir, hash_key, force_reload, verbose, transform)\n",
        "\n",
        "  @property\n",
        "  def data_folder_name(self):\n",
        "    return self.data_folder\n",
        "\n",
        "  @property\n",
        "  def raw_path(self):\n",
        "    return os.path.join(self.raw_dir, self.data_folder_name)\n",
        "\n",
        "  @property\n",
        "  def save_path(self):\n",
        "    return os.path.join(self.save_dir, self.data_folder_name)\n",
        "\n",
        "  @property\n",
        "  def graph_path(self):\n",
        "    return os.path.join(self.save_path, 'graphs_and_labels')\n",
        "\n",
        "  @property\n",
        "  def info_path(self):\n",
        "    return os.path.join(self.save_path, 'graphs_and_labels')\n",
        "\n",
        "  def load(self):\n",
        "    graphs, label_dict = dgl.load_graphs(str(self.graph_path))\n",
        "    info_dict = dgl.data.utils.load_info(str(self.info_path))\n",
        "\n",
        "    self.graph_lists = graphs\n",
        "    self.graph_labels = label_dict[\"labels\"]\n",
        "    self.max_num_node = info_dict[\"max_num_node\"]\n",
        "    self.num_labels = info_dict[\"num_labels\"]\n",
        "\n",
        "  # def save(self,):\n",
        "  #   label_dict = {\"labels\": self.graph_labels}\n",
        "  #   info_dict = {\n",
        "  #           \"max_num_node\": self.max_num_node,\n",
        "  #           \"num_labels\": self.num_labels,\n",
        "  #       }\n",
        "  #   dgl.save_graphs(str(self.graph_path), self.graph_lists, label_dict)\n",
        "  #   dgl.data.utils.save_info(str(self.info_path), info_dict)\n",
        "\n",
        "  def process(self,):\n",
        "    data = np.load(os.path.join(self.raw_path, self.datafile_name))\n",
        "    X = data['X']\n",
        "    y = data['y']\n",
        "    X_l, y_l = [], []\n",
        "    i = 0\n",
        "\n",
        "    while len(X_l)!=self.dataset_size:\n",
        "        if np.unique(X[i].sum(axis=1).nonzero()).shape[0] >= self.nodes_per_graph:\n",
        "            sorted_inds = np.argsort(X[i,:,0])[::-1]\n",
        "            x = X[i][sorted_inds]\n",
        "            X_l.append(x[:self.nodes_per_graph, :])\n",
        "            y_l.append(y[i])\n",
        "        i += 1\n",
        "    X = np.array(X_l)\n",
        "    y = np.array(y_l)\n",
        "\n",
        "\n",
        "    if self.datatype == 'particles':\n",
        "      self.graph_lists = []\n",
        "      self.rationale_augmented_graph_lists_1 = []\n",
        "      self.rationale_augmented_graph_lists_2 = []\n",
        "      self.complement_augmented_graph_lists = []\n",
        "      x_data_proc, y_data_proc, edge_tensor, edge_indx_tensor, edge_attr_matrix, graph_help, masses = preprocess_fixed_nodes(X,y,nodes_per_graph = self.nodes_per_graph) #,masses[:N])\n",
        "      self.max_num_node = x_data_proc.shape[1]\n",
        "      self.graph_labels = y_data_proc\n",
        "      self.num_labels = y_data_proc.shape[0]\n",
        "\n",
        "      print('--- Creating graphs ---')\n",
        "      for i in tqdm(range(x_data_proc.shape[0])):\n",
        "        g = dgl.graph((edge_indx_tensor[i][0], edge_indx_tensor[i][1]))\n",
        "        g.ndata['node_attr'] = x_data_proc[i]\n",
        "        g.ndata['node_indices'] = torch.arange(x_data_proc[i].shape[0]).reshape(-1,1)\n",
        "        g.ndata['node_mass'] = masses[i][:self.nodes_per_graph]\n",
        "        g.edata['edge_attr'] = edge_attr_matrix[i].view(-1,)\n",
        "        g.to(self.device)\n",
        "        self.graph_lists.append(g)\n",
        "        self.rationale_augmented_graph_lists_1.append(g)\n",
        "        self.rationale_augmented_graph_lists_2.append(g)\n",
        "        self.complement_augmented_graph_lists.append(g)\n",
        "\n",
        "      if self.spectral_augmentation:\n",
        "        self.spectral_graph_lists = []\n",
        "        print('--- Creating spectral graphs ---')\n",
        "        for i in tqdm(range(x_data_proc.shape[0])):\n",
        "          g = SpectralGraph((edge_indx_tensor[i][0], edge_indx_tensor[i][1]), theta=0.1, delta_origin=0.05, edge_weights_matrix=edge_tensor[i])\n",
        "          g.ndata['node_attr'] = x_data_proc[i]\n",
        "          g.edata['edge_attr'] = edge_attr_matrix[i].view(-1,)\n",
        "          self.spectral_graph_lists.append(g)\n",
        "        # print(self.graph_lists)\n",
        "\n",
        "      if self.irc_safety_aug:\n",
        "        for idx in range(len(self.graph_lists)):\n",
        "          g = self.graph_lists[idx]\n",
        "          g.ndata['node_attr_irc'] = g.ndata['node_attr'].clone()\n",
        "          if self.device=='cuda':\n",
        "            g.ndata['node_attr_irc'][:,:3] = torch.Tensor(distort_jets(collinear_fill_jets(g.ndata['node_attr'][:,:3].T.unsqueeze(0).cpu().numpy()))).squeeze(0).T.cuda()\n",
        "          else:\n",
        "            g.ndata['node_attr_irc'][:,:3] = torch.Tensor(distort_jets(collinear_fill_jets(g.ndata['node_attr'][:,:3].T.unsqueeze(0).numpy()))).squeeze(0).T\n",
        "          pt, rapidity, phi = g.ndata['node_attr_irc'][:, 0], g.ndata['node_attr_irc'][:, 1], g.ndata['node_attr_irc'][:, 2]\n",
        "          mt = (pt**2+g.ndata['node_mass']**2).sqrt()\n",
        "          energy = mt*torch.cosh(rapidity)\n",
        "          px, py, pz = pt*torch.cos(phi), pt*torch.sin(phi), mt*torch.sinh(rapidity)\n",
        "          g.ndata['node_attr_irc'][:,3] =  mt\n",
        "          g.ndata['node_attr_irc'][:,4] = energy\n",
        "          g.ndata['node_attr_irc'][:,5] = px\n",
        "          g.ndata['node_attr_irc'][:,6] = py\n",
        "          g.ndata['node_attr_irc'][:,7] = pz\n",
        "\n",
        "  def has_cache(self):\n",
        "    if os.path.exists(self.graph_path) and os.path.exists(self.info_path):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def __len__(self,):\n",
        "    return len(self.graph_lists)\n",
        "\n",
        "  def augment_dataset(self, type, batched_graph, batch_size):\n",
        "    self.augment = True\n",
        "\n",
        "    if type == 'rationale':\n",
        "      return drop_nodes_prob_batch(batched_graph, batch_size), drop_nodes_prob_batch(batched_graph, batch_size)\n",
        "\n",
        "    if type == 'complement':\n",
        "      return drop_nodes_cp_batch(batched_graph, batch_size)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.spectral_augmentation:\n",
        "      g1 = self.graph_lists[idx]\n",
        "      g2 = self.spectral_graph_lists[idx]\n",
        "      if self._transform is not None:\n",
        "        g1 = self._transform(g1)\n",
        "        g2 = self._transform(g2)\n",
        "      return g1, g2, self.graph_labels[idx]\n",
        "\n",
        "    else:\n",
        "      g = self.graph_lists[idx]\n",
        "      if self._transform is not None:\n",
        "        g = self._transform(g)\n",
        "      return g, self.graph_labels[idx]\n",
        "\n",
        "  @property\n",
        "  def num_classes(self):\n",
        "    return int(self.num_labels)"
      ],
      "metadata": {
        "id": "Ym9AH6-SVLr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = ''\n",
        "jet_folder_path = ''\n",
        "\n",
        "qg_dataset = QuarkGluonGraphDataset(dataset_name='Quark Gluon', raw_dir=main_dir, save_dir='/content',\n",
        "                                    data_folder_name=jet_folder_path, datafile_name=jet_file_path, labelsfile_name=jet_file_path,\n",
        "                                    datatype='particles', dataset_size=10000, nodes_per_graph=10, spectral_augmentation=False, irc_safety_aug=True,\n",
        "                                   device='cuda')"
      ],
      "metadata": {
        "id": "lTcJqvhaVYQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import run\n",
        "import torch.multiprocessing as mp\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    world_size = 4\n",
        "    mp.start_processes(run.run, args=(world_size, 50, 'ParticleNet', 10), nprocs=world_size, start_method=\"spawn\")"
      ],
      "metadata": {
        "id": "8rhlQ9bV8Xj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "checkpoint = torch.load(main_dir+'/particle_net_model.checkpoint', weights_only=True)\n",
        "\n",
        "trained_gnn_state = dict()\n",
        "for k in checkpoint.keys():\n",
        "    if 'module.gnn.pn' in k:\n",
        "        new_k = k.replace('module.gnn.pn.', 'gnn.')\n",
        "        trained_gnn_state[new_k] = checkpoint[k]"
      ],
      "metadata": {
        "id": "pvHnKjmm8e94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from run import ParticleNetTagger1Path\n",
        "\n",
        "gnn = ParticleNetTagger1Path(8,2)\n",
        "gnn.load_state_dict(trained_gnn_state)"
      ],
      "metadata": {
        "id": "SBCwAmoA8hPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_per_graph_original = 10\n",
        "test_dataset = QuarkGluonGraphDataset(dataset_name='Quark Gluon', raw_dir=main_dir, save_dir='/content',\n",
        "                                    data_folder_name=jet_folder_path, datafile_name=jet_file_path, labelsfile_name=jet_file_path,\n",
        "                                    datatype='particles', dataset_size=7000, nodes_per_graph=nodes_per_graph_original, spectral_augmentation=False)"
      ],
      "metadata": {
        "id": "geSjHgy0YDwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = torch.tensor(np.arange(2000,7000).astype('int32'))\n",
        "test_sampler = SubsetRandomSampler(test_samples)\n",
        "\n",
        "test_dataloader = test_dataloader = GraphDataLoader(\n",
        "    test_dataset, sampler=test_sampler, batch_size=500, drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "2NHBtA-yYGK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embds = torch.Tensor([])\n",
        "cls_labels = torch.Tensor([])\n",
        "\n",
        "for batched_graph, labels in test_dataloader:\n",
        "  graphs = []\n",
        "  unbatched_graph = dgl.unbatch(batched_graph)\n",
        "  for graph in unbatched_graph:\n",
        "    graphs.append(dgl.add_self_loop(graph))\n",
        "  batched_graph = dgl.batch(graphs)\n",
        "  batch_t = torch.arange(0, batched_graph.batch_size).reshape(-1,1).expand(batched_graph.batch_size, test_dataset.nodes_per_graph).reshape(-1,)\n",
        "\n",
        "  ## For custom GNN\n",
        "  # cls_emb = gnn.forward(batched_graph.ndata[\"node_attr\"].float(), torch.stack(batched_graph.edges()), batched_graph.edata[\"edge_attr\"].float())\n",
        "\n",
        "  ## For ParticleNet\n",
        "  pf_feats = batched_graph.ndata[\"node_attr\"].reshape(len(unbatched_graph), nodes_per_graph_original, -1).float()\n",
        "  points = pf_feats[:,:,1:3]\n",
        "  cls_emb = gnn.forward(points.reshape(points.shape[0], points.shape[2], points.shape[1])\n",
        "                     , pf_feats.reshape(pf_feats.shape[0], pf_feats.shape[2], pf_feats.shape[1]), None)\n",
        "  cls_emb = cls_emb.reshape(cls_emb.shape[0], cls_emb.shape[2], cls_emb.shape[1])\n",
        "  cls_emb = cls_emb.reshape(cls_emb.shape[0]*cls_emb.shape[1], cls_emb.shape[2])\n",
        "\n",
        "  # cls_emb = batched_graph.ndata[\"node_attr\"].float()\n",
        "  cls_emb = global_mean_pool(cls_emb, batch_t)\n",
        "  cls_embds = torch.cat((cls_embds, cls_emb.detach()), 0)     #cls_emb\n",
        "  cls_labels = torch.cat((cls_labels, labels))"
      ],
      "metadata": {
        "id": "zG95SQuHYHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_epochs = 1000\n",
        "cls_train_data = cls_embds[ : int(0.8*len(cls_embds))]\n",
        "targets = cls_labels[ : int(0.8*len(cls_embds))]\n",
        "cls_test_data = cls_embds[int(0.8*len(cls_embds)) : ]\n",
        "testtargets = cls_labels[int(0.8*len(cls_embds)) : ]"
      ],
      "metadata": {
        "id": "bpwPGFaRYJH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/sdogsq/LorentzNet-release/blob/main/scripts/QGTaggingROC/ROC.py\n",
        "\n",
        "# Function that takes the labels and score of the positive class\n",
        "# (top class) and returns a ROC curve, as well as the signal efficiency\n",
        "# and background rejection at a given targe signal efficiency, defaults\n",
        "# to 0.3\n",
        "\n",
        "def buildROC(labels, score, targetEff=[0.3,0.5]):\n",
        "    if not isinstance(targetEff, list):\n",
        "        targetEff = [targetEff]\n",
        "    fpr, tpr, threshold = metrics.roc_curve(labels, score)\n",
        "    idx = [np.argmin(np.abs(tpr - Eff)) for Eff in targetEff]\n",
        "    eB, eS = fpr[idx], tpr[idx]\n",
        "    return fpr, tpr, threshold, eB, eS\n",
        "\n",
        "### Reference - https://github.com/bmdillon/JetCLR/blob/main/scripts/modules/perf_eval.py\n",
        "\n",
        "def find_nearest( array, value ):\n",
        "    array = np.asarray( array )\n",
        "    idx = ( np.abs( array-value ) ).argmin()\n",
        "    return array[idx]\n",
        "\n",
        "def get_perf_stats( labels, measures ):\n",
        "    measures = np.nan_to_num( measures )\n",
        "    auc = metrics.roc_auc_score( labels, measures )\n",
        "    fpr,tpr,thresholds = metrics.roc_curve( labels, measures )\n",
        "    fpr2 = [ fpr[i] for i in range( len( fpr ) ) if tpr[i]>=0.5]\n",
        "    tpr2 = [ tpr[i] for i in range( len( tpr ) ) if tpr[i]>=0.5]\n",
        "    try:\n",
        "        imtafe = np.nan_to_num( 1 / fpr2[ list( tpr2 ).index( find_nearest( list( tpr2 ), 0.5 ) ) ] )\n",
        "    except:\n",
        "        imtafe = 1\n",
        "    return auc, imtafe"
      ],
      "metadata": {
        "id": "c9DjNmLGZHe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(128,1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "\n",
        "\n",
        "def train_classifier(cls_epochs, classifier, cls_train_data, labels, cls_test_data, testlabels):\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "  cls_loss = []\n",
        "  cls_accuracy = []\n",
        "  test_cls_loss = []\n",
        "  test_cls_accuracy = []\n",
        "\n",
        "  for ce in range(cls_epochs):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    t_correct = 0\n",
        "    t_total = 0\n",
        "    optimizer.zero_grad()\n",
        "    outputs = classifier(cls_train_data)\n",
        "    loss = torch.nn.BCELoss()(outputs,labels.view(-1,1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    predicted = np.round(outputs.cpu().detach().numpy())\n",
        "    total += labels.size(0)\n",
        "    correct += np.sum(torch.eq(torch.Tensor(predicted), labels.view(-1,1)).cpu().detach().numpy())\n",
        "    accuracy = 100 * correct / total\n",
        "    cls_loss.append(loss.cpu().detach().numpy())\n",
        "    cls_accuracy.append(accuracy)\n",
        "    testoutputs = classifier(cls_test_data)\n",
        "    testloss = torch.nn.BCELoss()(testoutputs, testlabels.view(-1,1))\n",
        "    testpredicted = np.round(testoutputs.cpu().detach().numpy())\n",
        "    t_total += testlabels.size(0)\n",
        "    t_correct += np.sum(torch.eq(torch.Tensor(testpredicted), testlabels.view(-1,1)).cpu().detach().numpy())\n",
        "    testaccuracy = 100 * t_correct / t_total\n",
        "    test_cls_loss.append(testloss.cpu().detach().numpy())  #np.mean(e_loss)\n",
        "    test_cls_accuracy.append(testaccuracy)\n",
        "    if epochs % 50 == 0:\n",
        "      print(f'Epochs : {ce} ; Loss : {loss.cpu().detach().numpy()} ; Accuracy : {accuracy} ; Test Loss : {testloss} ; Test accuracy : {testaccuracy}' )   #np.mean(e_loss)\n",
        "\n",
        "  testoutputs = classifier(cls_test_data)\n",
        "  # fpr, tpr, thresholds = metrics.roc_curve(testlabels.cpu(), testoutputs.cpu().detach().numpy())\n",
        "  fpr, tpr, threshold, eB, eS = buildROC(testlabels.cpu(), testoutputs.cpu().detach().numpy())\n",
        "  auc = metrics.auc(fpr, tpr)\n",
        "  f1_score = metrics.f1_score(testlabels.cpu(), np.round(testoutputs.cpu().detach().numpy()), average='macro')\n",
        "  _ , imtafe = get_perf_stats(testlabels.cpu(), testoutputs.cpu().detach().numpy())\n",
        "  return cls_loss, cls_accuracy, test_cls_loss, test_cls_accuracy, fpr, tpr, auc, eB, eS, f1_score, imtafe\n",
        "\n",
        "\n",
        "cls_loss, cls_accuracy, test_cls_loss, test_cls_accuracy, fpr, tpr, auc, eB, eS, f1_score, imtafe = train_classifier(cls_epochs, classifier, cls_train_data, targets, cls_test_data, testtargets)"
      ],
      "metadata": {
        "id": "FtrCG9eSYLSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cls_loss, label='Train Loss')\n",
        "plt.plot(test_cls_loss, label='Val Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "RPawpog4YODT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cls_accuracy, label='Train Accuracy')\n",
        "plt.plot(test_cls_accuracy, label='Val Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3Xxf7zLSZQfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as mpl_patches\n",
        "\n",
        "font = {'family': 'serif',\n",
        "        'color':  'darkblue',\n",
        "        'weight': 'normal',\n",
        "        'size': 12,\n",
        "        }\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('FPR', fontsize=14)\n",
        "plt.ylabel('TPR', fontsize=14)\n",
        "# plt.text(0.9, 0.9, 'AUC = '+str(auc.round(6)), fontdict=font, wrap=True)\n",
        "# create a list with two empty handles (or more if needed)\n",
        "handles = [mpl_patches.Rectangle((0, 0), 1, 1, fc=\"white\", ec=\"white\",\n",
        "                                 lw=0, alpha=0)]\n",
        "\n",
        "# create the corresponding number of labels (= the text you want to display)\n",
        "labels = []\n",
        "labels.append(\"AUC = \"+str(auc.round(6)))\n",
        "\n",
        "# create the legend, supressing the blank space of the empty line symbol and the\n",
        "# padding between symbol and label by setting handlelenght and handletextpad\n",
        "plt.legend(handles, labels, loc='best', fontsize='large',\n",
        "          fancybox=True, framealpha=0.7,\n",
        "          handlelength=0, handletextpad=0)"
      ],
      "metadata": {
        "id": "1d5JA8Q0ZSRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy : ', test_cls_accuracy[-1])\n",
        "print('AUC : ', auc)\n",
        "print('F1 score : ', f1_score)"
      ],
      "metadata": {
        "id": "dxQgZz78ZU0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dRDnhWFO8kNV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}