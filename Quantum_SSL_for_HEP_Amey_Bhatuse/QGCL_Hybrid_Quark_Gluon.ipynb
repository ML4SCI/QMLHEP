{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install torch==2.2.0\n",
        "!pip install torch_geometric\n",
        "!pip install particle\n",
        "!pip install pennylane\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install torchvision==0.17.0\n",
        "!pip install qiskit==0.46.0\n",
        "!pip install torchquantum\n",
        "!pip install qiskit-ibm-runtime==0.18.0\n",
        "!pip install qiskit-aer==0.13.2\n",
        "!pip install dgl -f https://data.dgl.ai/wheels/cu121/repo.html\n",
        "!pip install dglgo -f https://data.dgl.ai/wheels-test/repo.html\n",
        "!pip install energyflow\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "-LvCO-zQ2nmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "gt_ntIZ7S00B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchdata\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchquantum as tq\n",
        "from torchquantum.layer.entanglement.op2_layer import Op2QAllLayer\n",
        "from torchquantum.layer.layers.layers import Op1QAllLayer, Op2QAllLayer\n",
        "from torchquantum.measurement import measure\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import scipy\n",
        "import warnings\n",
        "import dgl\n",
        "from dgl.data import DGLDataset\n",
        "from dgl.dataloading import GraphDataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import scipy.sparse as sp\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "import pickle\n",
        "import multiprocessing\n",
        "import joblib\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.utils import add_self_loops, degree, softmax\n",
        "import torch.optim as optim\n",
        "\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "\n",
        "from particle import Particle\n",
        "import pennylane as qml\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "kDgEhsePTK1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Download the jets along with storing them in a folder for future use, eliminating the need to download them again\n",
        "# main_dir = ''\n",
        "\n",
        "# import energyflow\n",
        "# data = energyflow.qg_jets.load(num_data=20000, pad=True, ncol=4, generator='pythia',\n",
        "#                         with_bc=False, cache_dir=main_dir+'/energyflow')"
      ],
      "metadata": {
        "id": "aCmapYGUTWG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jet_file_path = ''\n",
        "data = np.load(jet_file_path)"
      ],
      "metadata": {
        "id": "IZJZuPAvUJ-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/ML4SCI/QMLHEP/blob/main/Quantum_GNN_for_HEP_Roy_Forestano/utils/preprocess.py\n",
        "\n",
        "def preprocess_fixed_nodes(x_data,y_data,nodes_per_graph=10): #,masses):\n",
        "    print('--- Finding All Unique Particles ---')\n",
        "    unique_particles = np.unique(x_data[:,:,3])\n",
        "    x_data = torch.tensor(x_data)\n",
        "    y_data = torch.tensor(y_data)\n",
        "    print()\n",
        "    print('--- Inserting Masses ---')\n",
        "    masses = torch.zeros((x_data.shape[0],x_data.shape[1]))\n",
        "    for i,particle in tqdm(enumerate(unique_particles)):\n",
        "        if particle!=0:\n",
        "            mass = Particle.from_pdgid(particle).mass/1000\n",
        "            inds = torch.where(particle==x_data[:,:,3])\n",
        "            masses[inds]=mass # GeV\n",
        "    print()\n",
        "    print('--- Calculating Momenta and Energies ---')\n",
        "    #theta = torch.arctan(torch.exp(-X[:,:,1]))*2 # polar angle\n",
        "    pt        = x_data[:,:,0]     # transverse momentum\n",
        "    rapidity  = x_data[:,:,1]     # rapidity\n",
        "    phi       = x_data[:,:,2]     # azimuthal angle\n",
        "\n",
        "    mt        = (pt**2+masses**2).sqrt() # Transverse mass\n",
        "    energy    = mt*torch.cosh(rapidity) # Energy per multiplicity bin\n",
        "    e_per_jet = energy.sum(axis=1)  # total energy per jet summed across multiplicity bins\n",
        "\n",
        "    px = pt*torch.cos(phi)  # momentum in x\n",
        "    py = pt*torch.sin(phi)  # momentum in y\n",
        "    pz = mt*torch.sinh(rapidity)  # momentum in z\n",
        "\n",
        "    # three momentum\n",
        "    p  = torch.cat(( px[:,:,None],\n",
        "                     py[:,:,None],\n",
        "                     pz[:,:,None]), dim=2 )\n",
        "\n",
        "    p_per_jet        = (p).sum(axis=1)  # total componet momentum per jet\n",
        "    pt_per_Mbin      = (p_per_jet[:,:2]**2).sum(axis=1).sqrt()  # transverse momentum per jet\n",
        "    mass_per_jet     = (e_per_jet**2-(p_per_jet**2).sum(axis=1)).sqrt() # mass per jet\n",
        "    rapidity_per_jet = torch.log( (e_per_jet+p_per_jet[:,2])/(e_per_jet-p_per_jet[:,2]) )/2  # rapidity per jet from analytical formula\n",
        "    end_multiplicity_indx_per_jet = (pt!=0).sum(axis=1).int() # see where the jet (graph) ends\n",
        "\n",
        "    x_data = torch.cat( ( x_data[:,:,:3],\n",
        "                          x_data[:,:,4:],\n",
        "                          masses[:,:,None],\n",
        "                          energy[:,:,None],\n",
        "                          p), dim=2)\n",
        "\n",
        "    x_data_max = (x_data.max(dim=1).values).max(dim=0).values\n",
        "    x_data = x_data/x_data_max\n",
        "\n",
        "    print()\n",
        "    print('--- Calculating Edge Tensors ---')\n",
        "    N = x_data[:,0,3].shape[0]  # number of jets (graphs)\n",
        "    M = nodes_per_graph #x_data[0,:,3].shape[0]  # number of max multiplicty\n",
        "    connections = nodes_per_graph\n",
        "    edge_tensor = torch.zeros((N,M,M))\n",
        "    edge_indx_tensor = torch.zeros((N,2,connections*(connections-1) )) # M*(connections-1) is the max number of edges we allow per jet\n",
        "    edge_attr_matrix = torch.zeros((N,connections*(connections-1),1))\n",
        "#     fixed_edges_list = torch.tensor([ [i,j] for i in range(connections) for j in range(connections) if i!=j]).reshape(2,90)\n",
        "\n",
        "    for jet in tqdm(range(N)):\n",
        "        stop_indx = end_multiplicity_indx_per_jet[jet] #connections # stop finding edges once we hit zeros -> when we hit 10\n",
        "        if end_multiplicity_indx_per_jet[jet]>=connections:\n",
        "            for m in range(connections):\n",
        "#                 inds_edge = np.argsort((energy[jet,m]+energy[jet,:stop_indx])**2-torch.sum((p[jet,m,:stop_indx]+p[jet,:stop_indx,:])**2,axis=1))[:connections]\n",
        "#                 edge_tensor[jet,m,:] = (energy[jet,m]+energy[jet,:connections])**2-torch.sum((p[jet,m,:]+p[jet,:connections,:])**2,axis=1)\n",
        "#                 edge_tensor[jet,m,m] = 0.\n",
        "#                 edge_tensor[jet,m,m]=((energy[jet,m]+energy[jet,m])**2-torch.sum((p[jet,m,:]+p[jet,m,:])**2,axis=0))\n",
        "                # inds_edge = torch.sqrt( (phi[jet,m]-phi[jet,:])**2 + (rapidity[jet,m]-rapidity[jet,:])**2 ).argsort()[:connections]\n",
        "                # edge_tensor[jet,m,:] = torch.sqrt( (phi[jet,m]-phi[jet,inds_edge])**2 + (rapidity[jet,m]-rapidity[jet,inds_edge])**2 )\n",
        "                edge_tensor[jet,m,:] = torch.sqrt( (phi[jet,m]-phi[jet,:connections])**2 + (rapidity[jet,m]-rapidity[jet,:connections])**2 )\n",
        "#                 inds_edge = np.argsort( (energy[jet,m]+energy[jet,:stop_indx])**2-torch.sum((p[jet,m,:stop_indx]+p[jet,:stop_indx,:])**2,axis=1) )[:connections]\n",
        "#                 edge_tensor[jet,m,inds_edge] = (energy[jet,m]+energy[jet,inds_edge])**2-torch.sum((p[jet,m,:]+p[jet,inds_edge,:])**2,axis=1)\n",
        "            edges_exist_at = torch.where(edge_tensor[jet,:,:].abs()>0)\n",
        "\n",
        "#             edge_indx_tensor[jet,:,:(edge_tensor[jet,:,:].abs()>0).sum()] = fixed_edges_list\n",
        "            edge_indx_tensor[jet,:,:(edge_tensor[jet,:,:].abs()>0).sum()] = torch.cat((edges_exist_at[0][None,:],edges_exist_at[1][None,:]),dim=0).reshape((2,edges_exist_at[0].shape[0]))\n",
        "            edge_attr_matrix[jet,:(edge_tensor[jet,:,:].abs()>0).sum(),0]  =  edge_tensor[jet,edges_exist_at[0],edges_exist_at[1]].flatten()\n",
        "\n",
        "    end_edges_indx_per_jet = (edge_attr_matrix!=0).sum(axis=1).int()\n",
        "    keep_inds =  torch.where(end_edges_indx_per_jet>=connections)[0]\n",
        "\n",
        "    edge_tensor = edge_tensor/edge_tensor.max()\n",
        "    edge_attr_matrix = edge_attr_matrix/edge_attr_matrix.max()\n",
        "\n",
        "    graph_help = torch.cat( ( (energy.max(axis=1).values/e_per_jet).reshape(x_data[:,0,3].shape[0],1),\n",
        "                              (mass_per_jet).reshape(x_data[:,0,3].shape[0],1),\n",
        "                              (end_multiplicity_indx_per_jet).reshape(x_data[:,0,3].shape[0],1).int(),\n",
        "                              (end_edges_indx_per_jet).reshape(x_data[:,0,3].shape[0],1).int() ), dim=1)\n",
        "\n",
        "    return x_data[keep_inds,:nodes_per_graph], y_data[keep_inds].long(), edge_tensor[keep_inds], edge_indx_tensor[keep_inds].long(), edge_attr_matrix[keep_inds], graph_help[keep_inds], masses"
      ],
      "metadata": {
        "id": "lhY05LBQUeYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/bmdillon/JetCLR/blob/main/scripts/modules/jet_augs.py\n",
        "\n",
        "def distort_jets( batch, strength=0.1, pT_clip_min=0.1 ):\n",
        "    '''\n",
        "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
        "    dim 1 ordering: (pT, eta, phi)\n",
        "    Output: batch of jets with each constituents position shifted independently, shifts drawn from normal with mean 0, std strength/pT, same shape as input\n",
        "    '''\n",
        "    pT = batch[:,0]   # (batchsize, n_constit)\n",
        "    shift_eta = np.nan_to_num( strength * np.random.randn(batch.shape[0], batch.shape[2]) / pT.clip(min=pT_clip_min), posinf = 0.0, neginf = 0.0 )# * mask\n",
        "    shift_phi = np.nan_to_num( strength * np.random.randn(batch.shape[0], batch.shape[2]) / pT.clip(min=pT_clip_min), posinf = 0.0, neginf = 0.0 )# * mask\n",
        "    shift = np.stack( [ np.zeros( (batch.shape[0], batch.shape[2]) ), shift_eta, shift_phi ], 1)\n",
        "    return batch + shift\n",
        "\n",
        "def collinear_fill_jets( batch ):\n",
        "    '''\n",
        "    Input: batch of jets, shape (batchsize, 3, n_constit)\n",
        "    dim 1 ordering: (pT, eta, phi)\n",
        "    Output: batch of jets with collinear splittings, the function attempts to fill as many of the zero-padded args.nconstit\n",
        "    entries with collinear splittings of the constituents by splitting each constituent at most once, same shape as input\n",
        "    '''\n",
        "    batchb = batch.copy()\n",
        "    nc = batch.shape[2]\n",
        "    nzs = np.array( [ np.where( batch[:,0,:][i]>0.0)[0].shape[0] for i in range(len(batch)) ] )\n",
        "\n",
        "    for k in range(len(batch)):\n",
        "        nzs1 = np.max( [ nzs[k], int(nc/2) ] )\n",
        "        zs1 = int(nc-nzs1)\n",
        "        els = np.random.choice( np.linspace(0,nzs1-1,nzs1), size=zs1, replace=False )\n",
        "        rs = np.random.uniform( size=zs1 )\n",
        "        for j in range(zs1):\n",
        "            batchb[k,0,int(els[j])] = rs[j]*batch[k,0,int(els[j])]\n",
        "            batchb[k,0,int(nzs[k]+j)] = (1-rs[j])*batch[k,0,int(els[j])]\n",
        "            batchb[k,1,int(nzs[k]+j)] = batch[k,1,int(els[j])]\n",
        "            batchb[k,2,int(nzs[k]+j)] = batch[k,2,int(els[j])]\n",
        "\n",
        "    return batchb"
      ],
      "metadata": {
        "id": "-uyS1h4zU4XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuarkGluonGraphDataset(dgl.data.dgl_dataset.DGLDataset):\n",
        "\n",
        "  def __init__(self, dataset_name, raw_dir, save_dir, data_folder_name, datafile_name, labelsfile_name, datatype='particles', dataset_size=12500,\n",
        "               nodes_per_graph = 5, spectral_augmentation=False, irc_safety_aug=False, url=None, hash_key=..., force_reload=False, verbose=False, transform=None,\n",
        "              device='cpu'):\n",
        "    self.data_folder = data_folder_name\n",
        "    self.datafile_name = datafile_name\n",
        "    self.labelsfile_name = labelsfile_name\n",
        "    self.datatype = datatype\n",
        "    self.nodes_per_graph = nodes_per_graph\n",
        "    self.spectral_augmentation = spectral_augmentation\n",
        "    self.drop_ra_nodes = False\n",
        "    self.drop_cp_nodes = False\n",
        "    self.aug_ratio = None\n",
        "    self.irc_safety_aug = irc_safety_aug\n",
        "    self.device = device\n",
        "    self.dataset_size = dataset_size\n",
        "    self.augment = False\n",
        "    self.nodes_per_aug_graph = None\n",
        "    super().__init__(dataset_name, url, raw_dir, save_dir, hash_key, force_reload, verbose, transform)\n",
        "\n",
        "  @property\n",
        "  def data_folder_name(self):\n",
        "    return self.data_folder\n",
        "\n",
        "  @property\n",
        "  def raw_path(self):\n",
        "    return os.path.join(self.raw_dir, self.data_folder_name)\n",
        "\n",
        "  @property\n",
        "  def save_path(self):\n",
        "    return os.path.join(self.save_dir, self.data_folder_name)\n",
        "\n",
        "  @property\n",
        "  def graph_path(self):\n",
        "    return os.path.join(self.save_path, 'graphs_and_labels')\n",
        "\n",
        "  @property\n",
        "  def info_path(self):\n",
        "    return os.path.join(self.save_path, 'graphs_and_labels')\n",
        "\n",
        "  def load(self):\n",
        "    graphs, label_dict = dgl.load_graphs(str(self.graph_path))\n",
        "    info_dict = dgl.data.utils.load_info(str(self.info_path))\n",
        "\n",
        "    self.graph_lists = graphs\n",
        "    self.graph_labels = label_dict[\"labels\"]\n",
        "    self.max_num_node = info_dict[\"max_num_node\"]\n",
        "    self.num_labels = info_dict[\"num_labels\"]\n",
        "\n",
        "  # def save(self,):\n",
        "  #   label_dict = {\"labels\": self.graph_labels}\n",
        "  #   info_dict = {\n",
        "  #           \"max_num_node\": self.max_num_node,\n",
        "  #           \"num_labels\": self.num_labels,\n",
        "  #       }\n",
        "  #   dgl.save_graphs(str(self.graph_path), self.graph_lists, label_dict)\n",
        "  #   dgl.data.utils.save_info(str(self.info_path), info_dict)\n",
        "\n",
        "  def process(self,):\n",
        "    data = np.load(os.path.join(self.raw_path, self.datafile_name))\n",
        "    X = data['X']\n",
        "    y = data['y']\n",
        "    X_l, y_l = [], []\n",
        "    i = 0\n",
        "\n",
        "    while len(X_l)!=self.dataset_size:\n",
        "        if np.unique(X[i].sum(axis=1).nonzero()).shape[0] >= self.nodes_per_graph:\n",
        "            sorted_inds = np.argsort(X[i,:,0])[::-1]\n",
        "            x = X[i][sorted_inds]\n",
        "            X_l.append(x[:self.nodes_per_graph, :])\n",
        "            y_l.append(y[i])\n",
        "        i += 1\n",
        "    X = np.array(X_l)\n",
        "    y = np.array(y_l)\n",
        "\n",
        "\n",
        "    if self.datatype == 'particles':\n",
        "      self.graph_lists = []\n",
        "      self.rationale_augmented_graph_lists_1 = []\n",
        "      self.rationale_augmented_graph_lists_2 = []\n",
        "      self.complement_augmented_graph_lists = []\n",
        "      x_data_proc, y_data_proc, edge_tensor, edge_indx_tensor, edge_attr_matrix, graph_help, masses = preprocess_fixed_nodes(X,y,nodes_per_graph = self.nodes_per_graph) #,masses[:N])\n",
        "      self.max_num_node = x_data_proc.shape[1]\n",
        "      self.graph_labels = y_data_proc\n",
        "      self.num_labels = y_data_proc.shape[0]\n",
        "\n",
        "      print('--- Creating graphs ---')\n",
        "      for i in tqdm(range(x_data_proc.shape[0])):\n",
        "        g = dgl.graph((edge_indx_tensor[i][0], edge_indx_tensor[i][1]))\n",
        "        g.ndata['node_attr'] = x_data_proc[i]\n",
        "        g.ndata['node_indices'] = torch.arange(x_data_proc[i].shape[0]).reshape(-1,1)\n",
        "        g.ndata['node_mass'] = masses[i][:self.nodes_per_graph]\n",
        "        g.edata['edge_attr'] = edge_attr_matrix[i].view(-1,)\n",
        "        g.to(self.device)\n",
        "        self.graph_lists.append(g)\n",
        "        self.rationale_augmented_graph_lists_1.append(g)\n",
        "        self.rationale_augmented_graph_lists_2.append(g)\n",
        "        self.complement_augmented_graph_lists.append(g)\n",
        "\n",
        "      if self.spectral_augmentation:\n",
        "        self.spectral_graph_lists = []\n",
        "        print('--- Creating spectral graphs ---')\n",
        "        for i in tqdm(range(x_data_proc.shape[0])):\n",
        "          g = SpectralGraph((edge_indx_tensor[i][0], edge_indx_tensor[i][1]), theta=0.1, delta_origin=0.05, edge_weights_matrix=edge_tensor[i])\n",
        "          g.ndata['node_attr'] = x_data_proc[i]\n",
        "          g.edata['edge_attr'] = edge_attr_matrix[i].view(-1,)\n",
        "          self.spectral_graph_lists.append(g)\n",
        "        # print(self.graph_lists)\n",
        "\n",
        "      if self.irc_safety_aug:\n",
        "        for idx in range(len(self.graph_lists)):\n",
        "          g = self.graph_lists[idx]\n",
        "          g.ndata['node_attr_irc'] = g.ndata['node_attr'].clone()\n",
        "          if self.device=='cuda':\n",
        "            g.ndata['node_attr_irc'][:,:3] = torch.Tensor(distort_jets(collinear_fill_jets(g.ndata['node_attr'][:,:3].T.unsqueeze(0).cpu().numpy()))).squeeze(0).T.cuda()\n",
        "          else:\n",
        "            g.ndata['node_attr_irc'][:,:3] = torch.Tensor(distort_jets(collinear_fill_jets(g.ndata['node_attr'][:,:3].T.unsqueeze(0).numpy()))).squeeze(0).T\n",
        "          pt, rapidity, phi = g.ndata['node_attr_irc'][:, 0], g.ndata['node_attr_irc'][:, 1], g.ndata['node_attr_irc'][:, 2]\n",
        "          mt = (pt**2+g.ndata['node_mass']**2).sqrt()\n",
        "          energy = mt*torch.cosh(rapidity)\n",
        "          px, py, pz = pt*torch.cos(phi), pt*torch.sin(phi), mt*torch.sinh(rapidity)\n",
        "          g.ndata['node_attr_irc'][:,3] =  mt\n",
        "          g.ndata['node_attr_irc'][:,4] = energy\n",
        "          g.ndata['node_attr_irc'][:,5] = px\n",
        "          g.ndata['node_attr_irc'][:,6] = py\n",
        "          g.ndata['node_attr_irc'][:,7] = pz\n",
        "\n",
        "  def has_cache(self):\n",
        "    if os.path.exists(self.graph_path) and os.path.exists(self.info_path):\n",
        "      return True\n",
        "    return False\n",
        "\n",
        "  def __len__(self,):\n",
        "    return len(self.graph_lists)\n",
        "\n",
        "  def augment_dataset(self, type, batched_graph, batch_size):\n",
        "    self.augment = True\n",
        "\n",
        "    if type == 'rationale':\n",
        "      return drop_nodes_prob_batch(batched_graph, batch_size), drop_nodes_prob_batch(batched_graph, batch_size)\n",
        "\n",
        "    if type == 'complement':\n",
        "      return drop_nodes_cp_batch(batched_graph, batch_size)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.spectral_augmentation:\n",
        "      g1 = self.graph_lists[idx]\n",
        "      g2 = self.spectral_graph_lists[idx]\n",
        "      if self._transform is not None:\n",
        "        g1 = self._transform(g1)\n",
        "        g2 = self._transform(g2)\n",
        "      return g1, g2, self.graph_labels[idx]\n",
        "\n",
        "    else:\n",
        "      g = self.graph_lists[idx]\n",
        "      if self._transform is not None:\n",
        "        g = self._transform(g)\n",
        "      return g, self.graph_labels[idx]\n",
        "\n",
        "  @property\n",
        "  def num_classes(self):\n",
        "    return int(self.num_labels)"
      ],
      "metadata": {
        "id": "Ym9AH6-SVLr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = ''\n",
        "jet_folder_path = ''\n",
        "\n",
        "qg_dataset = QuarkGluonGraphDataset(dataset_name='Quark Gluon', raw_dir=main_dir, save_dir='/content',\n",
        "                                    data_folder_name=jet_folder_path, datafile_name=jet_file_path, labelsfile_name=jet_file_path,\n",
        "                                    datatype='particles', dataset_size=10000, nodes_per_graph=10, spectral_augmentation=False, irc_safety_aug=True,\n",
        "                                   device='cuda')"
      ],
      "metadata": {
        "id": "lTcJqvhaVYQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN_imp_estimator(torch.nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    Args:\n",
        "        num_layer (int): the number of GNN layers\n",
        "        emb_dim (int): dimensionality of embeddings\n",
        "        JK (str): last, concat, max or sum.\n",
        "        max_pool_layer (int): the layer from which we use max pool rather than add pool for neighbor aggregation\n",
        "        drop_ratio (float): dropout rate\n",
        "        gnn_type: gin, gcn, graphsage, gat\n",
        "\n",
        "    Output:\n",
        "        node representations\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_layer, emb_dim, in_dim, JK=\"last\", drop_ratio=0):\n",
        "        super(GNN_imp_estimator, self).__init__()\n",
        "        self.num_layer = num_layer\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.JK = JK\n",
        "\n",
        "        if self.num_layer < 2:\n",
        "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
        "\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.gnns = torch.nn.ModuleList()\n",
        "        # self.gnns.append(torch_geometric.nn.conv.GCNConv(in_dim, 32))\n",
        "        # self.gnns.append(torch_geometric.nn.conv.GCNConv(32, 16))\n",
        "        # self.gnns.append(torch_geometric.nn.conv.GCNConv(16, 8))\n",
        "\n",
        "        self.gnns.append(dgl.nn.GraphConv(in_dim, 32, weight=True, bias=True))\n",
        "        self.gnns.append(dgl.nn.GraphConv(32, 16, weight=True, bias=True))\n",
        "        self.gnns.append(dgl.nn.GraphConv(16, 8, weight=True, bias=True))\n",
        "\n",
        "        ###List of batchnorms\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "        self.batch_norms.append(torch.nn.BatchNorm1d(32))\n",
        "        self.batch_norms.append(torch.nn.BatchNorm1d(16))\n",
        "        self.batch_norms.append(torch.nn.BatchNorm1d(8))\n",
        "\n",
        "        self.linear = torch.nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, *argv):\n",
        "        if len(argv) == 4:\n",
        "            x, edge_index, edge_attr, batch = argv[0], argv[1], argv[2], argv[3]\n",
        "        elif len(argv) == 2:\n",
        "            graph, batch = argv[0], argv[1]\n",
        "            x, edge_index, edge_attr = graph.ndata['node_attr'], graph.edges(), graph.edata['edge_attr']\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        # x = self.x_embedding1(x[:, 0]) + self.x_embedding2(x[:, 1])\n",
        "\n",
        "        # h_list = [x]\n",
        "        # for layer in range(len(self.gnns)):\n",
        "        #     print('Layer : ',layer)\n",
        "        #     h = self.gnns[layer](graph, h_list[layer].float(), edge_weight=edge_attr)   #\n",
        "        #     h = self.batch_norms[layer](h)\n",
        "        #     if layer == len(self.gnns) - 1:\n",
        "        #         # remove relu for the last layer\n",
        "        #         h = F.dropout(h, self.drop_ratio, training=self.training)\n",
        "        #     else:\n",
        "        #         h = F.dropout(nn.ReLU()(h), self.drop_ratio, training=self.training)\n",
        "        #     h_list.append(h)\n",
        "\n",
        "        h0 = self.gnns[0](graph, x.float(), edge_weight=edge_attr)\n",
        "        h0 = self.batch_norms[0](h0)\n",
        "        h0 = F.dropout(nn.ReLU()(h0), self.drop_ratio, training=self.training)\n",
        "        h1 = self.gnns[1](graph, h0, edge_weight=edge_attr)\n",
        "        h1 = self.batch_norms[1](h1)\n",
        "        h1 = F.dropout(nn.ReLU()(h1), self.drop_ratio, training=self.training)\n",
        "        h2 = self.gnns[2](graph, h1, edge_weight=edge_attr)\n",
        "        h2 = self.batch_norms[2](h2)\n",
        "        h2 = F.dropout(h2, self.drop_ratio, training=self.training)\n",
        "\n",
        "        node_representation = h2  #h_list[-1]\n",
        "        node_representation = self.linear(node_representation)\n",
        "        node_representation = softmax(node_representation, batch)\n",
        "\n",
        "        return node_representation"
      ],
      "metadata": {
        "id": "iwFoIOb2XdjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    Args:\n",
        "        num_layer (int): the number of GNN layers\n",
        "        emb_dim (int): dimensionality of embeddings\n",
        "        JK (str): last, concat, max or sum.\n",
        "        max_pool_layer (int): the layer from which we use max pool rather than add pool for neighbor aggregation\n",
        "        drop_ratio (float): dropout rate\n",
        "        gnn_type: gin, gcn, graphsage, gat\n",
        "\n",
        "    Output:\n",
        "        node representations\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layer, in_dim, emb_dim, inter_dim, JK = \"last\", drop_ratio=0, gnn_type = \"gin\"):\n",
        "        super(GNN, self).__init__()\n",
        "        self.num_layer = num_layer\n",
        "        self.drop_ratio = drop_ratio\n",
        "        self.JK = JK\n",
        "\n",
        "        if self.num_layer < 2:\n",
        "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
        "\n",
        "        ###List of MLPs\n",
        "        self.gnns = torch.nn.ModuleList()\n",
        "        self.batch_norms = torch.nn.ModuleList()\n",
        "        # self.gnns.append(torch_geometric.nn.conv.GCNConv(in_dim, emb_dim))\n",
        "        if gnn_type == \"gin\":\n",
        "            self.gnns.append(GINConv(in_dim, aggr=\"add\"))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "        elif gnn_type == \"gcn\":\n",
        "            self.gnns.append(torch_geometric.nn.conv.GCNConv(in_dim, inter_dim))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "        elif gnn_type == \"gat\":\n",
        "            self.gnns.append(torch_geometric.nn.conv.GATConv(in_dim, inter_dim, heads=3, concat=False))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "        elif gnn_type == \"graphsage\":\n",
        "            self.gnns.append(GraphSAGEConv(in_dim))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "\n",
        "        for layer in range(num_layer):\n",
        "            if gnn_type == \"gin\":\n",
        "                self.gnns.append(GINConv(inter_dim, aggr=\"add\"))\n",
        "                self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "            elif gnn_type == \"gcn\":\n",
        "                self.gnns.append(torch_geometric.nn.conv.GCNConv(inter_dim, inter_dim))\n",
        "                self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "            elif gnn_type == \"gat\":\n",
        "                self.gnns.append(torch_geometric.nn.conv.GATConv(inter_dim, inter_dim, heads=3, concat=False))\n",
        "                self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "            elif gnn_type == \"graphsage\":\n",
        "                self.gnns.append(GraphSAGEConv(inter_dim))\n",
        "                self.batch_norms.append(torch.nn.BatchNorm1d(inter_dim))\n",
        "\n",
        "        if gnn_type == \"gin\":\n",
        "            self.gnns.append(GINConv(emb_dim, aggr=\"add\"))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
        "        elif gnn_type == \"gcn\":\n",
        "            self.gnns.append(torch_geometric.nn.conv.GCNConv(inter_dim, emb_dim))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
        "        elif gnn_type == \"gat\":\n",
        "            self.gnns.append(torch_geometric.nn.conv.GATConv(inter_dim, emb_dim, heads=3, concat=False))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
        "        elif gnn_type == \"graphsage\":\n",
        "            self.gnns.append(GraphSAGEConv(emb_dim))\n",
        "            self.batch_norms.append(torch.nn.BatchNorm1d(emb_dim))\n",
        "\n",
        "\n",
        "    #def forward(self, x, edge_index, edge_attr):\n",
        "    def forward(self, *argv):\n",
        "        if len(argv) == 3:\n",
        "            x, edge_index, edge_attr = argv[0], argv[1], argv[2]\n",
        "        elif len(argv) == 1:\n",
        "            # data = argv[0]\n",
        "            # x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "            graph = argv[0]\n",
        "            x, edge_index, edge_attr = graph.ndata['node_attr'], graph.edges(), graph.edata['edge_attr']\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        # print(x)\n",
        "        # h_list = [x]\n",
        "        h = x\n",
        "        for layer in range(self.num_layer+2):\n",
        "            h = self.gnns[layer](h, edge_index, edge_attr)   #h_list[layer]\n",
        "            h = self.batch_norms[layer](h)\n",
        "            #h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            if layer == self.num_layer+1:\n",
        "                #remove relu for the last layer\n",
        "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
        "            else:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            # h_list.append(h)\n",
        "\n",
        "        ### Different implementations of Jk-concat\n",
        "        if self.JK == \"concat\":\n",
        "            node_representation = torch.cat(h_list, dim = 1)\n",
        "        elif self.JK == \"last\":\n",
        "            node_representation = h   #h_list[-1]\n",
        "        elif self.JK == \"max\":\n",
        "            h_list = [h.unsqueeze(0) for h in h_list]\n",
        "            node_representation = torch.max(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
        "        elif self.JK == \"sum\":\n",
        "            h_list = [h.unsqueeze(0) for h in h_list]\n",
        "            node_representation = torch.sum(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
        "\n",
        "        return node_representation\n",
        "\n",
        "    def forward_gradc(self, *argv):\n",
        "        if len(argv) == 3:\n",
        "            x, edge_index, edge_attr = argv[0], argv[1], argv[2]\n",
        "        elif len(argv) == 1:\n",
        "            data = argv[0]\n",
        "            x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
        "        else:\n",
        "            raise ValueError(\"unmatched number of arguments.\")\n",
        "\n",
        "        h_list = [x]\n",
        "        for layer in range(self.num_layer+2):\n",
        "            h = self.gnns[layer](h_list[layer], edge_index, edge_attr)\n",
        "            h = self.batch_norms[layer](h)\n",
        "            h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            if layer == self.num_layer - 1:\n",
        "                #remove relu for the last layer\n",
        "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
        "            else:\n",
        "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
        "            h_list.append(h)\n",
        "\n",
        "        ### Different implementations of Jk-concat\n",
        "        if self.JK == \"concat\":\n",
        "            node_representation = torch.cat(h_list, dim = 1)\n",
        "        elif self.JK == \"last\":\n",
        "            node_representation = h_list[-1]\n",
        "        elif self.JK == \"max\":\n",
        "            h_list = [h.unsqueeze(0) for h in h_list]\n",
        "            node_representation = torch.max(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
        "        elif self.JK == \"sum\":\n",
        "            h_list = [h.unsqueeze(0) for h in h_list]\n",
        "            node_representation = torch.sum(torch.cat(h_list, dim = 0), dim = 0)[0]\n",
        "\n",
        "        return node_representation"
      ],
      "metadata": {
        "id": "zGuASKZx4Vxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BetterTorchLayer(torch.nn.Module):\n",
        "  def __init__(self, nodes_per_graph, num_layers):\n",
        "    super(BetterTorchLayer, self).__init__()\n",
        "\n",
        "    self.num_qubits = nodes_per_graph\n",
        "\n",
        "    self.dev = qml.device(\"default.qubit\", wires=self.num_qubits)\n",
        "    @partial(qml.batch_input, argnum=2)\n",
        "    @qml.qnode(self.dev, interface='torch')\n",
        "    def circuit(node_inputs, edge_inputs, params):\n",
        "\n",
        "      for i in range(node_inputs.shape[0]):\n",
        "        for j in range(node_inputs.shape[1]):\n",
        "          qml.RY(np.pi * node_inputs[i,j], wires=i)\n",
        "\n",
        "      if edge_inputs is not None:\n",
        "        # count = 0\n",
        "\n",
        "        for i in range(edge_inputs.shape[0]):\n",
        "          for j in range(i+1, edge_inputs.shape[1]):\n",
        "            qml.CRZ(edge_inputs[i,j], (i, j))\n",
        "\n",
        "      qml.StronglyEntanglingLayers(weights=params, wires=range(self.num_qubits))\n",
        "      return qml.probs(wires=range(self.num_qubits))\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.params_shape = qml.StronglyEntanglingLayers.shape(n_layers=self.num_layers, n_wires=self.num_qubits)\n",
        "    self.circuit = circuit\n",
        "    self.params = torch.nn.Parameter(torch.randn(self.params_shape))\n",
        "\n",
        "  # def forward(self, node_inputs, node_indices, edge_inputs):\n",
        "  #   return self.circuit(node_inputs, node_indices, edge_inputs, self.params)"
      ],
      "metadata": {
        "id": "5vY2ZUSdXePW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BetterBetterTorchLayer(torch.nn.Module):\n",
        "  def __init__(self, nodes_per_graph, num_layers, input_dim, device):\n",
        "    super(BetterBetterTorchLayer, self).__init__()\n",
        "    self.device =  device\n",
        "    self.num_qubits = nodes_per_graph\n",
        "    inputs = []\n",
        "    self.node_attr_count = 0\n",
        "    for q in range(self.num_qubits):\n",
        "      for d in range(input_dim):\n",
        "        inputs.append({'input_idx':self.node_attr_count, 'func':'ry', 'wires':[q]})\n",
        "        self.node_attr_count += 1\n",
        "    self.edge_attr_count = self.node_attr_count + 1\n",
        "    for q in range(self.num_qubits):\n",
        "      for e in range(q+1, self.num_qubits):\n",
        "        inputs.append({'input_idx':self.edge_attr_count, 'func':'crz', 'wires':[q,e]})\n",
        "        self.edge_attr_count += 1\n",
        "    self.edge_attr_count -= self.node_attr_count\n",
        "    self.encoder = tq.GeneralEncoder(inputs)\n",
        "    self.q_layers = tq.QuantumModuleList()\n",
        "    for layer in range(num_layers):\n",
        "      self.q_layers.append(\n",
        "                Op1QAllLayer(\n",
        "                    op=tq.RX, n_wires=self.num_qubits, has_params=True, trainable=True\n",
        "                )\n",
        "            )\n",
        "      self.q_layers.append(\n",
        "                Op1QAllLayer(\n",
        "                    op=tq.RY, n_wires=self.num_qubits, has_params=True, trainable=True\n",
        "                )\n",
        "            )\n",
        "      self.q_layers.append(\n",
        "                Op1QAllLayer(\n",
        "                    op=tq.RZ, n_wires=self.num_qubits, has_params=True, trainable=True\n",
        "                )\n",
        "            )\n",
        "      self.q_layers.append(\n",
        "                Op2QAllLayer(op=tq.CNOT, n_wires=self.num_qubits, jump = (layer+1)%(self.num_qubits-1), circular=True)\n",
        "            )\n",
        "    # self.measure = tq.MeasureAll(tq.PauliZ)\n",
        "\n",
        "\n",
        "  def forward(self, node_inputs, node_indices, edge_inputs):\n",
        "    qdev = tq.QuantumDevice(n_wires=self.num_qubits, bsz=node_inputs.shape[0], device=self.device, record_op=True)\n",
        "    self.encoder(qdev, torch.cat((node_inputs.reshape(node_inputs.shape[0],-1), edge_inputs.reshape(edge_inputs.shape[0],-1)), dim=1))\n",
        "    for l in range(len(self.q_layers)):\n",
        "      self.q_layers[l](qdev)\n",
        "    return torch.abs(qdev.get_states_1d())**2"
      ],
      "metadata": {
        "id": "rsfSJG-QXhw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QGNN_node_estimator(torch.nn.Module):\n",
        "  def __init__(self, nodes_per_graph, num_layers, input_dim=None, device='cpu'):\n",
        "    super(QGNN_node_estimator, self).__init__()\n",
        "    self.device = device\n",
        "    self.nodes_per_graph = nodes_per_graph\n",
        "    self.num_layers = num_layers\n",
        "    self.input_dim = input_dim\n",
        "    # self.quantum_nn = BetterTorchLayer(self.nodes_per_graph, num_layers)\n",
        "    self.quantum_nn = BetterBetterTorchLayer(self.nodes_per_graph, self.num_layers, self.input_dim, self.device)\n",
        "\n",
        "  def edge_attr_relevant(self, edge_attr, edge_attr_r):\n",
        "    count=0\n",
        "    for n in range(edge_attr.shape[0]):\n",
        "        for e in range(n+1, edge_attr.shape[1]):\n",
        "          edge_attr_r[count] = edge_attr[n, e]\n",
        "          count += 1\n",
        "    return edge_attr_r\n",
        "\n",
        "  def forward(self, x, node_indices, edge_attr=None):\n",
        "    if edge_attr is not None:\n",
        "      edge_attr_r = torch.zeros((edge_attr.shape[0], int(edge_attr.shape[1]*edge_attr.shape[2]/2 + 1)), device=self.device)\n",
        "      edge_attr = torch.vmap(self.edge_attr_relevant)(edge_attr, edge_attr_r)\n",
        "    output = self.quantum_nn(x, node_indices, edge_attr)\n",
        "    output = output[:, [2**i for i in range(self.nodes_per_graph)]]\n",
        "    return torch.vmap(lambda out, ind: out[ind])(output, node_indices)"
      ],
      "metadata": {
        "id": "_Ky_BkEUXiYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/colizz/weaver-benchmark/blob/main/top_tagging/networks/particlenet_pf.py\n",
        "\n",
        "'''Based on https://github.com/WangYueFt/dgcnn/blob/master/pytorch/model.py.'''\n",
        "\n",
        "\n",
        "def knn(x, k):\n",
        "    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n",
        "    xx = torch.sum(x ** 2, dim=1, keepdim=True)\n",
        "    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n",
        "    idx = pairwise_distance.topk(k=k + 1, dim=-1)[1][:, :, 1:]  # (batch_size, num_points, k)\n",
        "    return idx\n",
        "\n",
        "\n",
        "# v1 is faster on GPU\n",
        "def get_graph_feature_v1(x, k, idx):\n",
        "    batch_size, num_dims, num_points = x.size()\n",
        "\n",
        "    idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1) * num_points\n",
        "    idx = idx + idx_base\n",
        "    idx = idx.view(-1)\n",
        "\n",
        "    fts = x.transpose(2, 1).reshape(-1, num_dims)  # -> (batch_size, num_points, num_dims) -> (batch_size*num_points, num_dims)\n",
        "    fts = fts[idx, :].view(batch_size, num_points, k, num_dims)  # neighbors: -> (batch_size*num_points*k, num_dims) -> ...\n",
        "    fts = fts.permute(0, 3, 1, 2).contiguous()  # (batch_size, num_dims, num_points, k)\n",
        "    x = x.view(batch_size, num_dims, num_points, 1).repeat(1, 1, 1, k)\n",
        "    fts = torch.cat((x, fts - x), dim=1)  # ->(batch_size, 2*num_dims, num_points, k)\n",
        "    return fts\n",
        "\n",
        "\n",
        "# v2 is faster on CPU\n",
        "def get_graph_feature_v2(x, k, idx):\n",
        "    batch_size, num_dims, num_points = x.size()\n",
        "\n",
        "    idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1) * num_points\n",
        "    idx = idx + idx_base\n",
        "    idx = idx.view(-1)\n",
        "\n",
        "    fts = x.transpose(0, 1).reshape(num_dims, -1)  # -> (num_dims, batch_size, num_points) -> (num_dims, batch_size*num_points)\n",
        "    fts = fts[:, idx].view(num_dims, batch_size, num_points, k)  # neighbors: -> (num_dims, batch_size*num_points*k) -> ...\n",
        "    fts = fts.transpose(1, 0).contiguous()  # (batch_size, num_dims, num_points, k)\n",
        "\n",
        "    x = x.view(batch_size, num_dims, num_points, 1).repeat(1, 1, 1, k)\n",
        "    fts = torch.cat((x, fts - x), dim=1)  # ->(batch_size, 2*num_dims, num_points, k)\n",
        "\n",
        "    return fts\n",
        "\n",
        "\n",
        "class EdgeConvBlock(nn.Module):\n",
        "    r\"\"\"EdgeConv layer.\n",
        "    Introduced in \"`Dynamic Graph CNN for Learning on Point Clouds\n",
        "    <https://arxiv.org/pdf/1801.07829>`__\".  Can be described as follows:\n",
        "    .. math::\n",
        "       x_i^{(l+1)} = \\max_{j \\in \\mathcal{N}(i)} \\mathrm{ReLU}(\n",
        "       \\Theta \\cdot (x_j^{(l)} - x_i^{(l)}) + \\Phi \\cdot x_i^{(l)})\n",
        "    where :math:`\\mathcal{N}(i)` is the neighbor of :math:`i`.\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_feat : int\n",
        "        Input feature size.\n",
        "    out_feat : int\n",
        "        Output feature size.\n",
        "    batch_norm : bool\n",
        "        Whether to include batch normalization on messages.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k, in_feat, out_feats, batch_norm=True, activation=True, cpu_mode=False):\n",
        "        super(EdgeConvBlock, self).__init__()\n",
        "        self.k = k\n",
        "        self.batch_norm = batch_norm\n",
        "        self.activation = activation\n",
        "        self.num_layers = len(out_feats)\n",
        "        self.get_graph_feature = get_graph_feature_v2 if cpu_mode else get_graph_feature_v1\n",
        "\n",
        "        self.convs = nn.ModuleList()\n",
        "        for i in range(self.num_layers):\n",
        "            self.convs.append(nn.Conv2d(2 * in_feat if i == 0 else out_feats[i - 1], out_feats[i], kernel_size=1, bias=False if self.batch_norm else True))\n",
        "\n",
        "        if batch_norm:\n",
        "            self.bns = nn.ModuleList()\n",
        "            for i in range(self.num_layers):\n",
        "                self.bns.append(nn.BatchNorm2d(out_feats[i]))\n",
        "\n",
        "        if activation:\n",
        "            self.acts = nn.ModuleList()\n",
        "            for i in range(self.num_layers):\n",
        "                self.acts.append(nn.ReLU())\n",
        "\n",
        "        if in_feat == out_feats[-1]:\n",
        "            self.sc = None\n",
        "        else:\n",
        "            self.sc = nn.Conv1d(in_feat, out_feats[-1], kernel_size=1, bias=False)\n",
        "            self.sc_bn = nn.BatchNorm1d(out_feats[-1])\n",
        "\n",
        "        if activation:\n",
        "            self.sc_act = nn.ReLU()\n",
        "\n",
        "    def forward(self, points, features):\n",
        "\n",
        "        topk_indices = knn(points, self.k)\n",
        "        x = self.get_graph_feature(features, self.k, topk_indices)\n",
        "\n",
        "        for conv, bn, act in zip(self.convs, self.bns, self.acts):\n",
        "            x = conv(x)  # (N, C', P, K)\n",
        "            if bn:\n",
        "                x = bn(x)\n",
        "            if act:\n",
        "                x = act(x)\n",
        "\n",
        "        fts = x.mean(dim=-1)  # (N, C, P)\n",
        "\n",
        "        # shortcut\n",
        "        if self.sc:\n",
        "            sc = self.sc(features)  # (N, C_out, P)\n",
        "            sc = self.sc_bn(sc)\n",
        "        else:\n",
        "            sc = features\n",
        "\n",
        "        return self.sc_act(sc + fts)  # (N, C_out, P)\n",
        "\n",
        "\n",
        "class ParticleNet(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 input_dims,\n",
        "                 num_classes,\n",
        "                 conv_params=[(7, (32, 32, 32)), (7, (64, 64, 64))],\n",
        "                 fc_params=[(128, 0.1)],\n",
        "                 use_fusion=True,\n",
        "                 use_fts_bn=True,\n",
        "                 use_counts=True,\n",
        "                 for_inference=False,\n",
        "                 for_segmentation=True,\n",
        "                 **kwargs):\n",
        "        super(ParticleNet, self).__init__(**kwargs)\n",
        "\n",
        "        self.use_fts_bn = use_fts_bn\n",
        "        if self.use_fts_bn:\n",
        "            self.bn_fts = nn.BatchNorm1d(input_dims)\n",
        "\n",
        "        self.use_counts = use_counts\n",
        "\n",
        "        self.edge_convs = nn.ModuleList()\n",
        "        for idx, layer_param in enumerate(conv_params):\n",
        "            k, channels = layer_param\n",
        "            in_feat = input_dims if idx == 0 else conv_params[idx - 1][1][-1]\n",
        "            self.edge_convs.append(EdgeConvBlock(k=k, in_feat=in_feat, out_feats=channels, cpu_mode=for_inference))\n",
        "\n",
        "        self.use_fusion = use_fusion\n",
        "        if self.use_fusion:\n",
        "            in_chn = sum(x[-1] for _, x in conv_params)\n",
        "            out_chn = np.clip((in_chn // 128) * 128, 128, 1024)\n",
        "            self.fusion_block = nn.Sequential(nn.Conv1d(in_chn, out_chn, kernel_size=1, bias=False), nn.BatchNorm1d(out_chn), nn.ReLU())\n",
        "\n",
        "        self.for_segmentation = for_segmentation\n",
        "\n",
        "        fcs = []\n",
        "        for idx, layer_param in enumerate(fc_params):\n",
        "            channels, drop_rate = layer_param\n",
        "            if idx == 0:\n",
        "                in_chn = out_chn if self.use_fusion else conv_params[-1][1][-1]\n",
        "            else:\n",
        "                in_chn = fc_params[idx - 1][0]\n",
        "            if self.for_segmentation:\n",
        "                fcs.append(nn.Sequential(nn.Conv1d(in_chn, channels, kernel_size=1, bias=False),\n",
        "                                         nn.BatchNorm1d(channels), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "            else:\n",
        "                fcs.append(nn.Sequential(nn.Linear(in_chn, channels), nn.ReLU(), nn.Dropout(drop_rate)))\n",
        "        # if self.for_segmentation:\n",
        "        #     fcs.append(nn.Conv1d(fc_params[-1][0], num_classes, kernel_size=1))\n",
        "        # else:\n",
        "        #     fcs.append(nn.Linear(fc_params[-1][0], num_classes))\n",
        "        self.fc = nn.Sequential(*fcs)\n",
        "\n",
        "        self.for_inference = for_inference\n",
        "\n",
        "    def forward(self, points, features, mask=None):\n",
        "#         print('points:\\n', points)\n",
        "#         print('features:\\n', features)\n",
        "        if mask is None:\n",
        "            mask = (features.abs().sum(dim=1, keepdim=True) != 0)  # (N, 1, P)\n",
        "            # print(mask)\n",
        "            # print(mask.shape)\n",
        "        points *= mask\n",
        "        features *= mask\n",
        "        coord_shift = (mask == 0) * 1e9\n",
        "        if self.use_counts:\n",
        "            counts = mask.float().sum(dim=-1)\n",
        "            counts = torch.max(counts, torch.ones_like(counts))  # >=1\n",
        "\n",
        "        if self.use_fts_bn:\n",
        "            fts = self.bn_fts(features) * mask\n",
        "        else:\n",
        "            fts = features\n",
        "        outputs = []\n",
        "        for idx, conv in enumerate(self.edge_convs):\n",
        "            pts = (points if idx == 0 else fts) + coord_shift\n",
        "            fts = conv(pts, fts) * mask\n",
        "            if self.use_fusion:\n",
        "                outputs.append(fts)\n",
        "        if self.use_fusion:\n",
        "            fts = self.fusion_block(torch.cat(outputs, dim=1)) * mask\n",
        "\n",
        "#         assert(((fts.abs().sum(dim=1, keepdim=True) != 0).float() - mask.float()).abs().sum().item() == 0)\n",
        "\n",
        "        if self.for_segmentation:\n",
        "            x = fts\n",
        "        else:\n",
        "            if self.use_counts:\n",
        "                x = fts.sum(dim=-1) / counts  # divide by the real counts\n",
        "            else:\n",
        "                x = fts.mean(dim=-1)\n",
        "\n",
        "        output =  self.fc(x)\n",
        "\n",
        "        if self.for_inference:\n",
        "            output = torch.softmax(output, dim=1)\n",
        "        # print('output:\\n', output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeatureConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_chn, out_chn, **kwargs):\n",
        "        super(FeatureConv, self).__init__(**kwargs)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.BatchNorm1d(in_chn),\n",
        "            nn.Conv1d(in_chn, out_chn, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm1d(out_chn),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class ParticleNetTagger1Path(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 pf_features_dims,\n",
        "                 num_classes,\n",
        "                 conv_params= [(6, (32, 32, 32)), (6, (64, 64, 64)), (6, (128, 128, 128))],   #[(7, (32, 32, 32)), (7, (64, 64, 64))],  #\n",
        "                 fc_params=[(128, 0.1)],\n",
        "                 use_fusion=False,\n",
        "                 use_fts_bn=False,\n",
        "                 use_counts=True,\n",
        "                 pf_input_dropout=None,\n",
        "                 for_inference=False,\n",
        "                 **kwargs):\n",
        "        super(ParticleNetTagger1Path, self).__init__(**kwargs)\n",
        "        self.pf_input_dropout = nn.Dropout(pf_input_dropout) if pf_input_dropout else None\n",
        "        # self.pf_conv = FeatureConv(pf_features_dims, 32)\n",
        "        self.gnn = ParticleNet(input_dims=pf_features_dims,\n",
        "                              num_classes=num_classes,\n",
        "                              conv_params=conv_params,\n",
        "                              fc_params=fc_params,\n",
        "                              use_fusion=use_fusion,\n",
        "                              use_fts_bn=use_fts_bn,\n",
        "                              use_counts=use_counts,\n",
        "                              for_inference=for_inference)\n",
        "\n",
        "    def forward(self, pf_points, pf_features, pf_mask):\n",
        "        if self.pf_input_dropout:\n",
        "            pf_mask = (self.pf_input_dropout(pf_mask) != 0).float()\n",
        "            pf_points *= pf_mask\n",
        "            pf_features *= pf_mask\n",
        "        # mod_feats = self.pf_conv(pf_features)   #* pf_mask , * pf_mask\n",
        "        return self.gnn(pf_points, pf_features, pf_mask)\n",
        "\n",
        "def get_model(data_config, **kwargs):\n",
        "    # conv_params = [\n",
        "    #     (16, (64, 64, 64)),\n",
        "    #     (16, (128, 128, 128)),\n",
        "    #     (16, (256, 256, 256)),\n",
        "    #     ]\n",
        "    ec_k = kwargs.get('ec_k', 16)\n",
        "    ec_c1 = kwargs.get('ec_c1', 64)\n",
        "    ec_c2 = kwargs.get('ec_c2', 128)\n",
        "    ec_c3 = kwargs.get('ec_c3', 256)\n",
        "    fc_c, fc_p = kwargs.get('fc_c', 256), kwargs.get('fc_p', 0.1)\n",
        "    conv_params = [\n",
        "        (ec_k, (ec_c1, ec_c1, ec_c1)),\n",
        "        (ec_k, (ec_c2, ec_c2, ec_c2)),\n",
        "        (ec_k, (ec_c3, ec_c3, ec_c3)),\n",
        "        ]\n",
        "    fc_params = [(fc_c, fc_p)]\n",
        "    use_fusion = True\n",
        "\n",
        "    pf_features_dims = len(data_config.input_dicts['pf_features'])\n",
        "    num_classes = len(data_config.label_value)\n",
        "    model = ParticleNetTagger1Path(pf_features_dims, num_classes,\n",
        "                                   conv_params, fc_params,\n",
        "                                   use_fusion=use_fusion,\n",
        "                                   use_fts_bn=kwargs.get('use_fts_bn', False),\n",
        "                                   use_counts=kwargs.get('use_counts', True),\n",
        "                                   pf_input_dropout=kwargs.get('pf_input_dropout', None),\n",
        "                                   for_inference=kwargs.get('for_inference', False)\n",
        "                                   )\n",
        "    model_info = {\n",
        "        'input_names':list(data_config.input_names),\n",
        "        'input_shapes':{k:((1,) + s[1:]) for k, s in data_config.input_shapes.items()},\n",
        "        'output_names':['softmax'],\n",
        "        'dynamic_axes':{**{k:{0:'N', 2:'n_' + k.split('_')[0]} for k in data_config.input_names}, **{'softmax':{0:'N'}}},\n",
        "        }\n",
        "\n",
        "    return model, model_info"
      ],
      "metadata": {
        "id": "apm4c_MrZw0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_nodes_prob_batch(graph, batch_size):\n",
        "\n",
        "    node_num = graph.num_nodes()\n",
        "    edge_num = graph.num_edges()\n",
        "    node_num_sin = int(graph.num_nodes()/batch_size)\n",
        "    edge_num_sin = int(graph.num_edges()/batch_size)\n",
        "    drop_num_sin = int(node_num_sin * aug_ratio)\n",
        "    drop_num = batch_size*drop_num_sin\n",
        "    node_score = graph.ndata['node_score'].reshape(batch_size, -1)\n",
        "    node_prob = node_score.float()\n",
        "    node_prob += 0.001\n",
        "    node_prob = cp.array(node_prob)\n",
        "    node_prob /= node_prob.sum(axis=1)[:, None]\n",
        "    node_prob = node_prob.get()\n",
        "\n",
        "    idx_nondrop = np.zeros((batch_size, node_num_sin-drop_num_sin), dtype=np.int64)\n",
        "    idx_drop_set = []\n",
        "    idx_nondrop_e = np.zeros((batch_size, node_num_sin-drop_num_sin), dtype=np.int64)\n",
        "    for b in range(batch_size):\n",
        "      idx_nondrop[b] = np.random.choice(node_num_sin, node_num_sin-drop_num_sin, replace=False, p=node_prob[b])\n",
        "      idx_drop_set.append(set(np.setdiff1d(np.arange(node_num_sin), idx_nondrop[b]).tolist()))\n",
        "      idx_nondrop[b].sort()\n",
        "      idx_nondrop_e[b] += idx_nondrop[b] + b*node_num_sin\n",
        "\n",
        "    idx_nondrop_e = idx_nondrop_e.reshape(-1,)\n",
        "\n",
        "    idx_drop_set_e = set(np.setdiff1d(np.arange(node_num), idx_nondrop_e).tolist())\n",
        "\n",
        "    idx_dict = np.zeros((idx_nondrop_e[-1] + 1,), dtype=np.int64)\n",
        "    idx_dict[idx_nondrop_e] = np.arange(len(idx_nondrop_e), dtype=np.int64)\n",
        "\n",
        "    # edge_index = data.edge_index.numpy()\n",
        "    edge_index = np.array([graph.edges()[0].cpu().numpy(), graph.edges()[1].cpu().numpy()])\n",
        "\n",
        "    edge_mask = []\n",
        "    for n in range(edge_num):\n",
        "        if edge_index[0, n] not in idx_drop_set_e and edge_index[1, n] not in idx_drop_set_e:\n",
        "          edge_mask.append(n)\n",
        "    edge_mask = np.asarray(edge_mask, dtype=np.int64)\n",
        "    edge_index = idx_dict[edge_index[:, edge_mask]]\n",
        "\n",
        "    ng = dgl.graph([])\n",
        "    src = graph.edges()[0]\n",
        "    dst = graph.edges()[1]\n",
        "    nsrc = edge_index[0]\n",
        "    ndst = edge_index[1]\n",
        "    ng.add_edges(nsrc, ndst)\n",
        "    ng.to(device)\n",
        "    ng.ndata['node_attr'] = graph.ndata['node_attr'].clone().reshape(batch_size,node_num_sin,-1)[torch.arange(batch_size, device=device).unsqueeze(-1), idx_nondrop].reshape(node_num-drop_num,-1)\n",
        "    ng.ndata['node_attr_irc'] = graph.ndata['node_attr_irc'].clone().reshape(batch_size,node_num_sin,-1)[torch.arange(batch_size, device=device).unsqueeze(-1), idx_nondrop].reshape(node_num-drop_num,-1)\n",
        "    ng.ndata['node_indices'] = torch.Tensor(idx_nondrop, device=device).int().reshape(-1,1)\n",
        "    ng.edata['edge_attr'] = graph.edata['edge_attr'].clone()[edge_mask]\n",
        "    ng.edata['edge_indices'] = torch.Tensor(edge_mask, device=device).int()\n",
        "\n",
        "    return ng"
      ],
      "metadata": {
        "id": "poBXGx9yXlIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def drop_nodes_cp_batch(graph, batch_size):\n",
        "\n",
        "    node_num = graph.num_nodes()\n",
        "    edge_num = graph.num_edges()\n",
        "    node_num_sin = int(graph.num_nodes()/batch_size)\n",
        "    edge_num_sin = int(graph.num_edges()/batch_size)\n",
        "    drop_num_sin = int(node_num_sin * aug_ratio)\n",
        "    drop_num = batch_size*drop_num_sin\n",
        "    node_score = graph.ndata['node_score'].reshape(batch_size, -1)\n",
        "    node_prob = torch.sub(torch.max(node_score, dim=1).values.reshape(-1,1), node_score)\n",
        "    node_prob += 0.001\n",
        "    node_prob = cp.array(node_prob)\n",
        "    node_prob /= node_prob.sum(axis=1)[:, None]\n",
        "    node_prob = node_prob.get()\n",
        "\n",
        "    idx_nondrop = np.zeros((batch_size, node_num_sin-drop_num_sin), dtype=np.int64)\n",
        "    idx_drop_set = []\n",
        "    idx_nondrop_e = np.zeros((batch_size, node_num_sin-drop_num_sin), dtype=np.int64)\n",
        "    for b in range(batch_size):\n",
        "      idx_nondrop[b] = np.random.choice(node_num_sin, node_num_sin-drop_num_sin, replace=False, p=node_prob[b])\n",
        "      idx_drop_set.append(set(np.setdiff1d(np.arange(node_num_sin), idx_nondrop[b]).tolist()))\n",
        "      idx_nondrop[b].sort()\n",
        "      idx_nondrop_e[b] += idx_nondrop[b] + b*node_num_sin\n",
        "\n",
        "    idx_nondrop_e = idx_nondrop_e.reshape(-1,)\n",
        "\n",
        "    idx_drop_set_e = set(np.setdiff1d(np.arange(node_num), idx_nondrop_e).tolist())\n",
        "\n",
        "    idx_dict = np.zeros((idx_nondrop_e[-1] + 1,), dtype=np.int64)\n",
        "    idx_dict[idx_nondrop_e] = np.arange(len(idx_nondrop_e), dtype=np.int64)\n",
        "\n",
        "    # edge_index = data.edge_index.numpy()\n",
        "    edge_index = np.array([graph.edges()[0].cpu().numpy(), graph.edges()[1].cpu().numpy()])\n",
        "\n",
        "    edge_mask = []\n",
        "    for n in range(edge_num):\n",
        "        if edge_index[0, n] not in idx_drop_set_e and edge_index[1, n] not in idx_drop_set_e:\n",
        "          edge_mask.append(n)\n",
        "    edge_mask = np.asarray(edge_mask, dtype=np.int64)\n",
        "    edge_index = idx_dict[edge_index[:, edge_mask]]\n",
        "\n",
        "    ng = dgl.graph([])\n",
        "    src = graph.edges()[0]\n",
        "    dst = graph.edges()[1]\n",
        "    nsrc = edge_index[0]\n",
        "    ndst = edge_index[1]\n",
        "    ng.add_edges(nsrc, ndst)\n",
        "    ng.to(device)\n",
        "    ng.ndata['node_attr'] = graph.ndata['node_attr'].clone().reshape(batch_size,node_num_sin,-1)[torch.arange(batch_size,  device=device).unsqueeze(-1), idx_nondrop].reshape(node_num-drop_num,-1)\n",
        "    ng.ndata['node_attr_irc'] = graph.ndata['node_attr_irc'].clone().reshape(batch_size,node_num_sin,-1)[torch.arange(batch_size, device=device).unsqueeze(-1), idx_nondrop].reshape(node_num-drop_num,-1)\n",
        "    ng.ndata['node_indices'] = torch.Tensor(idx_nondrop, device=device).int().reshape(-1,1)\n",
        "    ng.edata['edge_attr'] = graph.edata['edge_attr'].clone()[edge_mask]\n",
        "    ng.edata['edge_indices'] = torch.Tensor(edge_mask, device=device).int()\n",
        "\n",
        "    return ng"
      ],
      "metadata": {
        "id": "9pqevFSEXptq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class graphcl(nn.Module):\n",
        "    def __init__(self, gnn, node_imp_estimator, emb_dim, out_dim):\n",
        "        super(graphcl, self).__init__()\n",
        "        self.gnn = gnn\n",
        "        self.node_imp_estimator = node_imp_estimator\n",
        "        self.pool = global_mean_pool\n",
        "        self.projection_head = nn.Sequential(nn.Linear(emb_dim, out_dim), nn.ReLU(inplace=True), nn.Linear(out_dim, out_dim))\n",
        "\n",
        "    def prepare_variables_rg(self, gr_n, g_n, n_i):\n",
        "      g_n[n_i] = gr_n\n",
        "      return g_n\n",
        "\n",
        "    def forward_cl(self, x, edge_index, edge_attr, batch, nodes_per_graph, g_batch=None, q_edge_attr=False, device='cpu', node_est='classical'):\n",
        "      if node_est == 'classical':\n",
        "          node_imp = self.node_imp_estimator(x, edge_index, edge_attr, batch)\n",
        "      if node_est == 'quantum':\n",
        "          # ############ Quantum Node Est.\n",
        "\n",
        "          g_n = g_batch.ndata['node_attr'].clone().reshape(3*batch_size, nodes_per_graph, -1)\n",
        "          n_i = g_batch.ndata['node_indices'].clone().reshape(3*batch_size, nodes_per_graph)\n",
        "          g_n_0 = torch.zeros((3*batch_size, nodes_per_graph_original, g_n.shape[2]), device=device)\n",
        "          g_n = torch.vmap(self.prepare_variables_rg)(g_n.float(), g_n_0, n_i.long())\n",
        "\n",
        "          e_n = g_batch.edata['edge_attr'].clone()\n",
        "          e_i = g_batch.edata['edge_indices'].clone()\n",
        "          e_n_0 = torch.zeros((3*batch_size*nodes_per_graph_original*(nodes_per_graph_original-1)), device=device)\n",
        "          e_n_0[e_i.long()]  = e_n\n",
        "          e_n = e_n_0.reshape(3*batch_size, nodes_per_graph_original, -1)\n",
        "\n",
        "          if q_edge_attr:\n",
        "            node_imp = model.node_imp_estimator(g_n, n_i, e_n)\n",
        "          else:\n",
        "            node_imp = model.node_imp_estimator(g_n, n_i)\n",
        "          node_imp = node_imp.reshape(-1,1)\n",
        "          ############\n",
        "\n",
        "      out = torch.max(node_imp.reshape(-1, nodes_per_graph), 1)[0]\n",
        "      out = out.reshape(-1, 1)\n",
        "      out = out[batch]\n",
        "      node_imp /= (out * 10)\n",
        "      node_imp += 0.9\n",
        "\n",
        "      pf_feats = x.reshape(3*batch_size, nodes_per_graph, -1)\n",
        "      points = pf_feats[:,:,1:3]\n",
        "\n",
        "      x = self.gnn(points.reshape(points.shape[0], points.shape[2], points.shape[1])\n",
        "                     , pf_feats.reshape(pf_feats.shape[0], pf_feats.shape[2], pf_feats.shape[1]), None)\n",
        "      x = x.reshape(x.shape[0], x.shape[2], x.shape[1])\n",
        "      x = x.reshape(x.shape[0]*x.shape[1], x.shape[2])\n",
        "\n",
        "      node_imp = node_imp.expand(-1, x.shape[1])\n",
        "      x = torch.mul(x, node_imp)\n",
        "      x = self.pool(x, batch)\n",
        "      x = self.projection_head(x.float())\n",
        "\n",
        "      return x\n",
        "\n",
        "    def loss_cl(self, x1, x2, temp):\n",
        "        T = temp\n",
        "        batch_size, _ = x1.size()\n",
        "        x1_abs = x1.norm(dim=1)\n",
        "        x2_abs = x2.norm(dim=1)\n",
        "\n",
        "        sim_matrix = torch.einsum('ik,jk->ij', x1, x2) / torch.einsum('i,j->ij', x1_abs, x2_abs)\n",
        "        sim_matrix = torch.exp(sim_matrix / T)\n",
        "        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
        "        loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
        "        loss = - torch.log(loss).mean()\n",
        "        return loss\n",
        "\n",
        "    def loss_infonce(self, x1, x2, temp):\n",
        "        T = temp\n",
        "        batch_size, _ = x1.size()\n",
        "        x1_abs = x1.norm(dim=1)\n",
        "        x2_abs = x2.norm(dim=1)\n",
        "\n",
        "        sim_matrix = torch.einsum('ik,jk->ij', x1, x2) / torch.einsum('i,j->ij', x1_abs, x2_abs)\n",
        "        sim_matrix = torch.exp(sim_matrix / T)\n",
        "        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
        "        loss = pos_sim / sim_matrix.sum(dim=1)\n",
        "        loss = - torch.log(loss).mean()\n",
        "        return loss\n",
        "\n",
        "    def loss_ra(self, x1, x2, x3, temp, lamda):\n",
        "        batch_size, _ = x1.size()\n",
        "        x1_abs = x1.norm(dim=1)\n",
        "        x2_abs = x2.norm(dim=1)\n",
        "        x3_abs = x3.norm(dim=1)\n",
        "\n",
        "        cp_sim_matrix = torch.einsum('ik,jk->ij', x1, x3) / torch.einsum('i,j->ij', x1_abs, x3_abs)\n",
        "        cp_sim_matrix = torch.exp(cp_sim_matrix / temp)\n",
        "\n",
        "        sim_matrix = torch.einsum('ik,jk->ij', x1, x2) / torch.einsum('i,j->ij', x1_abs, x2_abs)\n",
        "        sim_matrix = torch.exp(sim_matrix / temp)\n",
        "\n",
        "        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
        "\n",
        "        ra_loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
        "        ra_loss = - torch.log(ra_loss).mean()\n",
        "\n",
        "        cp_loss = pos_sim / (cp_sim_matrix.sum(dim=1) + pos_sim)\n",
        "        cp_loss = - torch.log(cp_loss).mean()\n",
        "\n",
        "        uni_loss_1 = self.lunif(torch.nn.functional.normalize(x1, dim=1))\n",
        "        uni_loss_2 = self.lunif(torch.nn.functional.normalize(x2, dim=1))\n",
        "        uni_loss = (uni_loss_1 + uni_loss_2) / 2\n",
        "        al_loss = self.lalign(torch.nn.functional.normalize(x1, dim=1), torch.nn.functional.normalize(x2, dim=1))\n",
        "\n",
        "        loss = ra_loss + lamda * cp_loss\n",
        "        # loss = 0.5*uni_loss + al_loss + lamda * cp_loss\n",
        "\n",
        "        return ra_loss, cp_loss, loss, uni_loss, al_loss\n",
        "\n",
        "    def lalign(self, x, y, alpha=2):\n",
        "      return (x - y).norm(dim=1).pow(alpha).mean()\n",
        "\n",
        "    def lunif(self, x, t=2):\n",
        "      sq_pdist = torch.pdist(x, p=2).pow(2)\n",
        "      return sq_pdist.mul(-t).exp().mean().log()\n",
        "\n",
        "def train(epoch, model, device, dataset, optimizer, batch_size, nodes_per_graph, aug_ratio, loss_temp, lamda, irc_safety, q_edge_attr=False, loader=None, node_est='classical'):\n",
        "\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "    dataset.aug = \"none\"\n",
        "    imp_batch_size = batch_size\n",
        "    loader = GraphDataLoader(dataset, batch_size=imp_batch_size, shuffle=False, drop_last=False)\n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    node_imp_l = []\n",
        "    # loader.set_epoch(epoch)\n",
        "    for step, (g_batch, _) in enumerate(loader):\n",
        "\n",
        "        batch = torch.arange(0, g_batch.batch_size, device=device).reshape(-1,1).expand(g_batch.batch_size, nodes_per_graph).reshape(-1,)\n",
        "        if node_est == 'classical':\n",
        "          node_imp = model.node_imp_estimator(g_batch.ndata['node_attr'], torch.stack(g_batch.edges()), g_batch.edata['edge_attr'], batch).detach()\n",
        "        else:\n",
        "          ############ Quantum Node Est.\n",
        "          g_batch.to(device)\n",
        "          g_n = g_batch.ndata['node_attr'].clone().reshape(batch_size, nodes_per_graph, -1)\n",
        "          e_n = g_batch.edata['edge_attr'].clone().reshape(batch_size, nodes_per_graph, -1)\n",
        "          n_i = g_batch.ndata['node_indices'].clone().reshape(batch_size, nodes_per_graph)\n",
        "\n",
        "          if q_edge_attr:\n",
        "            node_imp = model.node_imp_estimator(g_n, n_i, e_n)\n",
        "          else:\n",
        "            node_imp = model.node_imp_estimator(g_n, n_i)\n",
        "          node_imp = node_imp.reshape(-1,1)\n",
        "\n",
        "          ############\n",
        "\n",
        "        node_imp_l.append(node_imp.squeeze())\n",
        "\n",
        "    for i, b in enumerate(node_imp_l):\n",
        "      n = b.reshape(-1,nodes_per_graph)\n",
        "      for g in range(len(n)):\n",
        "        dataset[i*len(n)+g][0].ndata['node_score'] = torch.Tensor(n[g])\n",
        "\n",
        "    dataset.nodes_per_aug_graph = dataset.nodes_per_graph-int(aug_ratio*dataset.nodes_per_graph)\n",
        "\n",
        "    torch.set_grad_enabled(True)\n",
        "    model.train()\n",
        "\n",
        "    train_loss_accum = 0\n",
        "    ra_loss_accum = 0\n",
        "    cp_loss_accum = 0\n",
        "    uni_loss_accum = 0\n",
        "    alignment_loss_accum = 0\n",
        "\n",
        "    for step, (g_batch, _) in enumerate(loader):\n",
        "        batch1, batch2 = dataset.augment_dataset('rationale', g_batch, batch_size)\n",
        "        batch3 = dataset.augment_dataset('complement', g_batch, batch_size)\n",
        "\n",
        "        batch1 = batch1.to(device)\n",
        "        batch2 = batch2.to(device)\n",
        "        batch3 = batch3.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_1 = torch.arange(0, batch_size, device=device).reshape(-1,1).expand(batch_size, dataset.nodes_per_aug_graph).reshape(-1,)\n",
        "        batch_2 = torch.arange(0, batch_size, device=device).reshape(-1,1).expand(batch_size, dataset.nodes_per_aug_graph).reshape(-1,)\n",
        "        batch_3 = torch.arange(0, batch_size, device=device).reshape(-1,1).expand(batch_size, dataset.nodes_per_aug_graph).reshape(-1,)\n",
        "\n",
        "        node_attr_123 = torch.cat((batch1.ndata['node_attr'].float(), batch2.ndata['node_attr'].float(), batch3.ndata['node_attr'].float()), dim=0)\n",
        "        edges_123 = torch.stack(dgl.batch([batch1, batch2, batch3]).edges())\n",
        "        edge_attr_123 = torch.cat((batch1.edata['edge_attr'], batch2.edata['edge_attr'], batch3.edata['edge_attr']), dim=0)\n",
        "        overall_batch = torch.arange(0, batch_size*3).reshape(-1,1).expand(batch_size*3, dataset.nodes_per_aug_graph).reshape(-1,)\n",
        "        output = model.forward_cl(node_attr_123, edges_123, edge_attr_123, overall_batch, dataset.nodes_per_aug_graph, dgl.batch([batch1, batch2, batch3]), q_edge_attr, device=device, node_est=node_est)\n",
        "        x1, x2, x3 = torch.split(output, [batch_size, batch_size, batch_size], dim=0)\n",
        "\n",
        "        ra_loss, cp_loss, loss, uni_loss, al_loss = model.loss_ra(x1, x2, x3, loss_temp, lamda)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss_accum += float(loss.detach().cpu().item())\n",
        "        ra_loss_accum += float(ra_loss.detach().cpu().item())\n",
        "        cp_loss_accum += float(cp_loss.detach().cpu().item())\n",
        "        uni_loss_accum += float(uni_loss.detach().cpu().item())\n",
        "        alignment_loss_accum += float(al_loss.detach().cpu().item())\n",
        "\n",
        "    gc.collect()\n",
        "    return train_loss_accum/(step+1), ra_loss_accum/(step+1), cp_loss_accum/(step+1), uni_loss_accum/(step+1), alignment_loss_accum/(step+1)"
      ],
      "metadata": {
        "id": "pR4PZBx2XsD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set up model\n",
        "device = 'cuda'\n",
        "nodes_per_graph_original = 10\n",
        "num_layer_gnn = 2\n",
        "num_layer_gnn_est = 2\n",
        "qnn_layers = 3\n",
        "emb_dim = 128\n",
        "in_dim = 8\n",
        "inter_dim = 256\n",
        "out_dim = 128\n",
        "JK = 'last'\n",
        "dropout_ratio = 0.1\n",
        "gnn_type = 'gat'    #'gcn'\n",
        "lr = 0.001\n",
        "decay = 0\n",
        "aug_ratio = 0.1\n",
        "batch_size = 2000\n",
        "loss_temp = 0.1\n",
        "lamda = 0.1\n",
        "node_est = 'quantum'\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "qg_dataset = QuarkGluonGraphDataset(dataset_name='Quark Gluon', raw_dir=main_dir, save_dir='/content',\n",
        "                                    data_folder_name=jet_folder_path, datafile_name=jet_file_path, labelsfile_name=jet_file_path,\n",
        "                                    datatype='particles', dataset_size=10000, nodes_per_graph=nodes_per_graph_original, spectral_augmentation=False, irc_safety_aug=True,\n",
        "                                    device='cuda')\n",
        "\n",
        "# gnn = GNN(num_layer=num_layer_gnn, in_dim=in_dim, emb_dim=emb_dim, inter_dim=inter_dim, JK=JK, drop_ratio=dropout_ratio, gnn_type=gnn_type)\n",
        "gnn = ParticleNetTagger1Path(in_dim, 2)\n",
        "if node_est == 'classical':\n",
        "  node_imp_estimator = GNN_imp_estimator(num_layer=num_layer_gnn_est, emb_dim=emb_dim, in_dim=in_dim, JK=JK, drop_ratio=dropout_ratio)\n",
        "if node_est == 'quantum':\n",
        "  node_imp_estimator = QGNN_node_estimator(nodes_per_graph_original, qnn_layers, in_dim, device=device)\n",
        "\n",
        "model = graphcl(gnn, node_imp_estimator, emb_dim, out_dim)\n",
        "model.to(device)\n",
        "\n",
        "import cupy as cp\n",
        "#set up optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=decay)"
      ],
      "metadata": {
        "id": "BeXbU8V9XzWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "con_loss = []\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(\"====epoch \" + str(epoch))\n",
        "    qg_dataset.augment = False\n",
        "    train_loss, ra_loss, cp_loss, uni_loss, al_loss = train(epoch, model=model, device=device, dataset=qg_dataset, optimizer=optimizer, batch_size=batch_size,\n",
        "                                      nodes_per_graph=qg_dataset.nodes_per_graph, aug_ratio=aug_ratio, loss_temp=loss_temp, lamda=lamda,\n",
        "                                      irc_safety=True, q_edge_attr=True)\n",
        "    con_loss.append(train_loss)\n",
        "    print(train_loss)\n",
        "    print(ra_loss)\n",
        "    print(cp_loss)\n",
        "    print('UNI : ', uni_loss)\n",
        "    print('ALIGN : ', al_loss)"
      ],
      "metadata": {
        "id": "M_gVJlaDX8RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nodes_per_graph_original = 10\n",
        "test_dataset = QuarkGluonGraphDataset(dataset_name='Quark Gluon', raw_dir=main_dir, save_dir='/content',\n",
        "                                    data_folder_name=jet_folder_path, datafile_name=jet_file_path, labelsfile_name=jet_file_path,\n",
        "                                    datatype='particles', dataset_size=7000, nodes_per_graph=nodes_per_graph_original, spectral_augmentation=False)"
      ],
      "metadata": {
        "id": "geSjHgy0YDwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = torch.tensor(np.arange(2000,7000).astype('int32'))\n",
        "test_sampler = SubsetRandomSampler(test_samples)\n",
        "\n",
        "test_dataloader = test_dataloader = GraphDataLoader(\n",
        "    test_dataset, sampler=test_sampler, batch_size=500, drop_last=False\n",
        ")"
      ],
      "metadata": {
        "id": "2NHBtA-yYGK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_embds = torch.Tensor([])\n",
        "cls_labels = torch.Tensor([])\n",
        "\n",
        "for batched_graph, labels in test_dataloader:\n",
        "  graphs = []\n",
        "  unbatched_graph = dgl.unbatch(batched_graph)\n",
        "  for graph in unbatched_graph:\n",
        "    graphs.append(dgl.add_self_loop(graph))\n",
        "  batched_graph = dgl.batch(graphs)\n",
        "  batch_t = torch.arange(0, batched_graph.batch_size).reshape(-1,1).expand(batched_graph.batch_size, test_dataset.nodes_per_graph).reshape(-1,)\n",
        "\n",
        "  ## For custom GNN\n",
        "  # cls_emb = gnn.forward(batched_graph.ndata[\"node_attr\"].float(), torch.stack(batched_graph.edges()), batched_graph.edata[\"edge_attr\"].float())\n",
        "\n",
        "  ## For ParticleNet\n",
        "  pf_feats = batched_graph.ndata[\"node_attr\"].reshape(len(unbatched_graph), nodes_per_graph_original, -1).float()\n",
        "  points = pf_feats[:,:,1:3]\n",
        "  cls_emb = gnn.forward(points.reshape(points.shape[0], points.shape[2], points.shape[1])\n",
        "                     , pf_feats.reshape(pf_feats.shape[0], pf_feats.shape[2], pf_feats.shape[1]), None)\n",
        "  cls_emb = cls_emb.reshape(cls_emb.shape[0], cls_emb.shape[2], cls_emb.shape[1])\n",
        "  cls_emb = cls_emb.reshape(cls_emb.shape[0]*cls_emb.shape[1], cls_emb.shape[2])\n",
        "\n",
        "  # cls_emb = batched_graph.ndata[\"node_attr\"].float()\n",
        "  cls_emb = global_mean_pool(cls_emb, batch_t)\n",
        "  cls_embds = torch.cat((cls_embds, cls_emb.detach()), 0)     #cls_emb\n",
        "  cls_labels = torch.cat((cls_labels, labels))"
      ],
      "metadata": {
        "id": "zG95SQuHYHB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_epochs = 1000\n",
        "cls_train_data = cls_embds[ : int(0.8*len(cls_embds))]\n",
        "targets = cls_labels[ : int(0.8*len(cls_embds))]\n",
        "cls_test_data = cls_embds[int(0.8*len(cls_embds)) : ]\n",
        "testtargets = cls_labels[int(0.8*len(cls_embds)) : ]"
      ],
      "metadata": {
        "id": "bpwPGFaRYJH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Reference - https://github.com/sdogsq/LorentzNet-release/blob/main/scripts/QGTaggingROC/ROC.py\n",
        "\n",
        "# Function that takes the labels and score of the positive class\n",
        "# (top class) and returns a ROC curve, as well as the signal efficiency\n",
        "# and background rejection at a given targe signal efficiency, defaults\n",
        "# to 0.3\n",
        "\n",
        "def buildROC(labels, score, targetEff=[0.3,0.5]):\n",
        "    if not isinstance(targetEff, list):\n",
        "        targetEff = [targetEff]\n",
        "    fpr, tpr, threshold = metrics.roc_curve(labels, score)\n",
        "    idx = [np.argmin(np.abs(tpr - Eff)) for Eff in targetEff]\n",
        "    eB, eS = fpr[idx], tpr[idx]\n",
        "    return fpr, tpr, threshold, eB, eS\n",
        "\n",
        "### Reference - https://github.com/bmdillon/JetCLR/blob/main/scripts/modules/perf_eval.py\n",
        "\n",
        "def find_nearest( array, value ):\n",
        "    array = np.asarray( array )\n",
        "    idx = ( np.abs( array-value ) ).argmin()\n",
        "    return array[idx]\n",
        "\n",
        "def get_perf_stats( labels, measures ):\n",
        "    measures = np.nan_to_num( measures )\n",
        "    auc = metrics.roc_auc_score( labels, measures )\n",
        "    fpr,tpr,thresholds = metrics.roc_curve( labels, measures )\n",
        "    fpr2 = [ fpr[i] for i in range( len( fpr ) ) if tpr[i]>=0.5]\n",
        "    tpr2 = [ tpr[i] for i in range( len( tpr ) ) if tpr[i]>=0.5]\n",
        "    try:\n",
        "        imtafe = np.nan_to_num( 1 / fpr2[ list( tpr2 ).index( find_nearest( list( tpr2 ), 0.5 ) ) ] )\n",
        "    except:\n",
        "        imtafe = 1\n",
        "    return auc, imtafe"
      ],
      "metadata": {
        "id": "c9DjNmLGZHe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(128,1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "\n",
        "\n",
        "def train_classifier(cls_epochs, classifier, cls_train_data, labels, cls_test_data, testlabels):\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "  cls_loss = []\n",
        "  cls_accuracy = []\n",
        "  test_cls_loss = []\n",
        "  test_cls_accuracy = []\n",
        "\n",
        "  for ce in range(cls_epochs):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    t_correct = 0\n",
        "    t_total = 0\n",
        "    optimizer.zero_grad()\n",
        "    outputs = classifier(cls_train_data)\n",
        "    loss = torch.nn.BCELoss()(outputs,labels.view(-1,1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    predicted = np.round(outputs.cpu().detach().numpy())\n",
        "    total += labels.size(0)\n",
        "    correct += np.sum(torch.eq(torch.Tensor(predicted), labels.view(-1,1)).cpu().detach().numpy())\n",
        "    accuracy = 100 * correct / total\n",
        "    cls_loss.append(loss.cpu().detach().numpy())\n",
        "    cls_accuracy.append(accuracy)\n",
        "    testoutputs = classifier(cls_test_data)\n",
        "    testloss = torch.nn.BCELoss()(testoutputs, testlabels.view(-1,1))\n",
        "    testpredicted = np.round(testoutputs.cpu().detach().numpy())\n",
        "    t_total += testlabels.size(0)\n",
        "    t_correct += np.sum(torch.eq(torch.Tensor(testpredicted), testlabels.view(-1,1)).cpu().detach().numpy())\n",
        "    testaccuracy = 100 * t_correct / t_total\n",
        "    test_cls_loss.append(testloss.cpu().detach().numpy())  #np.mean(e_loss)\n",
        "    test_cls_accuracy.append(testaccuracy)\n",
        "    if epochs % 50 == 0:\n",
        "      print(f'Epochs : {ce} ; Loss : {loss.cpu().detach().numpy()} ; Accuracy : {accuracy} ; Test Loss : {testloss} ; Test accuracy : {testaccuracy}' )   #np.mean(e_loss)\n",
        "\n",
        "  testoutputs = classifier(cls_test_data)\n",
        "  # fpr, tpr, thresholds = metrics.roc_curve(testlabels.cpu(), testoutputs.cpu().detach().numpy())\n",
        "  fpr, tpr, threshold, eB, eS = buildROC(testlabels.cpu(), testoutputs.cpu().detach().numpy())\n",
        "  auc = metrics.auc(fpr, tpr)\n",
        "  f1_score = metrics.f1_score(testlabels.cpu(), np.round(testoutputs.cpu().detach().numpy()), average='macro')\n",
        "  _ , imtafe = get_perf_stats(testlabels.cpu(), testoutputs.cpu().detach().numpy())\n",
        "  return cls_loss, cls_accuracy, test_cls_loss, test_cls_accuracy, fpr, tpr, auc, eB, eS, f1_score, imtafe\n",
        "\n",
        "\n",
        "cls_loss, cls_accuracy, test_cls_loss, test_cls_accuracy, fpr, tpr, auc, eB, eS, f1_score, imtafe = train_classifier(cls_epochs, classifier, cls_train_data, targets, cls_test_data, testtargets)"
      ],
      "metadata": {
        "id": "FtrCG9eSYLSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cls_loss, label='Train Loss')\n",
        "plt.plot(test_cls_loss, label='Val Loss')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "RPawpog4YODT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(cls_accuracy, label='Train Accuracy')\n",
        "plt.plot(test_cls_accuracy, label='Val Accuracy')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3Xxf7zLSZQfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.patches as mpl_patches\n",
        "\n",
        "font = {'family': 'serif',\n",
        "        'color':  'darkblue',\n",
        "        'weight': 'normal',\n",
        "        'size': 12,\n",
        "        }\n",
        "\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('FPR', fontsize=14)\n",
        "plt.ylabel('TPR', fontsize=14)\n",
        "# plt.text(0.9, 0.9, 'AUC = '+str(auc.round(6)), fontdict=font, wrap=True)\n",
        "# create a list with two empty handles (or more if needed)\n",
        "handles = [mpl_patches.Rectangle((0, 0), 1, 1, fc=\"white\", ec=\"white\",\n",
        "                                 lw=0, alpha=0)]\n",
        "\n",
        "# create the corresponding number of labels (= the text you want to display)\n",
        "labels = []\n",
        "labels.append(\"AUC = \"+str(auc.round(6)))\n",
        "\n",
        "# create the legend, supressing the blank space of the empty line symbol and the\n",
        "# padding between symbol and label by setting handlelenght and handletextpad\n",
        "plt.legend(handles, labels, loc='best', fontsize='large',\n",
        "          fancybox=True, framealpha=0.7,\n",
        "          handlelength=0, handletextpad=0)"
      ],
      "metadata": {
        "id": "1d5JA8Q0ZSRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Accuracy : ', test_cls_accuracy[-1])\n",
        "print('AUC : ', auc)\n",
        "print('F1 score : ', f1_score)"
      ],
      "metadata": {
        "id": "dxQgZz78ZU0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8l7_IN9UZXPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}