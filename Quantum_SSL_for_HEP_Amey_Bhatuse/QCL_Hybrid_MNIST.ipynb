{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ffy0DH6bwxGX"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "!pip install pennylane\n",
        "\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyxcU4jnw0ka"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pennylane as qml\n",
        "import scipy\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import jax\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from PIL import Image\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob2eeLeHPAVA"
      },
      "outputs": [],
      "source": [
        "num_qubits = 3\n",
        "num_layers = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOaqO58AyeOP"
      },
      "outputs": [],
      "source": [
        "dev = qml.device(\"default.qubit\", wires=num_qubits)\n",
        "\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def circuit(inputs, params):\n",
        "    # Encoding of classical input values using RY gates\n",
        "    for j in range(num_qubits):\n",
        "        qml.RY(np.pi * inputs[j], wires=j)\n",
        "\n",
        "    # Strongly entangled paramterised quantum circuit\n",
        "    qml.StronglyEntanglingLayers(weights=params, wires=range(num_qubits))     #, imprimitive=qml.ops.CZ\n",
        "\n",
        "    # Measurement producing 1 classical output value\n",
        "    return qml.expval(qml.PauliZ(num_qubits-1))\n",
        "\n",
        "@qml.qnode(dev, interface='torch')\n",
        "def drc_circuit(inputs, input_params, weights):   #, output_params\n",
        "    # Encoding of classical input values using RY gates\n",
        "    drc_layers = input_params.shape[0]\n",
        "\n",
        "    inputs = torch.reshape(inputs, (drc_layers, int(inputs.shape[0]/drc_layers)))\n",
        "    for layer in range(drc_layers):\n",
        "      for j in range(num_qubits):\n",
        "        qml.RY(input_params[layer,j] * inputs[layer,j], wires=j)  #np.pi\n",
        "\n",
        "      qml.StronglyEntanglingLayers(weights=weights[layer,:,:].reshape((1,weights.shape[1],weights.shape[2])), wires=range(num_qubits))    #, imprimitive=qml.ops.CZ\n",
        "\n",
        "    # Measurement producing 1 classical output value\n",
        "    return qml.expval(qml.PauliZ(num_qubits-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDIkROCszuAf"
      },
      "outputs": [],
      "source": [
        "class QuantumConvolution(torch.nn.Module):\n",
        "  def __init__(self, input_size, in_filters, out_filters, kernel_size, stride, kernel, layers_per_circuit):\n",
        "    super(QuantumConvolution, self).__init__()\n",
        "    self.in_filters = in_filters\n",
        "    self.out_filters = out_filters\n",
        "    self.kernel = kernel\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.lpc = layers_per_circuit\n",
        "    self.iter = int(np.ceil(1 + (input_size[0] - self.kernel_size[0])/self.stride))\n",
        "    drc_layers = int((kernel_size[0]*kernel_size[1]) / num_qubits)\n",
        "    self.params_per_kernel_shape = qml.StronglyEntanglingLayers.shape(n_layers=drc_layers, n_wires=num_qubits)   #num_layers\n",
        "\n",
        "    for o_f in range(self.out_filters):\n",
        "      for i_f in range(self.in_filters):\n",
        "        self.add_module('torch_window_'+str(o_f)+str(i_f), qml.qnn.TorchLayer(drc_circuit, {'input_params': (drc_layers, num_qubits), 'weights':self.params_per_kernel_shape}, init_method = torch.nn.init.normal_))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    output = torch.zeros((inputs.shape[0], self.out_filters, self.iter, self.iter))\n",
        "\n",
        "    for o_f in range(self.out_filters):\n",
        "      for i_f in range(self.in_filters):\n",
        "        for l in range(self.iter):\n",
        "          for b in range(self.iter):\n",
        "\n",
        "            flattened_inputs_window = torch.nn.Flatten()(inputs[:, i_f, l*self.stride : l*self.stride + self.kernel_size[0], b*self.stride : b*self.stride + self.kernel_size[0]])\n",
        "            out_i_f = torch.vmap(self.get_submodule('torch_window_'+str(o_f)+str(i_f)))(flattened_inputs_window)\n",
        "            output[:,o_f,l,b] += out_i_f\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv4toqN53kRX"
      },
      "outputs": [],
      "source": [
        "class QuantumModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(QuantumModel, self).__init__()\n",
        "\n",
        "    # self.layer_1 = QuantumConvolution(input_size=(28,28), filters=3, kernel_size=(2,2), stride=2, kernel=circuit, layers_per_circuit=1)\n",
        "    # self.layer_2 = QuantumConvolution(input_size=(14,14), filters=3, kernel_size=(2,2), stride=2, kernel=circuit, layers_per_circuit=num_layers)\n",
        "    self.layer_3 = QuantumConvolution(input_size=(18+1,18+1), in_filters=1, out_filters=3, kernel_size=(3,3), stride=2, kernel=circuit, layers_per_circuit=num_layers)\n",
        "    self.layer_4 = QuantumConvolution(input_size=(9,9), in_filters=3, out_filters=1, kernel_size=(3,3), stride=2, kernel=circuit, layers_per_circuit=num_layers)\n",
        "    # self.layer_5 = QuantumConvolution(input_size=(4+1,4+1), in_filters=2, out_filters=1, kernel_size=(3,3), stride=2, kernel=circuit, layers_per_circuit=1)\n",
        "    # self.layer_6 = QuantumConvolution(input_size=(2,2), filters=3, kernel_size=(2,2), stride=1, kernel=circuit, layers_per_circuit=1)\n",
        "    self.layer_7 = torch.nn.Flatten()\n",
        "    self.layer_8 = torch.nn.Linear(16,2)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.nn.functional.pad(x, (0,1,0,1), 'constant', value = 0)\n",
        "    x = self.layer_3(x)\n",
        "    x = self.layer_4(x)\n",
        "    x = self.layer_7(x)\n",
        "    x = self.layer_8(x)\n",
        "    x = torch.nn.functional.normalize(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qm = QuantumModel()\n",
        "def count_parameters(model):\n",
        "  model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "  params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "  return params\n",
        "\n",
        "count_parameters(qm)"
      ],
      "metadata": {
        "id": "OV1FG35l3YL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_X6WbCCrxStY"
      },
      "outputs": [],
      "source": [
        "class TwoCropTransform:\n",
        "    \"\"\"Create two crops of the same image\"\"\"\n",
        "    def __init__(self, transform):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return [self.transform(x), self.transform(x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1tYyHkSMbY0"
      },
      "outputs": [],
      "source": [
        "class SupConLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07, contrast_mode='all',\n",
        "                 base_temperature=0.07):\n",
        "        super(SupConLoss, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.contrast_mode = contrast_mode\n",
        "        self.base_temperature = base_temperature\n",
        "\n",
        "    def calculate_fidelity(self, feature_1, feature_2):\n",
        "      swap_test = SwapTest(feature_1, feature_2)\n",
        "      fidelity = torch.sub(torch.mul(swap_test[:,0].reshape(feature_1.shape[0], feature_2.shape[0]), 2), 1)\n",
        "      return fidelity\n",
        "\n",
        "    def matrix_mul(self, mat1, mat2):\n",
        "      return torch.matmul(mat1, mat2.T)\n",
        "\n",
        "    def dot_prod(self, v1, v2):\n",
        "      return torch.dot(v1,v2)\n",
        "\n",
        "    def forward(self, features, labels=None, mask=None):\n",
        "\n",
        "        if len(features.shape) < 3:\n",
        "            raise ValueError('`features` needs to be [bsz, n_views, ...],'\n",
        "                             'at least 3 dimensions are required')\n",
        "        if len(features.shape) > 3:\n",
        "            features = features.view(features.shape[0], features.shape[1], -1)\n",
        "\n",
        "        batch_size = features.shape[0]\n",
        "        if labels is not None and mask is not None:\n",
        "            raise ValueError('Cannot define both `labels` and `mask`')\n",
        "        elif labels is None and mask is None:\n",
        "            mask = torch.eye(batch_size, dtype=torch.float32) #.to(device)\n",
        "        elif labels is not None:\n",
        "            labels = labels.contiguous().view(-1, 1)\n",
        "            if labels.shape[0] != batch_size:\n",
        "                raise ValueError('Num of labels does not match num of features')\n",
        "            mask = torch.eq(labels, labels.T).float() #.to(device)\n",
        "        else:\n",
        "            mask = mask.float()\n",
        "\n",
        "        contrast_count = features.shape[1]\n",
        "        contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
        "        if self.contrast_mode == 'one':\n",
        "            anchor_feature = features[:, 0]\n",
        "            anchor_count = 1\n",
        "        elif self.contrast_mode == 'all':\n",
        "            anchor_feature = contrast_feature\n",
        "            anchor_count = contrast_count\n",
        "        else:\n",
        "            raise ValueError('Unknown mode: {}'.format(self.contrast_mode))\n",
        "\n",
        "\n",
        "        anchor_dot_contrast = torch.div(\n",
        "            (torch.vmap(torch.vmap(self.dot_prod, in_dims=(None,0)), in_dims=(0,None)))(anchor_feature, contrast_feature),\n",
        "            self.temperature\n",
        "            )\n",
        "        logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
        "        logits = anchor_dot_contrast - logits_max.detach()\n",
        "\n",
        "        # tile mask\n",
        "        mask = mask.repeat(anchor_count, contrast_count)\n",
        "\n",
        "        # mask-out self-contrast cases\n",
        "        logits_mask = torch.scatter(\n",
        "            torch.ones_like(mask),\n",
        "            1,\n",
        "            torch.arange(batch_size * anchor_count).view(-1, 1), #.to(device),\n",
        "            0\n",
        "        )\n",
        "        mask = mask * logits_mask\n",
        "\n",
        "        # compute log_prob\n",
        "        exp_logits = torch.exp(logits) * logits_mask\n",
        "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        # compute mean of log-likelihood over positive\n",
        "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
        "\n",
        "        # loss\n",
        "        loss = - (self.temperature / self.base_temperature) * mean_log_prob_pos\n",
        "        loss = loss.view(anchor_count, batch_size).mean()\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNb-UjSE2W0E"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class MetricMonitor:\n",
        "    def __init__(self, float_precision=4):\n",
        "        self.float_precision = float_precision\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n",
        "\n",
        "    def update(self, metric_name, val):\n",
        "        metric = self.metrics[metric_name]\n",
        "\n",
        "        metric[\"val\"] += val\n",
        "        metric[\"count\"] += 1\n",
        "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \" | \".join(\n",
        "            [\n",
        "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
        "                    metric_name=metric_name, avg=metric[\"avg\"], float_precision=self.float_precision\n",
        "                )\n",
        "                for (metric_name, metric) in self.metrics.items()\n",
        "            ]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMZhVQ2uZUwQ"
      },
      "outputs": [],
      "source": [
        "def quantum_state_fidelity(state1, state2):\n",
        "  return 1 - torch.abs(torch.dot(torch.conj(state1), state2))**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gehy_hIOY50V"
      },
      "outputs": [],
      "source": [
        "def lalign(x, y, alpha=2):\n",
        "  return (x - y).norm(dim=1).pow(alpha).mean()\n",
        "\n",
        "def lunif(x, t=2):\n",
        "  sq_pdist = torch.pdist(x, p=2).pow(2)\n",
        "  return sq_pdist.mul(-t).exp().mean().log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vv0yWC_ZMw1B"
      },
      "outputs": [],
      "source": [
        "def pretraining(epoch, model, contrastive_loader, optimizer, criterion, method='SimCLR', lama=None, lamu=None, alpha=None, t=None):  #SupCon\n",
        "    \"Contrastive pre-training over an epoch\"\n",
        "    metric_monitor = MetricMonitor()\n",
        "    for batch_idx, (data,labels) in enumerate(contrastive_loader):\n",
        "        data = torch.cat([data[0], data[1]], dim=0)\n",
        "        if torch.cuda.is_available():\n",
        "            data,labels = data.cuda(), labels.cuda()\n",
        "        data, labels = torch.autograd.Variable(data,False), torch.autograd.Variable(labels)\n",
        "        bsz = labels.shape[0]\n",
        "        features = model(data)\n",
        "        f1, f2 = torch.split(features, [bsz, bsz], dim=0)\n",
        "        features = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\n",
        "        if method == 'SupCon':\n",
        "            loss = criterion(features, labels)\n",
        "        elif method == 'SimCLR':\n",
        "            loss = criterion(features)\n",
        "            la = lalign(features[:,0,:], features[:,1,:])\n",
        "            lux = lunif(features[:,0,:])\n",
        "            luy = lunif(features[:,1,:])\n",
        "            print('lalign', la)\n",
        "            print('luniform', (lux + luy) / 2)\n",
        "        elif method == 'AlignAndUniform':\n",
        "            # print(features[:,0,:].shape)\n",
        "            la = lalign(features[:,0,:], features[:,1,:], alpha)\n",
        "            lux = lunif(features[:,0,:], t)\n",
        "            luy = lunif(features[:,1,:], t)\n",
        "            print('lalign', la)\n",
        "            print('luniform', (lux + luy) / 2)\n",
        "            loss = lama * la + lamu * (lux + luy) / 2\n",
        "        else:\n",
        "            raise ValueError('contrastive method not supported: {}'.format(method))\n",
        "        metric_monitor.update(\"Loss\", loss.item())\n",
        "        metric_monitor.update(\"Learning Rate\", optimizer.param_groups[0]['lr'])\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"[Epoch: {epoch:03d}] Contrastive Pre-train | {metric_monitor}\".format(epoch=epoch, metric_monitor=metric_monitor))\n",
        "    return metric_monitor.metrics['Loss']['avg'], metric_monitor.metrics['Learning Rate']['avg']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Z2_Normalization(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Z2_Normalization, self).__init__()\n",
        "\n",
        "  def forward(self, img):\n",
        "    if len(img.shape) > 2:\n",
        "      mean = torch.Tensor([img[0].mean(), img[1].mean(), img[2].mean()])\n",
        "      std = torch.Tensor([img[0].std(), img[1].std(), img[2].std()])\n",
        "      return torchvision.transforms.functional.normalize(img, mean, std, inplace=True)\n",
        "    else:\n",
        "      return torchvision.transforms.functional.normalize(img, img.mean(), img.std(), inplace=True)"
      ],
      "metadata": {
        "id": "qjZHNIXPoPYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LQZEEQPM-Q0",
        "outputId": "909539b1-e43c-4a43-92d5-f2a915a45ce3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 125, 125, 3)\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "\n",
        "num_epochs = 50\n",
        "batch_size = 256  #1280\n",
        "use_early_stopping = True\n",
        "use_scheduler = False\n",
        "\n",
        "\n",
        "contrastive_transform = transforms.Compose([\n",
        "                                    transforms.RandomHorizontalFlip(p=0.5),\n",
        "                                    transforms.RandomVerticalFlip(p=0.5),\n",
        "                                    # transforms.RandomRotation([0,180]),\n",
        "                                    transforms.ToImage(),\n",
        "                                    transforms.ToDtype(torch.float32, scale=True),\n",
        "                                    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
        "                                    # Z2_Normalization(),\n",
        "                                    ])\n",
        "\n",
        "\n",
        "contrastive_set = datasets.MNIST('./data', download=True, train=True, transform=TwoCropTransform(contrastive_transform))\n",
        "\n",
        "inds = []\n",
        "for index,i in enumerate(contrastive_set.targets):\n",
        "  if i == 0 or i == 8:\n",
        "    inds.append(index)\n",
        "\n",
        "contrastive_set.data = contrastive_set.data[inds]\n",
        "contrastive_set.targets = contrastive_set.targets[inds]\n",
        "\n",
        "# Resizing images to size 8x8\n",
        "contrastive_set.data = transforms.Resize(18)(contrastive_set.data)\n",
        "print(contrastive_set.data.shape)\n",
        "# Define the desired subset size\n",
        "subset_size = 1024 #5120\n",
        "\n",
        "# Create a subset of the dataset\n",
        "subset_indices = torch.randperm(len(contrastive_set), generator=torch.manual_seed(0))[:subset_size]\n",
        "subset_con = torch.utils.data.Subset(contrastive_set, subset_indices)\n",
        "\n",
        "contrastive_loader = torch.utils.data.DataLoader(subset_con, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC4pmnxgNJL8"
      },
      "outputs": [],
      "source": [
        "# Part 1\n",
        "model = QuantumModel()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)  #, weight_decay=1e-3\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs/5, gamma=0.9)\n",
        "\n",
        "contrastive_loss, contrastive_lr = [], []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  loss, lr = pretraining(epoch, model, contrastive_loader, optimizer, criterion, method='AlignAndUniform', lama=1, lamu=0.9, alpha=2, t=2)  #SupCon #SimCLR #\n",
        "  if use_scheduler:\n",
        "    scheduler.step()\n",
        "  print(criterion.temperature)\n",
        "  contrastive_loss.append(loss)\n",
        "  contrastive_lr.append(lr)\n",
        "\n",
        "# save_model(model, optimizer, num_epochs, save_file)\n",
        "\n",
        "plt.plot(range(1,len(contrastive_lr)+1),contrastive_lr, color='b', label = 'learning rate')\n",
        "plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Learning Rate'), plt.show()\n",
        "\n",
        "plt.plot(range(1,len(contrastive_loss)+1),contrastive_loss, color='b', label = 'loss')\n",
        "plt.legend(), plt.ylabel('loss'), plt.xlabel('epochs'), plt.title('Loss'), plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TFZptIJkspF"
      },
      "outputs": [],
      "source": [
        "model.training = False\n",
        "cls_epochs = 1000\n",
        "\n",
        "cls_data_set = datasets.MNIST('./data', download=True, train=True,\n",
        "                              transform=transforms.Compose([ #transforms.ToTensor(),\n",
        "                                                             transforms.ToImage(),\n",
        "                                                             transforms.ToDtype(torch.float32, scale=True),\n",
        "                                                             transforms.Normalize((0.5,), (0.5,)),]))\n",
        "\n",
        "cls_data_set.data = transforms.Resize(18)(cls_data_set.data)\n",
        "cls_data_set.data = cls_data_set.data[inds]\n",
        "cls_data_set.targets = cls_data_set.targets[inds]\n",
        "cls_data_set.data = cls_data_set.data[subset_indices]         #[:1000]\n",
        "cls_data_set.targets = cls_data_set.targets[subset_indices]    #[:1000]\n",
        "\n",
        "targets = []\n",
        "for i in cls_data_set.targets:\n",
        "  if i == 0:\n",
        "    targets.append(0)\n",
        "  else:\n",
        "    targets.append(1)\n",
        "\n",
        "a = []\n",
        "for i, (data, label) in enumerate(cls_data_set):\n",
        "  a.append(data)\n",
        "\n",
        "test_set_size = 250\n",
        "cls_test_set = datasets.MNIST('./data', download=True, train=False,\n",
        "                              transform=transforms.Compose([ #transforms.ToTensor(),\n",
        "                                                             transforms.ToImage(),\n",
        "                                                             transforms.ToDtype(torch.float32, scale=True),\n",
        "                                                             transforms.Normalize((0.5,), (0.5,)),]))\n",
        "\n",
        "testinds = []\n",
        "for index,i in enumerate(cls_test_set.targets):\n",
        "  if i == 0 or i == 8:\n",
        "    testinds.append(index)\n",
        "\n",
        "cls_test_set.data = transforms.Resize(18)(cls_test_set.data)\n",
        "cls_test_set.data = cls_test_set.data[testinds]\n",
        "cls_test_set.targets = cls_test_set.targets[testinds]\n",
        "cls_test_set.data = cls_test_set.data[:test_set_size]\n",
        "cls_test_set.targets = cls_test_set.targets[:test_set_size]\n",
        "\n",
        "testtargets = []\n",
        "for i in cls_test_set.targets:\n",
        "  if i == 0:\n",
        "    testtargets.append(0)\n",
        "  else:\n",
        "    testtargets.append(1)\n",
        "\n",
        "b = []\n",
        "for i, (data, label) in enumerate(cls_test_set):\n",
        "  b.append(data)\n",
        "\n",
        "\n",
        "classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(2,32),\n",
        "    torch.nn.BatchNorm1d(32),\n",
        "    torch.nn.LeakyReLU(),\n",
        "    torch.nn.Linear(32,1),\n",
        "    torch.nn.Sigmoid()\n",
        ")\n",
        "\n",
        "\n",
        "# random_model = QuantumModel()\n",
        "# cls_train_data = random_model(torch.stack(a))\n",
        "cls_train_data = model(torch.autograd.Variable(torch.stack(a), False))   #torch.squeeze(torch.stack(a), 1)\n",
        "print(cls_train_data.shape)\n",
        "\n",
        "cls_test_data = model(torch.autograd.Variable(torch.stack(b), False))   #torch.squeeze(torch.stack(a), 1)\n",
        "print(cls_test_data.shape)\n",
        "\n",
        "targets = torch.unsqueeze(torch.Tensor(targets), 1)\n",
        "cls_train_data, targets = torch.autograd.Variable(cls_train_data,False), torch.autograd.Variable(torch.Tensor(targets)).to(torch.float32)\n",
        "\n",
        "testtargets = torch.unsqueeze(torch.Tensor(testtargets), 1)\n",
        "cls_test_data, testtargets = torch.autograd.Variable(cls_test_data,False), torch.autograd.Variable(torch.Tensor(testtargets)).to(torch.float32)\n",
        "\n",
        "\n",
        "def train_classifier(cls_epochs, classifier, cls_train_data, labels, subset_size, batch_size, cls_test_data, testlabels):\n",
        "  optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
        "  cls_loss = []\n",
        "  cls_accuracy = []\n",
        "  test_cls_loss = []\n",
        "  test_cls_accuracy = []\n",
        "\n",
        "  for ce in range(cls_epochs):\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    t_correct = 0\n",
        "    t_total = 0\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    outputs = classifier(cls_train_data)\n",
        "    loss = torch.nn.BCELoss()(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    predicted = np.round(outputs.detach().numpy())\n",
        "    total += labels.size(0)\n",
        "    correct += np.sum(torch.eq(torch.Tensor(predicted), labels).detach().numpy())\n",
        "    accuracy = 100 * correct / total\n",
        "    cls_loss.append(loss.detach().numpy())\n",
        "    cls_accuracy.append(accuracy)\n",
        "    testoutputs = classifier(cls_test_data)\n",
        "    testloss = torch.nn.BCELoss()(testoutputs, testlabels)\n",
        "    testpredicted = np.round(testoutputs.detach().numpy())\n",
        "    t_total += testlabels.size(0)\n",
        "    t_correct += np.sum(torch.eq(torch.Tensor(testpredicted), testlabels).detach().numpy())\n",
        "    testaccuracy = 100 * t_correct / t_total\n",
        "    test_cls_loss.append(testloss.detach().numpy())  #np.mean(e_loss)\n",
        "    test_cls_accuracy.append(testaccuracy)\n",
        "    print(f'Epochs : {ce} ; Loss : {loss.detach().numpy()} ; Accuracy : {accuracy} ; Test Loss : {testloss} ; Test accuracy : {testaccuracy}' )   #np.mean(e_loss)\n",
        "\n",
        "  return cls_loss, cls_accuracy, test_cls_loss, test_cls_accuracy\n",
        "\n",
        "\n",
        "cls_loss, cls_accuracy, test_cls_loss, test_cls_accuracy = train_classifier(cls_epochs, classifier, cls_train_data, targets, subset_size, batch_size, cls_test_data, testtargets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQA3EX7isuZM"
      },
      "outputs": [],
      "source": [
        "plt.plot(cls_loss, label='Train loss')\n",
        "plt.plot(test_cls_loss, label='Test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kwMBsRys1Qb"
      },
      "outputs": [],
      "source": [
        "plt.plot(cls_accuracy, label='Train accuracy')\n",
        "plt.plot(test_cls_accuracy, label='Test accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMUDtPWf8Esb"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "\n",
        "v = []\n",
        "for i, (data, label) in enumerate(cls_data_set):\n",
        "  v.append(data)\n",
        "\n",
        "# randommodel = QuantumModel()\n",
        "# vdata = randommodel(torch.stack(a)).detach().numpy()\n",
        "vdata = model(torch.autograd.Variable(torch.stack(v), False)).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n41gJu2eCTuX"
      },
      "outputs": [],
      "source": [
        "kde = KernelDensity(kernel='gaussian', bandwidth=0.01).fit(vdata)\n",
        "gv = kde.score_samples(vdata)\n",
        "\n",
        "gvs = kde.sample(1024)\n",
        "plt.scatter(gvs[:,0], gvs[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgcg8mjNPoGk"
      },
      "outputs": [],
      "source": [
        "plt.scatter(vdata[:,0], vdata[:,1])\n",
        "plt.xticks(np.arange(-1,1,0.1))\n",
        "plt.yticks(np.arange(-1,1,0.1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEjtn6o3DRmz"
      },
      "outputs": [],
      "source": [
        "class_0 = np.array([[0,0]])\n",
        "class_1 = np.array([[0,0]])\n",
        "for i,t in enumerate(cls_data_set.targets):\n",
        "  if t == 8:\n",
        "    class_1 = np.append(class_1, [vdata[i]],axis=0)\n",
        "  else:\n",
        "    class_0 = np.append(class_0, [vdata[i]],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybeHeMlILyRD"
      },
      "outputs": [],
      "source": [
        "plt.scatter(class_1[1:,0],class_1[1:,1])\n",
        "plt.xticks(np.arange(-1,1,0.1))\n",
        "plt.yticks(np.arange(-1,1,0.1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvYINpXbMEBP"
      },
      "outputs": [],
      "source": [
        "plt.scatter(class_0[1:,0],class_0[1:,1])\n",
        "plt.xticks(np.arange(-1,1,0.1))\n",
        "plt.yticks(np.arange(-1,1,0.1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R349sPS24hUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}